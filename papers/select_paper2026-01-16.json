[
  {
    "date": "2026-1-16",
    "title": "Resource-Efficient Personal Large Language Models Fine-Tuning with Collaborative Edge Computing",
    "authors": "Shengyuan Ye, Bei Ouyang, Tianyi Qian, Liekang Zeng, Jingyi Li, Jiangsu Du, Xiaowen Chu, Guoliang Xing, Xu Chen",
    "publish": "IEEE Transactions on Parallel and Distributed Systems",
    "url": "https://doi.org/10.1109/tpds.2026.3654957",
    "source": "IEEE",
    "abstract": "Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. Other studies focus on exploiting the potential of edge devices through resource management optimization, yet are ultimately bottlenecked by the resource wall of individual devices. To tackle these challenges, we propose <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">PAC+</monospace>, a resource efficient collaborative edge AI framework for in-situ personal LLMs fine-tuning. <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">PAC+</monospace> breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">PAC+</monospace> implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">PAC+</monospace> leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone, enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation of the prototype implementation demonstrates that <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">PAC+</monospace> significantly outperforms existing collaborative edge training systems, achieving up to a <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$9.7\\times$</tex-math></inline-formula> end-to-end speedup. Furthermore, compared to mainstream LLM fine-tuning algorithms, <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">PAC+</monospace> reduces memory footprint by up to <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$88.16\\%$</tex-math></inline-formula>.",
    "title_zh": "基于协同边缘计算的资源高效个人大语言模型微调",
    "abstract_zh": "大型语言模型（LLMs）在边缘网络上开启了众多强大应用，例如智能个人助手。然而，数据隐私与安全问题促使人们转向基于边缘的个性化LLM微调，以减少对云端的依赖。这一转变带来了计算密集和资源稀缺的问题，严重制约了训练效率与可行性。尽管现有研究探索了参数高效微调（PEFT）技术以缓解资源限制，但我们的分析表明，这些技术在边缘设备上仍不够资源高效。另一些研究则致力于通过优化资源管理来挖掘边缘设备的潜力，但最终仍受限于单个设备的资源瓶颈。为应对这些挑战，我们提出了PAC+——一种面向现场个性化LLM微调的高效协同边缘AI框架。PAC+通过算法-系统协同设计，突破了个性化LLM微调的资源壁垒。\n\n（1）在算法层面，PAC+实现了一种在参数量、时间和内存方面均高效的个性化LLM微调方法。它采用并行适配器（Parallel Adapters），避免了对LLM主干网络进行完整的反向传播。此外，激活缓存机制进一步简化流程，消除了多轮次训练中重复前向传播的需求。\n\n（2）在系统层面，PAC+利用邻近的边缘设备，将其聚合为一个集体资源池，用于现场个性化LLM的微调。通过混合数据并行与流水线并行策略，实现分布式训练的协调。由于激活缓存消除了对LLM主干网络的前向传播需求，因此可仅使用数据并行方式对并行适配器进行专属微调。\n\n对原型系统的大量评估表明，PAC+显著优于现有的协同边缘训练系统，在端到端性能上实现了最高达9.7倍的加速。此外，相较于主流LLM微调算法，PAC+将内存占用降低了高达88.16%。"
  },
  {
    "date": "2026-1-16",
    "title": "Refactoring for Sustainable Maintainability: A SonarQube-Based Case Study on a Campus Canteen Application",
    "authors": "Reyhan Arim Yulanda, Adam Shidqul Aziz, Umi Sa'adah, Desy Intan Permatasari, Fadilah Fahrul Hardiansyah, Andhik Ampuh Yunanto, Nailussa'ada",
    "publish": "2025 8th International Conference on Informatics and Computational Sciences (ICICoS)",
    "url": "https://doi.org/10.1109/icicos68590.2025.11329637",
    "source": "IEEE",
    "abstract": "Ensuring software maintainability is essential for the long-term sustainability of digital systems in educational institutions, where applications often evolve under limited resources and high turnover of student developers. The FoodLAB application, which manages online canteen operations at PENS, has accumulated technical debt over time due to inconsistent coding practices and poor code quality. This study introduces a structured refactoring process guided by SonarQube analysis and the McCall Software Quality Model to address these maintainability challenges. Various code smells, including long methods, dead code, duplicated logic, and feature envy, were systematically identified and resolved through targeted refactoring techniques. Improvements were applied to both backend and frontend modules, resulting in reduced complexity, cleaner structure, and better modularity. The findings demonstrate a significant reduction in code smells and duplication, producing a more reliable and maintainable codebase. Overall, this research highlights the role of software quality assurance in supporting critical campus applications and offers practical insights for improving maintainability in resource-constrained educational environments, with future work focusing on integrating automated testing and continuous quality monitoring.",
    "title_zh": "面向可持续可维护性的重构：基于SonarQube的校园食堂应用案例研究",
    "abstract_zh": "确保软件可维护性对于教育机构中数字系统的长期可持续发展至关重要，尤其是在资源有限且学生开发者流动性高的背景下。PENS大学的FoodLAB应用在管理在线食堂运营过程中，由于编码规范不一致和代码质量低下，积累了大量的技术债务。本研究提出了一种基于SonarQube分析与McCall软件质量模型的系统化重构流程，以应对这些可维护性挑战。通过针对性的重构技术，系统性地识别并解决了多种代码异味问题，包括过长方法、无用代码、重复逻辑以及特征痴迷等。改进措施同时应用于后端与前端模块，显著降低了代码复杂度，优化了结构清晰度，并提升了模块化程度。研究结果表明，代码异味和重复代码数量均得到显著减少，构建出更加可靠且易于维护的代码库。总体而言，本研究凸显了软件质量保障在支持关键校园应用中的重要作用，并为资源受限的教育环境提供了切实可行的可维护性提升策略。未来工作将聚焦于集成自动化测试与持续质量监控机制，进一步推动软件质量的持续改进。"
  },
  {
    "date": "2026-1-16",
    "title": "Toward Deployable CXL-PNM: The CMM-Ax Prototype and Software Stack",
    "authors": "Kwangsik Shin, Kangkyu Park, Joonseop Sim, Thomas Won ha Choi, Youngpyo Joo, Hoshik Kim",
    "publish": "IEEE Transactions on Computers",
    "url": "https://doi.org/10.1109/tc.2026.3654850",
    "source": "IEEE",
    "abstract": "Processing-Near-Memory (PNM) over Compute Express Link (CXL) has strong architectural appeal. However, most existing CXL-attached PNM prototypes remain inflexible or simulation-based and lack integration with full software stacks, which limits their practical deployment. This work introduces CMM-Ax, a deployable hybrid CXL-PNM system that combines streaming near-memory pipelines with device-side programmability and integrates tightly with the Heterogeneous Memory Software Development Kit (HMSDK). Through HMSDK, CMM-Ax exposes CXL-attached memory via standard Linux allocation paths (e.g., malloc and mmap). This allows existing vector databases and search engines to adopt heterogeneous memory with minimal application-level changes, such as selecting the CMM-Ax FAISS backend. CMM-Ax further provides a comprehensive software stack—FAISS integration, a domain-specific compiler for user-defined operators, and a Kubernetes device plugin supporting multi-tenant slicing—capabilities not demonstrated in prior CXL-PNM. Using an FPGA-based CXL Type-3 prototype, CMM-Ax achieves 4.54× higher throughput and 5.56× lower energy per query than a CXL memory-only baseline on the exact k-nearest neighbor (kNN) search. For approximate nearest neighbor (ANN) inverted-file (IVF) workloads, CMM-Ax sustains bandwidth-proportional efficiency at small batches (batch=1: 37% vs. 12% CPU, 7% GPU). In Kubernetes deployments, CMM-Ax–equipped servers can replace a significantly larger CPU-only cluster at equivalent memory capacity and throughput, reducing node count and system energy.",
    "title_zh": "面向可部署CXL-PNM：CMM-Ax原型及其软件栈",
    "abstract_zh": "基于计算互连标准（CXL）的近内存处理（PNM）具有强大的架构吸引力。然而，目前大多数基于CXL的近内存原型系统仍存在灵活性差或仅限于仿真，且缺乏与完整软件栈的集成，限制了其实际部署。本文提出了一种可部署的混合式CXL-PNM系统——CMM-Ax，该系统结合了流式近内存处理管道与设备端可编程性，并与异构内存软件开发工具包（HMSDK）深度集成。通过HMSDK，CMM-Ax能够通过标准Linux内存分配路径（如malloc和mmap）暴露CXL连接的内存，使现有向量数据库和搜索引擎仅需少量应用层修改即可采用异构内存，例如选择CMM-Ax的FAISS后端。此外，CMM-Ax提供了一套完整的软件栈：支持FAISS集成、为用户自定义操作符设计的领域专用编译器，以及支持多租户资源切分的Kubernetes设备插件，这些能力在以往的CXL-PNM系统中尚未实现。基于FPGA的CXL Type-3原型测试表明，在精确k近邻（kNN）搜索任务中，CMM-Ax相比纯CXL内存基线实现了4.54倍更高的吞吐量和5.56倍更低的每查询能耗；在近似最近邻（ANN）倒排文件（IVF）工作负载下，小批量场景（batch=1）中CMM-Ax保持了与带宽成比例的效率（CPU为37% vs. 12%，GPU为7%）。在Kubernetes部署环境中，配备CMM-Ax的服务器可在等效内存容量和吞吐量条件下，替代规模大得多的纯CPU集群，显著减少节点数量与系统能耗。"
  },
  {
    "date": "2026-1-16",
    "title": "SQL Injection in LLM-Generated Queries: Systematic Analysis of Detection Gaps and Security Risks",
    "authors": "Eunyoung Kim, Sangkyun Lee",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2026.3654822",
    "source": "IEEE",
    "abstract": "Large language models (LLMs) are being woven into software systems at a remarkable pace. When these systems include a back-end database, LLM integration opens new attack surfaces for SQL injection (SQLi). We present the first systematic security evaluation of LLM-generated SQL across 14 models—four accessed via web-based chat interfaces and ten via direct APIs. Using 100 expert-crafted attack prompts spanning three complexity tiers (basic, medium, advanced), we generated queries from each model and executed them on SQLite, MySQL, and PostgreSQL. The results expose pronounced security disparities. Web-interface models rejected 41.5% of potentially malicious prompts, whereas API-access models blocked only 13.4%, a 3.1× difference in protective behavior. Cross-database testing showed MySQL to be most susceptible (12.1% successful attacks), SQLite moderately vulnerable (8.1%), and PostgreSQL the most resilient (1.7%). We further assessed four popular SQLi-detection techniques—regular-expression matching, classical machine learning, a convolutional neural network, and a RoBERTa-based contextual model. All experienced severe performance degradation when confronted with LLM-generated queries: overall accuracy plummeted from roughly 98% on standard SQLi benchmarks to just 60% on our test set. Even the context-aware embedding model failed to reliably flag malicious patterns in LLM outputs. Our work contributes (i) the first comprehensive characterization of SQLi risks posed by LLMs, (ii) a publicly released evaluation framework with expert-validated test cases, and (iii) evidence of critical blind spots in current SQLi-detection mechanisms that must be addressed to secure next-generation LLM applications.",
    "title_zh": "LLM生成查询中的SQL注入：检测漏洞与安全风险的系统性分析",
    "abstract_zh": "大型语言模型（LLMs）正以惊人的速度被整合进各类软件系统中。当这些系统包含后端数据库时，LLM的集成会为SQL注入（SQLi）攻击开辟新的攻击面。本文首次对14种主流模型生成的SQL语句进行了系统的安全评估——其中包括4个通过基于网页的聊天界面访问的模型，以及10个通过直接API调用的模型。我们使用了100个由专家精心设计的攻击提示，涵盖三个复杂度层级（基础、中等、高级），从每种模型生成查询语句，并在SQLite、MySQL和PostgreSQL上执行测试。结果揭示出显著的安全差异：通过网页界面访问的模型拒绝了41.5%的潜在恶意输入，而通过API访问的模型仅阻止了13.4%，防护能力相差3.1倍。跨数据库测试表明，MySQL最易受攻击（成功攻击率达12.1%），SQLite中等脆弱（8.1%），而PostgreSQL最为稳健（仅1.7%）。此外，我们还评估了四种流行的SQL注入检测技术：正则表达式匹配、传统机器学习方法、卷积神经网络以及基于RoBERTa的上下文感知模型。所有方法在面对LLM生成的查询时均出现严重性能下降：整体准确率从标准SQLi基准测试中的约98%骤降至本研究测试集上的60%。即使具备上下文理解能力的嵌入模型也未能可靠识别LLM输出中的恶意模式。本研究贡献如下：(i) 首次全面刻画了LLM带来的SQL注入风险；(ii) 发布了一个公开可用的评估框架，包含经专家验证的测试用例；(iii) 揭示了当前SQL注入检测机制存在关键盲区，亟需改进，以保障下一代LLM应用的安全性。"
  },
  {
    "date": "2026-1-16",
    "title": "A Modular Architecture Framework for Flutter Using Clean Architecture and Design Patterns: A Software Quality Perspective",
    "authors": "Arianto Zaki Hamdalah, Nailussa'ada, Umi Sa'adah, Desy Intan Permatasari, Adam Shidqul Aziz, Maulidan Bagus Afridian Rasyid, Ahmad Jarir At-Thobari",
    "publish": "2025 8th International Conference on Informatics and Computational Sciences (ICICoS)",
    "url": "https://doi.org/10.1109/icicos68590.2025.11330072",
    "source": "IEEE",
    "abstract": "The increasing complexity of mobile applications demands architectural solutions that support scalability, maintainability, and modular design. This paper presents a reusable modular architecture framework for Flutter applications that integrates Clean Architecture principles with Creational and Structural design patterns. The framework organizes each feature as an independent module (View-Controller-Mediator), with the Facade pattern applied at the controller boundary and a Singleton Mediator coordinating cross-module interactions. To evaluate architectural quality, a case study was conducted on two representative modules (Login and Setting) using the Chidamber and Kemerer (CK94) variants of LCOM and CBO metrics to measure internal cohesion and coupling. The methodology includes structural design documentation, controller pseudocode, and manual metric computation derived from class diagrams. Results indicate high cohesion <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$(\\text{LCOM}=1)$</tex> and low coupling <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$(\\text{CBO}=1)$</tex> for the studied controllers, which supports adherence to Single Responsibility and Dependency Inversion principles. We further map these results to ISO/IEC 25010 maintainability sub-characteristics, highlighting modularity, testability, and analyzability. The main contribution is a practical and documentable framework with reproducible evaluation steps that software engineers can adopt as a baseline for modular Flutter systems. Future work will extend empirical evaluation to larger applications and include comparative baselines against Provider, BLoC, and MVVM architectures.",
    "title_zh": "基于清洁架构与设计模式的Flutter模块化架构框架：软件质量视角",
    "abstract_zh": "移动应用日益复杂的特性对可扩展性、可维护性和模块化设计的架构解决方案提出了更高要求。本文提出了一种适用于Flutter应用的可复用模块化架构框架，该框架将Clean Architecture原则与创建型及结构型设计模式相结合。该框架将每个功能组织为独立的模块（视图-控制器-中介者），在控制器边界应用外观（Facade）模式，并通过单例中介者协调跨模块的交互。为评估架构质量，选取两个代表性模块（登录与设置）进行案例研究，采用Chidamber和Kemerer（CK94）提出的LCOM和CBO度量指标，分别衡量模块内部的内聚度与耦合度。研究方法包括结构设计文档、控制器伪代码以及基于类图的手动度量计算。结果表明，所研究的控制器具有高内聚性（LCOM=1）和低耦合性（CBO=1），这验证了其符合单一职责原则和依赖倒置原则。进一步地，我们将这些结果映射至ISO/IEC 25010标准中的可维护性子特性，凸显了该架构在模块化、可测试性和可分析性方面的优势。本研究的主要贡献在于提供了一个实用且可文档化的框架，具备可重复的评估步骤，可供软件工程师作为构建模块化Flutter系统的基准参考。未来工作将扩展实证评估至更大规模的应用，并与Provider、BLoC及MVVM等架构进行对比分析。"
  }
]