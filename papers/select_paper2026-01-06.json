[
  {
    "date": "2026-01-06",
    "title": "From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures",
    "authors": "Waleed Khalid, Dmitry Ignatov, Radu Timofte",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02997v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.",
    "title_zh": "从记忆到创造：大语言模型作为新型神经架构的设计者",
    "abstract_zh": "大型语言模型（LLMs）在程序合成方面表现出色，但其在自主导航神经网络架构设计——兼顾语法可靠性、性能表现与结构新颖性——方面的能力仍鲜有探索。本文通过将一个以代码为导向的LLM置于闭环合成框架中，经过22轮受监督的微调循环，系统分析了其演化过程。该模型生成PyTorch卷积网络，经验证后通过低保真度性能信号（单轮次准确率）进行评估，并利用MinHash-Jaccard准则过滤冗余结构，以避免重复。高性能且具有创新性的架构被转化为提示-代码对，用于通过参数高效LoRA适配技术进行迭代微调，初始权重源自LEMUR数据集。在多个微调周期中，LLM逐步内化了经验性的架构先验知识，成为稳健的架构生成器：有效生成率稳定在50.6%（峰值达74.5%），平均首轮准确率从28.06%提升至50.99%，而准确率超过40%的候选架构比例则从2.04%大幅增长至96.81%。分析结果表明，该模型已超越对现有模式的简单复制，成功生成了455个原始语料库中不存在的高性能架构。本研究通过将代码合成与执行反馈相结合，提供了一种可扩展的范式，推动随机生成器向自主、性能驱动的神经网络设计者演进，证实了LLM能够内化非文本化的经验性奖励信号，从而突破其训练数据的局限。"
  },
  {
    "date": "2026-01-06",
    "title": "Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning",
    "authors": "Xinglang Zhang, Yunyao Zhang, ZeLiang Chen, Junqing Yu, Wei Yang, Zikai Song",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02902v1",
    "source": "arXiv",
    "abstract": "Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.",
    "title_zh": "逻辑相变：理解大语言模型逻辑推理中的崩溃现象",
    "abstract_zh": "符号逻辑推理是大型语言模型（LLMs）一项关键但尚未充分探索的能力，它在数学推理、法律判决等高风险领域中提供了可靠且可验证的决策支持。在本研究中，我们系统地分析了在逻辑复杂度受控增加条件下的逻辑推理表现，揭示了一种此前未被认识的现象，我们称之为“逻辑相变”：逻辑推理性能并非平滑退化，而是在某一特定逻辑深度范围内保持稳定，一旦超过临界深度则会突然崩溃，这一现象类似于物理中的相变过程（如水在临界温度以下结冰）。基于这一发现，我们提出了“神经符号课程调优”（Neuro-Symbolic Curriculum Tuning）框架，该框架通过自适应地将自然语言与逻辑符号对齐，建立共享表示，并围绕相变边界重塑训练动态，从而逐步增强模型在更高逻辑深度下的推理能力。在五个基准测试上的实验表明，我们的方法有效缓解了高复杂度下逻辑推理的崩溃问题，在零样本提示（naive prompting）中平均准确率提升+1.26，在思维链（CoT）提示中提升+3.95，同时显著提升了对未见逻辑组合的泛化能力。代码与数据已公开，详见 https://github.com/AI4SS/Logical-Phase-Transitions。"
  },
  {
    "date": "2026-01-06",
    "title": "MiMo-V2-Flash Technical Report",
    "authors": "Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, Gang Xie, Hailin Zhang, Hanglong Lv, Hanyu Li, Heyu Chen, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Lei Li, Liang Zhao, Linghao Zhang, Peidian Li, Qianli Chen, Shaohui Liu, Shihua Yu, Shijie Cao, Shimao Chen, Shouqiu Yu, Shuo Liu, Tianling Zhou, Weijiang Su, Weikun Wang, Wenhan Ma, Xiangwei Deng, Bohan Mao, Bowen Ye, Can Cai, Chenghua Wang, Chengxuan Zhu, Chong Ma, Chun Chen, Chunan Li, Dawei Zhu, Deshan Xiao, Dong Zhang, Duo Zhang, Fangyue Liu, Feiyu Yang, Fengyuan Shi, Guoan Wang, Hao Tian, Hao Wu, Heng Qu, Hongfei Yi, Hongxu An, Hongyi Guan, Xing Zhang, Yifan Song, Yihan Yan, Yihao Zhao, Yingchun Lai, Yizhao Gao, Yu Cheng, Yuanyuan Tian, Yudong Wang, Zhen Tang, Zhengju Tang, Zhengtao Wen, Zhichao Song, Zhixian Zheng, Zihan Jiang, Jian Wen, Jiarui Sun, Jiawei Li, Jinlong Xue, Jun Xia, Kai Fang, Menghang Zhu, Nuo Chen, Qian Tu, Qihao Zhang, Qiying Wang, Rang Li, Rui Ma, Shaolei Zhang, Shengfan Wang, Shicheng Li, Shuhao Gu, Shuhuai Ren, Sirui Deng, Tao Guo, Tianyang Lu, Weiji Zhuang, Weikang Zhang, Weimin Xiong, Wenshan Huang, Wenyu Yang, Xin Zhang, Xing Yong, Xu Wang, Xueyang Xie, Yilin Jiang, Yixin Yang, Yongzhe He, Yu Tu, Yuanliang Dong, Yuchen Liu, Yue Ma, Yue Yu, Yuxing Xiang, Zhaojun Huang, Zhenru Lin, Zhipeng Xu, Zhiyang Chen, Zhonghua Deng, Zihan Zhang, Zihao Yue",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02780v1",
    "source": "arXiv",
    "abstract": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.",
    "title_zh": "MiMo-V2-Flash 技术报告",
    "abstract_zh": "我们推出 MiMo-V2-Flash，这是一个拥有 3090 亿总参数、仅激活 150 亿参数的混合专家（MoE）模型，专为快速、强大的推理与智能体能力而设计。MiMo-V2-Flash 采用了一种混合注意力架构，将滑动窗口注意力（SWA）与全局注意力交错使用，滑动窗口长度为 128 个 token，混合比例为 5:1。该模型在 27 万亿 token 的数据上进行预训练，采用多 token 预测（MTP）目标，并原生支持 32k 上下文长度，随后扩展至 256k。为高效扩展后训练计算资源，MiMo-V2-Flash 引入了一种创新的“多教师在线蒸馏”（MOPD）范式：由领域专业化教师（例如通过大规模强化学习训练而成）提供密集且逐 token 的奖励信号，使学生模型能够完美掌握教师的知识专长。尽管仅使用 DeepSeek-V3.2 和 Kimi-K2 模型总参数量的 1/2 和 1/3，MiMo-V2-Flash 在性能上已可媲美顶级开源权重模型。在推理阶段，通过将 MTP 用作推测解码的草稿模型，MiMo-V2-Flash 实现了最高达 3.6 的接受长度和 2.6 倍的解码加速，仅需三层 MTP 结构即可达成。我们已开源模型权重及三层 MTP 权重，以促进开放研究与社区协作。"
  },
  {
    "date": "2026-01-06",
    "title": "Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization",
    "authors": "Dongyu Chen, Jian Ma, Xianpeng Zhang, Lei Zhang, Haonan Lu, Chen Chen, Chuangchuang Wang, Kai Tang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02683v1",
    "source": "arXiv",
    "abstract": "Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.",
    "title_zh": "从提示本身学习：层次化属性提示优化",
    "abstract_zh": "优化在众多学科中都具有基础性作用，通常遵循一种迭代过程：通过不断改进初始方案来提升性能。这一原则在提示工程（prompt engineering）中同样至关重要，因为为大型语言模型设计有效提示本身就是一个复杂的优化挑战。要实现结构化的优化方法，需要采用自动化或半自动化的流程来生成更优的提示，从而减少人工投入、提升性能，并获得可解释的过程。然而，当前的提示优化方法常常导致“提示漂移”（prompt drift）问题——新提示虽然修复了先前的缺陷，却损害了对以往成功任务的表现。此外，从零开始生成提示也可能牺牲可解释性。为解决这些局限，本文提出了一种分层归因提示优化（Hierarchical Attribution Prompt Optimization, HAPO）框架，引入三项创新：（1）动态归因机制，用于识别训练数据和提示历史中的错误模式；（2）语义单元优化，针对提示中功能性的片段进行精准编辑；（3）支持多模态友好演进，兼容端到端大语言模型（LLM）以及 LLM-MLLM 融合工作流。HAPO 在单图/多图问答（如 OCRV2）和复杂任务分析（如 BBH）等场景中得到应用，展现出更高的优化效率，优于现有的同类自动化提示优化方法，并建立了一个可扩展的范式，为大规模提示工程提供了有力支持。"
  },
  {
    "date": "2026-01-06",
    "title": "TAAF: A Trace Abstraction and Analysis Framework Synergizing Knowledge Graphs and LLMs",
    "authors": "Alireza Ezaz, Ghazal Khodabandeh, Majid Babaei, Naser Ezzati-Jivan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02632v1",
    "source": "arXiv",
    "abstract": "Execution traces are a critical source of information for understanding, debugging, and optimizing complex software systems. However, traces from OS kernels or large-scale applications like Chrome or MySQL are massive and difficult to analyze. Existing tools rely on predefined analyses, and custom insights often require writing domain-specific scripts, which is an error-prone and time-consuming task. This paper introduces TAAF (Trace Abstraction and Analysis Framework), a novel approach that combines time-indexing, knowledge graphs (KGs), and large language models (LLMs) to transform raw trace data into actionable insights. TAAF constructs a time-indexed KG from trace events to capture relationships among entities such as threads, CPUs, and system resources. An LLM then interprets query-specific subgraphs to answer natural-language questions, reducing the need for manual inspection and deep system expertise. To evaluate TAAF, we introduce TraceQA-100, a benchmark of 100 questions grounded in real kernel traces. Experiments across three LLMs and multiple temporal settings show that TAAF improves answer accuracy by up to 31.2%, particularly in multi-hop and causal reasoning tasks. We further analyze where graph-grounded reasoning helps and where limitations remain, offering a foundation for next-generation trace analysis tools.",
    "title_zh": "TAAF：一种融合知识图谱与大语言模型的追踪抽象与分析框架",
    "abstract_zh": "执行轨迹是理解、调试和优化复杂软件系统的关键信息来源。然而，操作系统内核或大型应用（如 Chrome 或 MySQL）产生的轨迹数据量巨大，难以分析。现有工具依赖预定义的分析方法，而自定义洞察通常需要编写领域特定的脚本，这一过程既容易出错又耗时。本文提出 TAAF（轨迹抽象与分析框架），一种结合时间索引、知识图谱（KG）和大语言模型（LLMs）的新方法，将原始轨迹数据转化为可操作的洞见。TAAF 从轨迹事件构建时间索引的知识图谱，以捕捉线程、CPU 和系统资源等实体之间的关系。随后，大语言模型对查询相关的子图进行解读，从而回答自然语言问题，显著减少对人工检查和深层系统知识的需求。为评估 TAAF，我们引入了 TraceQA-100——一个基于真实内核轨迹的 100 个问题基准测试集。在三个大语言模型及多种时间场景下的实验表明，TAAF 在答案准确率上最高提升达 31.2%，尤其在多跳推理和因果推理任务中表现突出。我们进一步分析了图谱驱动推理的优势所在以及当前存在的局限性，为下一代轨迹分析工具的发展奠定了基础。"
  },
  {
    "date": "2026-01-06",
    "title": "LAsset: An LLM-assisted Security Asset Identification Framework for System-on-Chip (SoC) Verification",
    "authors": "Md Ajoad Hasan, Dipayan Saha, Khan Thamid Hasan, Nashmin Alam, Azim Uddin, Sujan Kumar Saha, Mark Tehranipoor, Farimah Farahmandi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02624v1",
    "source": "arXiv",
    "abstract": "The growing complexity of modern system-on-chip (SoC) and IP designs is making security assurance difficult day by day. One of the fundamental steps in the pre-silicon security verification of a hardware design is the identification of security assets, as it substantially influences downstream security verification tasks, such as threat modeling, security property generation, and vulnerability detection. Traditionally, assets are determined manually by security experts, requiring significant time and expertise. To address this challenge, we present LAsset, a novel automated framework that leverages large language models (LLMs) to identify security assets from both hardware design specifications and register-transfer level (RTL) descriptions. The framework performs structural and semantic analysis to identify intra-module primary and secondary assets and derives inter-module relationships to systematically characterize security dependencies at the design level. Experimental results show that the proposed framework achieves high classification accuracy, reaching up to 90% recall rate in SoC design, and 93% recall rate in IP designs. This automation in asset identification significantly reduces manual overhead and supports a scalable path forward for secure hardware development.",
    "title_zh": "LAsset：一种基于大语言模型的片上系统（SoC）验证安全资产识别框架",
    "abstract_zh": "现代系统级芯片（SoC）和知识产权（IP）设计的复杂性日益增加，使得安全保证变得越来越困难。在硬件设计的预硅阶段安全验证中，识别安全资产是基础性步骤之一，因为它对后续的安全验证任务（如威胁建模、安全属性生成和漏洞检测）具有重大影响。传统上，安全资产由安全专家手动确定，需要耗费大量时间和专业知识。为应对这一挑战，我们提出了LAsset——一种新颖的自动化框架，该框架利用大语言模型（LLM）从硬件设计规格说明和寄存器传输级（RTL）描述中自动识别安全资产。该框架通过结构分析与语义分析，识别模块内部的主要和次要资产，并推导出模块间的关联关系，从而系统化地刻画设计层面的安全依赖关系。实验结果表明，所提出的框架在SoC设计中达到了高达90%的召回率，在IP设计中更是达到93%的召回率。该资产识别过程的自动化显著降低了人工工作量，为安全硬件开发提供了一条可扩展的发展路径。"
  },
  {
    "date": "2026-01-06",
    "title": "UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward",
    "authors": "Yile Liu, Yixian Liu, Zongwei Li, Yufei Huang, Xinhua Feng, Zhichao Hu, Jinglu Hu, Jianfeng Yan, Fengzong Lian, Yuhong Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03205v1",
    "source": "arXiv",
    "abstract": "While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.",
    "title_zh": "UltraLogic：通过大规模数据合成与双极浮点奖励增强大语言模型的推理能力",
    "abstract_zh": "尽管大型语言模型（LLMs）在自然语言处理领域展现出巨大潜力，但涉及多步逻辑推理、规划与验证的复杂通用推理任务仍面临关键瓶颈。虽然基于可验证奖励的强化学习（RLVR）已在特定领域取得成功，但该领域仍缺乏大规模、高质量且难度分级明确的通用推理数据。为解决这一问题，我们提出 UltraLogic 框架，通过基于代码的求解方法，将问题的逻辑核心与其自然语言表达相分离，从而实现高质量数据的自动化生成。该框架包含数百种独特的任务类型，并具备覆盖十个难度等级的自动化校准流程。此外，为缓解二元奖励稀疏性及“非负奖励陷阱”问题，我们引入了双极浮点奖励（BFR）机制，采用分级惩罚策略，有效区分完全正确的回答与存在逻辑缺陷的回答。实验结果表明，任务多样性是提升推理能力的主要驱动力；而结合难度匹配策略的 BFR 机制显著提升了训练效率，引导模型趋向全局逻辑最优解。"
  },
  {
    "date": "2026-01-06",
    "title": "Reducing Hallucinations in LLMs via Factuality-Aware Preference Learning",
    "authors": "Sindhuja Chaduvula, Ahmed Y. Radwan, Azib Farooq, Yani Ioannou, Shaina Raza",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03027v1",
    "source": "arXiv",
    "abstract": "Preference alignment methods such as RLHF and Direct Preference Optimization (DPO) improve instruction following, but they can also reinforce hallucinations when preference judgments reward fluency and confidence over factual correctness. We introduce F-DPO (Factuality-aware Direct Preference Optimization), a simple extension of DPO that uses only binary factuality labels. F-DPO (i) applies a label-flipping transformation that corrects misordered preference pairs so the chosen response is never less factual than the rejected one, and (ii) adds a factuality-aware margin that emphasizes pairs with clear correctness differences, while reducing to standard DPO when both responses share the same factuality. We construct factuality-aware preference data by augmenting DPO pairs with binary factuality indicators and synthetic hallucinated variants. Across seven open-weight LLMs (1B-14B), F-DPO consistently improves factuality and reduces hallucination rates relative to both base models and standard DPO. On Qwen3-8B, F-DPO reduces hallucination rates by five times (from 0.424 to 0.084) while improving factuality scores by 50 percent (from 5.26 to 7.90). F-DPO also generalizes to out-of-distribution benchmarks: on TruthfulQA, Qwen2.5-14B achieves plus 17 percent MC1 accuracy (0.500 to 0.585) and plus 49 percent MC2 accuracy (0.357 to 0.531). F-DPO requires no auxiliary reward model, token-level annotations, or multi-stage training.",
    "title_zh": "通过事实感知的偏好学习减少大语言模型中的幻觉",
    "abstract_zh": "偏好对齐方法如基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO）能够提升模型对指令的理解与遵循能力，但当偏好判断更倾向于奖励流畅性和自信表达而非事实正确性时，也可能加剧幻觉现象。为此，我们提出F-DPO（事实性感知的直接偏好优化），这是对DPO的一种简单扩展，仅依赖二元事实性标签。F-DPO具有两个核心机制：（i）引入标签翻转变换，修正顺序错误的偏好对，确保被选择的回答在事实性上从不劣于被拒绝的回答；（ii）引入事实性感知的间隔机制，突出强调事实正确性差异明显的样本，而在两个回答具有相同事实性时则退化为标准DPO。我们通过在原始DPO样本中添加二元事实性标识以及合成的幻觉变体，构建了具备事实性感知的偏好数据集。在七个开源大模型（1B–14B参数量）上的实验表明，F-DPO在保持或优于基线模型和标准DPO的基础上，持续提升了事实性表现并显著降低了幻觉率。以Qwen3-8B为例，F-DPO将幻觉率降低至原来的五分之一（从0.424降至0.084），同时使事实性评分提升50%（从5.26升至7.90）。此外，F-DPO在分布外基准测试中也展现出良好的泛化能力：在TruthfulQA任务上，Qwen2.5-14B的MC1准确率提升17%（从0.500增至0.585），MC2准确率提升49%（从0.357增至0.531）。值得注意的是，F-DPO无需额外的奖励模型、逐标记注释或多阶段训练流程，实现高效且可推广的事实性增强。"
  },
  {
    "date": "2026-01-06",
    "title": "Hardness of Regular Expression Matching with Extensions",
    "authors": "Taisei Nogami, Tachio Terauchi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03020v1",
    "source": "arXiv",
    "abstract": "The regular expression matching problem asks whether a given regular expression of length $m$ matches a given string of length $n$. As is well known, the problem can be solved in $O(nm)$ time using Thompson's algorithm. Moreover, recent studies have shown that the matching problem for regular expressions extended with a practical extension called lookaround can be solved in the same time complexity. In this work, we consider three well-known extensions to regular expressions called backreference, intersection and complement, and we show that, unlike in the case of lookaround, the matching problem for regular expressions extended with any of the three (for backreference, even when restricted to one capturing group) cannot be solved in $O(n^{2-\\varepsilon} \\mathrm{poly}(m))$ time for any constant $\\varepsilon > 0$ under the Orthogonal Vectors Conjecture. Moreover, we study the matching problem for regular expressions extended with complement in more detail, which is also known as extended regular expression (ERE) matching. We show that there is no ERE matching algorithm that runs in $O(n^{ω-\\varepsilon} \\mathrm{poly}(m))$ time ($2 \\le ω< 2.3716$ is the exponent of square matrix multiplication) for any constant $\\varepsilon > 0$ under the $k$-Clique Hypothesis, and there is no combinatorial ERE matching algorithm that runs in $O(n^{3-\\varepsilon} \\mathrm{poly}(m))$ time for any constant $\\varepsilon > 0$ under the Combinatorial $k$-Clique Hypothesis. This shows that the $O(n^3 m)$-time algorithm introduced by Hopcroft and Ullman in 1979 and recently improved by Bille et al. to run in $O(n^ωm)$ time using fast matrix multiplication was already optimal in a sense, and sheds light on why the theoretical computer science community has struggled to improve the time complexity of ERE matching with respect to $n$ and $m$ for more than 45 years.",
    "title_zh": "正则表达式匹配的复杂性（含扩展功能）",
    "abstract_zh": "正则表达式匹配问题要求判断一个长度为 $m$ 的正则表达式是否匹配一个长度为 $n$ 的字符串。众所周知，该问题可通过 Thompson 算法在 $O(nm)$ 时间内解决。此外，近期研究已表明，对于引入了一种实用扩展——“环视”（lookaround）的正则表达式，其匹配问题同样可在相同的时间复杂度下求解。\n\n在本文中，我们研究了正则表达式的三种经典扩展：**反向引用**（backreference）、**交集**（intersection）和**补集**（complement）。我们证明，与环视的情况不同，对于任意一种上述扩展（即使反向引用仅限于单个捕获组），其匹配问题无法在 $O(n^{2-\\varepsilon} \\mathrm{poly}(m))$ 时间内求解，其中 $\\varepsilon > 0$ 为任意常数，该结论基于**正交向量猜想**（Orthogonal Vectors Conjecture）。\n\n此外，我们对引入补集的正则表达式匹配问题进行了更深入的研究，该问题也被称为**扩展正则表达式**（Extended Regular Expression, ERE）匹配。我们证明，在** $k$-团假设**（$k$-Clique Hypothesis）下，不存在运行时间为 $O(n^{ω-\\varepsilon} \\mathrm{poly}(m))$ 的 ERE 匹配算法，其中 $2 \\le ω < 2.3716$ 是平方矩阵乘法的指数，$\\varepsilon > 0$ 为任意常数；同时，在**组合 $k$-团假设**（Combinatorial $k$-Clique Hypothesis）下，也不存在运行时间为 $O(n^{3-\\varepsilon} \\mathrm{poly}(m))$ 的组合型 ERE 匹配算法，其中 $\\varepsilon > 0$ 为任意常数。\n\n这些结果表明，Hopcroft 和 Ullman 于 1979 年提出的 $O(n^3 m)$ 时间算法，以及后来 Bille 等人通过快速矩阵乘法改进至 $O(n^ω m)$ 时间的算法，实际上在某种意义上已经是最优的。这一发现揭示了为何理论计算机科学界在过去四十五多年中始终难以在 $n$ 和 $m$ 的时间复杂度上进一步突破 ERE 匹配问题的性能瓶颈。"
  },
  {
    "date": "2026-01-06",
    "title": "ATLAS: Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning",
    "authors": "Tuc Nguyen, Thai Le",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03093v1",
    "source": "arXiv",
    "abstract": "Recent work on activation and latent steering has demonstrated that modifying internal representations can effectively guide large language models (LLMs) toward improved reasoning and efficiency without additional training. However, most existing approaches rely on fixed steering policies and static intervention strengths, which limit their robustness across problem instances and often result in over- or under-steering. We propose Adaptive Test-time Latent Steering, called (ATLAS), a task-specific framework that dynamically controls steering decisions at inference time using an external, lightweight latent verifier. Given intermediate hidden states, the verifier predicts the quality of ongoing reasoning and adaptively selects whether and how strongly to apply steering, enabling per-example and per-step adjustment with minimal overhead. To our knowledge, ATLAS is the first method to integrate learned latent verification into test-time steering for enhancing LLMs reasoning. Experiments on multiple mathematical reasoning benchmarks show that ATLAS consistently outperforms both vanilla decoding and fixed steering baselines, achieving higher accuracy while substantially reducing test-time token usage. These results demonstrate that verifier-guided latent adaptation provides an effective and scalable mechanism for controlling reasoning efficiency without sacrificing solution quality. All source code will be publicly available.",
    "title_zh": "ATLAS：基于外部验证器的自适应测试时潜在空间引导，以增强大语言模型的推理能力",
    "abstract_zh": "近期关于激活值与潜在空间操控的研究表明，通过修改模型内部表示，可以在不进行额外训练的情况下有效引导大型语言模型（LLMs）提升推理能力与效率。然而，大多数现有方法依赖于固定的操控策略和静态的干预强度，这限制了其在不同问题实例上的鲁棒性，常导致过度操控或操控不足。为此，我们提出了一种自适应的测试时潜在空间操控框架——ATLAS（Adaptive Test-time Latent Steering），该方法在推理阶段动态调控操控决策，利用一个外部、轻量级的潜在验证器来实现。给定中间隐藏状态后，验证器会预测当前推理的质量，并自适应地决定是否以及以多强的力度施加操控，从而实现针对每个样本、每一步的精细调整，且开销极小。据我们所知，ATLAS是首个将可学习的潜在验证机制融入测试时操控中以增强LLM推理能力的方法。在多个数学推理基准上的实验结果表明，ATLAS在准确率方面持续优于原始解码和固定操控基线方法，同时显著降低了测试阶段的token消耗。这些结果证明，由验证器引导的潜在空间自适应是一种高效且可扩展的推理效率控制机制，无需牺牲解题质量。所有源代码将公开发布。"
  },
  {
    "date": "2026-01-06",
    "title": "Backwards Data-Flow Analysis using Prophecy Variable in the BuildIt System",
    "authors": "Ajay Brahmakshatriya, Saman Amarasinghe, Martin Rinard",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02653v1",
    "source": "arXiv",
    "abstract": "Many program transformations and optimizations require information about the future behavior of the program. A standard way to obtain this information is to build an intermediate program representation, then use a backwards program analysis to propagate relevant information against the flow of control back to the transformation/optimization site. We instead propose to use prophecy variables, which predict information about the future execution of the program, to enable such transformations and optimizations. We implement prophecy variables in BuildIt, a lightweight domain specific language implementation system. BuildIt uses staged compilation to implement high performance domain specific languages embedded within a standard general purpose programming language (C++). The BuildIt first phase uses standard C++ program execution to generate optimized C, C++, and CUDA second phase code. This approach enables BuildIt to eliminate programming language implementation components such as parsers and intermediate representations, delivering a dramatic decrease in the engineering effort required to implement domain specific languages. The combination of prophecy variables and repeated forward program execution enables BuildIt to extend this approach to include transformations and optimizations that require information about the future execution of the program without backwards analyses and without the engineering overhead associated with implementing these analyses. We formalize the use of prophecy variables for this purpose, discuss the implementation of prophecy variables and repeated execution in BuildIt, and present experimental results for BuildIt computations that benefit from optimizations enabled by the information that prophecy variables provide.",
    "title_zh": "在BuildIt系统中使用预言变量进行逆向数据流分析",
    "abstract_zh": "许多程序变换和优化都需要了解程序未来的执行行为。传统方法是构建一个中间程序表示，然后通过反向程序分析，将相关的信息沿着控制流反向传播至需要进行变换或优化的位置。我们提出了一种新方法：使用预言变量（prophecy variables），这些变量能够预测程序未来执行时的信息，从而支持此类变换与优化。我们在 BuildIt——一个轻量级领域特定语言实现系统中实现了预言变量。BuildIt 采用分阶段编译技术，用于在标准通用编程语言（C++）中嵌入高性能的领域特定语言。BuildIt 的第一阶段利用标准 C++ 程序执行生成经过优化的 C、C++ 和 CUDA 第二阶段代码。这种方法使 BuildIt 能够省去编程语言实现中的诸多组件，如解析器和中间表示，从而大幅降低实现领域特定语言所需的工程工作量。通过将预言变量与多次前向程序执行相结合，BuildIt 能够在不依赖反向分析、也不承担实现这些分析所需额外工程开销的前提下，扩展其应用范围，涵盖那些需要未来执行信息的变换与优化。本文形式化地阐述了预言变量在此目的下的使用方法，讨论了预言变量及重复执行在 BuildIt 中的实现机制，并展示了若干实验结果，验证了 BuildIt 计算任务在预言变量提供的信息支持下所获得的优化收益。"
  },
  {
    "date": "2026-01-06",
    "title": "P-Check: Advancing Personalized Reward Model via Learning to Generate Dynamic Checklist",
    "authors": "Kwangwook Seo, Dongha Lee",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02986v1",
    "source": "arXiv",
    "abstract": "Recent approaches in personalized reward modeling have primarily focused on leveraging user interaction history to align model judgments with individual preferences. However, existing approaches largely treat user context as a static or implicit conditioning signal, failing to capture the dynamic and multi-faceted nature of human judgment. In this paper, we propose P-Check, a novel personalized reward modeling framework, designed to train a plug-and-play checklist generator that synthesizes dynamic evaluation criteria for guiding the reward prediction. To better align these checklists with personalized nuances, we introduce Preference-Contrastive Criterion Weighting, a training strategy that assigns saliency scores to criteria based on their discriminative power for personalized judgment. We conduct extensive experiments and demonstrate that P-Check not only improves reward accuracy but also enhances downstream personalized generation, and remains robust in OOD scenarios.",
    "title_zh": "P-Check：通过学习生成动态检查清单以推进个性化奖励模型",
    "abstract_zh": "近年来，个性化奖励建模的主流方法主要依赖用户交互历史来使模型判断与个体偏好对齐。然而，现有方法大多将用户上下文视为静态或隐式的条件信号，未能捕捉人类判断中动态且多维度的特性。本文提出P-Check，一种新颖的个性化奖励建模框架，旨在训练一个即插即用的检查清单生成器，以动态合成评估标准，从而指导奖励预测。为更好地使这些检查清单契合个性化细微差异，我们引入了“偏好对比准则加权”（Preference-Contrastive Criterion Weighting）训练策略，根据各项准则在个性化判断中的区分能力为其分配显著性得分。通过大量实验验证，我们发现P-Check不仅提升了奖励预测的准确性，还增强了下游个性化生成效果，并在分布外（OOD）场景下表现出良好的鲁棒性。"
  },
  {
    "date": "2026-01-06",
    "title": "Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders",
    "authors": "Ruikang Zhang, Shuo Wang, Qi Su",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02978v1",
    "source": "arXiv",
    "abstract": "Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors. Our method employs a contrastive feature retrieval pipeline based on controlled semantic oppositions, combing statistical activation analysis and generation-based validation to distill monosemantic functional features from sparse activation spaces. Using the Big Five personality traits as a case study, we demonstrate that our method enables precise, bidirectional steering of model behavior while maintaining superior stability and performance compared to existing activation steering methods like Contrastive Activation Addition (CAA). We further identify an empirical effect, which we term Functional Faithfulness, whereby intervening on a specific internal feature induces coherent and predictable shifts across multiple linguistic dimensions aligned with the target semantic attribute. Our findings suggest that LLMs internalize deeply integrated representations of high-order concepts, and provide a novel, robust mechanistic path for the regulation of complex AI behaviors.",
    "title_zh": "大型语言模型中的机制旋钮：通过稀疏自编码器检索与引导高阶语义特征",
    "abstract_zh": "近期在机制可解释性（Mechanistic Interpretability, MI）领域的研究，使得识别并干预大型语言模型（LLMs）中的内部特征成为可能。然而，一个持续存在的挑战在于如何将这些内部特征与语言生成中复杂、行为层面的语义属性进行可靠关联和控制。本文提出了一种基于稀疏自编码器（Sparse Autoencoder）的框架，用于检索和引导与高层语言行为相关的语义可解释内部特征。我们的方法采用基于受控语义对立的对比特征检索流程，结合统计激活分析与生成式验证，从稀疏激活空间中提炼出具有单一语义功能的内部特征。以“大五人格特质”为例，我们证明该方法能够在保持显著稳定性与性能优势的同时，实现对模型行为的精确、双向调控，优于现有的激活调控方法（如对比激活添加，Contrastive Activation Addition, CAA）。此外，我们发现了一种经验性现象，称之为“功能忠实性”（Functional Faithfulness）：对某一特定内部特征进行干预，会引发多个与目标语义属性一致的语言维度上协调且可预测的转变。这一发现表明，LLMs 内部蕴含着高度整合的高阶概念表征，并为复杂人工智能行为的调控提供了一条新颖而稳健的机制化路径。"
  },
  {
    "date": "2026-01-06",
    "title": "Beyond the Black Box: Theory and Mechanism of Large Language Models",
    "authors": "Zeyu Gan, Ruifeng Ren, Wei Yao, Xiaolin Hu, Gengze Xu, Chen Qian, Huayi Tang, Zixuan Gong, Xinhao Yao, Pengwei Tang, Zhenxing Dou, Yong Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02907v1",
    "source": "arXiv",
    "abstract": "The rapid emergence of Large Language Models (LLMs) has precipitated a profound paradigm shift in Artificial Intelligence, delivering monumental engineering successes that increasingly impact modern society. However, a critical paradox persists within the current field: despite the empirical efficacy, our theoretical understanding of LLMs remains disproportionately nascent, forcing these systems to be treated largely as ``black boxes''. To address this theoretical fragmentation, this survey proposes a unified lifecycle-based taxonomy that organizes the research landscape into six distinct stages: Data Preparation, Model Preparation, Training, Alignment, Inference, and Evaluation. Within this framework, we provide a systematic review of the foundational theories and internal mechanisms driving LLM performance. Specifically, we analyze core theoretical issues such as the mathematical justification for data mixtures, the representational limits of various architectures, and the optimization dynamics of alignment algorithms. Moving beyond current best practices, we identify critical frontier challenges, including the theoretical limits of synthetic data self-improvement, the mathematical bounds of safety guarantees, and the mechanistic origins of emergent intelligence. By connecting empirical observations with rigorous scientific inquiry, this work provides a structured roadmap for transitioning LLM development from engineering heuristics toward a principled scientific discipline.",
    "title_zh": "超越黑箱：大语言模型的理论与机制",
    "abstract_zh": "大型语言模型（LLMs）的迅速兴起，推动了人工智能领域的一场深刻范式变革，带来了重大的工程成就，并日益深刻地影响着现代社会。然而，当前该领域仍存在一个关键矛盾：尽管实证效果显著，我们对LLMs的理论理解却仍极为薄弱，导致这些系统在很大程度上仍被视为“黑箱”。为解决这一理论上的碎片化问题，本文提出了一种基于生命周期的统一分类框架，将研究格局划分为六个明确阶段：数据准备、模型准备、训练、对齐、推理和评估。在此框架下，我们系统性地回顾了驱动LLM性能的基础理论与内部机制。具体而言，我们分析了若干核心理论问题，包括数据混合的数学合理性、不同架构的表征能力局限，以及对齐算法的优化动态。超越现有的最佳实践，本文还识别出一系列关键前沿挑战，如合成数据自我改进的理论极限、安全保证的数学边界，以及涌现智能的机制根源。通过将经验观察与严谨的科学探究相连接，本研究为LLM的发展提供了一个结构化的路线图，旨在推动其从工程启发式方法向有原则的科学学科转型。"
  },
  {
    "date": "2026-01-06",
    "title": "PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback",
    "authors": "Dehao Yuan, Tyler Farnan, Stefan Tesliuc, Doron L Bergman, Yulun Wu, Xiaoyu Liu, Minghui Liu, James Montgomery, Nam H Nguyen, C. Bayan Bruss, Furong Huang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03149v1",
    "source": "arXiv",
    "abstract": "Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware \"nextprompt\" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.",
    "title_zh": "PersonaLedger：基于人物角色条件化大模型与规则驱动反馈生成真实金融交易",
    "abstract_zh": "严格的隐私法规限制了真实交易数据的访问，阻碍了金融人工智能领域的开放研究。合成数据可以弥合这一差距，但现有的生成方法难以同时实现行为多样性与逻辑一致性。基于规则的模拟器依赖手工设计的工作流程和浅层随机性，无法体现人类行为的丰富性；而基于学习的生成方法（如GAN）虽能捕捉数据相关性，却常常违反硬性金融约束，且仍需在私有数据上进行训练。我们提出PersonaLedger——一种生成引擎，利用大语言模型（LLM）结合丰富的用户画像来生成多样化的交易流，并通过一个可由专家配置的程序化引擎确保生成结果的正确性。LLM与引擎在闭环中协同工作：每次事件发生后，引擎会更新用户状态、强制执行金融规则，并返回一个上下文感知的“下一步提示”（nextprompt），引导LLM生成可行的后续操作。借助该引擎，我们构建了一个包含23,000名用户、3000万条交易记录的公开数据集，以及涵盖两项任务的基准测试套件：流动性不足分类与身份盗用分割。PersonaLedger提供了一种真实、隐私保护的资源，支持对预测与异常检测模型的严格评估。该系统为社区提供了丰富、真实且隐私安全的研究资源——完整包含代码、规则和生成日志——以加速金融AI领域的创新，并推动严谨、可复现的评估研究。"
  },
  {
    "date": "2026-01-06",
    "title": "Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning",
    "authors": "Nathanaël Carraz Rakotonirina, Ren Pang, Neha Anna John, Michael Bohlke-Schneider, Momchil Hardalov",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02972v1",
    "source": "arXiv",
    "abstract": "The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy-response length trade-off. Our approach reduces response length by an average of 28\\% for 8B models and 40\\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\\text{AUC}_{\\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.",
    "title_zh": "多阶段训练实现自适应推理",
    "abstract_zh": "通过增加测试时计算量，大型语言模型（LLMs）的推理能力得到了显著提升，通常表现为中间生成的思维链（Chain-of-Thought, CoT） token。然而，CoT 经常变得过于冗长，导致计算成本上升，却并未带来实际的准确率提升，甚至可能降低性能，这种现象被称为“过度思考”（overthinking）。我们提出了一种多阶段高效的推理方法，结合监督微调（通过拒绝采样或推理轨迹重格式化）与强化学习，并引入自适应长度惩罚机制。我们设计了一种轻量级奖励函数，对首次正确答案之后生成的 token 进行惩罚，同时仅在自我验证有益时才鼓励其进行。我们在七个多样化的推理任务上进行了全面评估，分析了准确率与响应长度之间的权衡关系。实验结果表明，该方法使 8B 模型的响应长度平均减少 28%，32B 模型则减少 40%，同时性能下降仅分别为 1.6 和 2.5 个点。尽管方法概念简单，但其在效率与准确率平衡方面表现优于更复杂的先进高效推理方法，在“经过过度思考修正的准确率曲线下面积”（$\\text{AUC}_{\\text{OAA}}$）指标上达到 76.6，比基线模型高出 5 分，比第二优的方法高出 2.5 分。"
  },
  {
    "date": "2026-01-06",
    "title": "SastBench: A Benchmark for Testing Agentic SAST Triage",
    "authors": "Jake Feiglin, Guy Dar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02941v1",
    "source": "arXiv",
    "abstract": "SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.",
    "title_zh": "SastBench：用于测试智能型SAST分类的基准",
    "abstract_zh": "SAST（静态应用安全测试）工具是防御性网络安全中使用最广泛的手段之一，被商业和非商业组织广泛用于识别软件中的潜在漏洞。尽管其具有巨大价值，但SAST工具会产生大量误报，需要耗费高昂成本进行手动筛选（即“归因分析”）。虽然基于大语言模型（LLM）的智能体在自动化网络安全任务方面展现出巨大潜力，但现有的评估基准无法真实模拟现实世界中SAST检测结果的分布情况。为此，我们提出了SastBench——一个用于评估SAST归因智能体的基准测试平台。该平台将真实的CVE（通用漏洞披露）作为真阳性样本，同时结合经过过滤的SAST工具检测结果作为近似假阳性样本。SastBench采用与智能体无关的设计理念，可兼容多种类型智能体。我们在该基准上对多种智能体进行了评估，并对其性能进行了对比分析；同时对数据集进行了详尽剖析，并探讨了对未来开发工作的启示。"
  },
  {
    "date": "2026-01-06",
    "title": "DIP: Dynamic In-Context Planner For Diffusion Language Models",
    "authors": "Yang Li, Han Meng, Chenan Wang, Haipeng Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03199v1",
    "source": "arXiv",
    "abstract": "Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows \\textit{efficient dynamic adjustment of the context} during generation. Building on this insight, we propose \\textbf{D}ynamic \\textbf{I}n-Context \\textbf{P}lanner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront. Results show DIP maintains generation quality while achieving up to 12.9$\\times$ inference speedup over standard inference and 1.17$\\times$ over KV cache-enhanced inference.",
    "title_zh": "DIP：用于扩散语言模型的动态上下文规划器",
    "abstract_zh": "扩散语言模型（DLMs）在利用上下文示例完成各类自然语言任务方面展现出强大的潜力。然而，由于其采用双向注意力机制，随着上下文长度的增加，计算开销显著上升。本文基于一项关键发现：与自回归语言模型（ARLMs）的顺序生成不同，DLMs的扩散生成范式允许在生成过程中**高效地动态调整上下文**。基于这一洞察，我们提出了**动 态 上 下 文 规 划 器**（Dynamic In-Context Planner, DIP），一种上下文优化方法——它在生成过程中动态选择并插入上下文示例，而非预先在提示中提供全部示例。实验结果表明，DIP在保持生成质量的同时，相较于标准推理实现了最高达12.9倍的推理加速，相较于KV缓存增强型推理也实现了1.17倍的加速。"
  },
  {
    "date": "2026-01-06",
    "title": "MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory",
    "authors": "Shengtao Zhang, Jiaqian Wang, Ruiwen Zhou, Junwei Liao, Yuchen Feng, Weinan Zhang, Ying Wen, Zhiyu Li, Feiyu Xiong, Yutao Qi, Bo Tang, Muning Wen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03192v1",
    "source": "arXiv",
    "abstract": "The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.",
    "title_zh": "MemRL：通过基于情景记忆的运行时强化学习实现自我演化的智能体",
    "abstract_zh": "人类智能的标志在于能够通过建设性情景模拟来掌握新技能——即调用过往经验，综合生成解决新任务的方案。尽管大型语言模型具备强大的推理能力，但在模拟这种自我进化方面仍存在困难：微调计算成本高昂且容易引发灾难性遗忘；而现有的基于记忆的方法则依赖于被动的语义匹配，常常检索到噪声信息。为应对这些挑战，我们提出 MemRL 框架，使智能体能够通过在情景记忆上的非参数强化学习实现自我演化。MemRL 明确将冻结的大型语言模型所具有的稳定推理能力与可塑、持续演化的记忆分离开来。与传统方法不同，MemRL 采用两阶段检索机制：首先根据语义相关性筛选候选记忆，再依据学习得到的 Q 值（效用）进行选择。这些效用值通过环境反馈以试错方式不断优化，使智能体能够有效区分高价值策略与相似的噪声信息。在 HLE、BigCodeBench、ALFWorld 和 Lifelong Agent Bench 等多个基准上的大量实验表明，MemRL 显著优于当前最先进的基线方法。分析实验进一步验证了 MemRL 有效解决了稳定性与可塑性之间的矛盾，实现了无需权重更新的持续运行时性能提升。"
  },
  {
    "date": "2026-01-06",
    "title": "DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation",
    "authors": "Jiajun jiao, Haowei Zhu, Puyuan Yang, Jianghui Wang, Ji Liu, Ziqiong Liu, Dong Li, Yuejian Fang, Junhai Yong, Bin Wang, Emad Barsoum",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03178v1",
    "source": "arXiv",
    "abstract": "Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.",
    "title_zh": "DiffBench 遇上 DiffAgent：端到端的基于大模型的扩散加速代码生成",
    "abstract_zh": "扩散模型在图像和视频生成方面取得了显著成功。然而，其固有的多步推理过程带来了巨大的计算开销，阻碍了实际应用部署。因此，加速扩散模型至关重要，但如何有效结合多种模型加速技术仍是一个重大挑战。为解决这一问题，我们提出了一种由大语言模型（LLM）驱动的框架，用于自动化生成和评估加速代码。首先，我们构建了DiffBench，一个全面的基准测试平台，实现了跨多种扩散模型架构、优化组合及部署场景的三阶段自动化评估流程。其次，我们提出了DiffAgent，一种能够为任意扩散模型生成最优加速策略与代码的智能代理。DiffAgent采用闭环工作流：规划组件与调试组件不断迭代优化代码生成组件的输出，同时通过遗传算法从执行环境中提取性能反馈，指导后续代码的改进。本文详细阐述了DiffBench的构建方法以及DiffAgent的设计原则。大量实验表明，DiffBench能够对生成的代码进行全面评估，而DiffAgent在生成高效扩散模型加速策略方面显著优于现有大语言模型。"
  },
  {
    "date": "2026-01-06",
    "title": "LLMs, You Can Evaluate It! Design of Multi-perspective Report Evaluation for Security Operation Centers",
    "authors": "Hiroyuki Okada, Tatsumi Oba, Naoto Yanai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03013v1",
    "source": "arXiv",
    "abstract": "Security operation centers (SOCs) often produce analysis reports on security incidents, and large language models (LLMs) will likely be used for this task in the near future. We postulate that a better understanding of how veteran analysts evaluate reports, including their feedback, can help produce analysis reports in SOCs. In this paper, we aim to leverage LLMs for analysis reports. To this end, we first construct a Analyst-wise checklist to reflect SOC practitioners' opinions for analysis report evaluation through literature review and user study with SOC practitioners. Next, we design a novel LLM-based conceptual framework, named MESSALA, by further introducing two new techniques, granularization guideline and multi-perspective evaluation. MESSALA can maximize report evaluation and provide feedback on veteran SOC practitioners' perceptions. When we conduct extensive experiments with MESSALA, the evaluation results by MESSALA are the closest to those of veteran SOC practitioners compared with the existing LLM-based methods. We then show two key insights. We also conduct qualitative analysis with MESSALA, and then identify that MESSALA can provide actionable items that are necessary for improving analysis reports.",
    "title_zh": "大语言模型，你可以评估它！面向安全运营中心的多视角报告评估设计",
    "abstract_zh": "安全运营中心（SOC）通常会生成关于安全事件的分析报告，而大型语言模型（LLM）未来很可能会被用于这一任务。我们提出，若能更好地理解资深分析师对报告的评估标准及其反馈意见，将有助于提升SOC中分析报告的质量。本文旨在利用LLM来改进分析报告的生成。为此，我们首先通过文献综述和与SOC从业人员的用户研究，构建了一套“以分析师为中心”的检查清单，以反映SOC从业者在评估分析报告时的核心观点。随后，我们设计了一种基于LLM的新型概念框架——MESSALA，该框架引入了两项新方法：细粒度化指导原则和多视角评估机制。MESSALA能够最大化地模拟资深SOC分析师的评价过程，并提供贴近其真实感知的反馈。在对MESSALA进行广泛实验后，结果显示，MESSALA的评估结果与资深SOC分析师的判断最为接近，优于现有的基于LLM的方法。我们进一步揭示了两个关键发现。此外，通过对MESSALA开展定性分析，我们确认其能够生成切实可行的改进建议，这些建议对于提升分析报告质量至关重要。"
  },
  {
    "date": "2026-01-06",
    "title": "AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents",
    "authors": "Mehmet Kurmaz",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02643v1",
    "source": "arXiv",
    "abstract": "Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with \"no results\" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.",
    "title_zh": "AWARE-US：面向工具调用智能体中偏好感知解析的基准测试",
    "abstract_zh": "工具调用型对话代理在查询结构化数据库时，常面临两个相互关联的失败：**约束不足**（缺少运行精确查询所需的约束条件）和**不可行性**（完全指定的查询返回空集，因为没有任何条目能满足所有约束）。现有方法通常以“无结果”回应，或使用临时规则放宽约束，这可能导致违背用户意图——即舍弃了用户最关心的要求。我们提出将不可行性处理建模为一种**偏好感知的查询修复问题**：当查询无法满足时，代理应放松对用户而言最重要程度最低的约束。为此，我们提出了三种基于大语言模型（LLM）的方法，用于从对话中推断约束的相对重要性：(1) 局部加权法，(2) 全局一次性加权法，以及 (3) 成对排序法。实验表明，局部加权法在偏好对齐方面表现最佳，而全局加权法在正确放松约束方面效果最优。此外，我们还引入了 AWARE-US 基准数据集，该数据集包含基于人物设定的查询任务，要求代理通过对话澄清请求，并以符合人物隐含偏好的方式解决不可行性问题。"
  },
  {
    "date": "2026-01-06",
    "title": "Decentralized Autoregressive Generation",
    "authors": "Stepan Maschan, Haoxuan Qu, Jun Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03184v1",
    "source": "arXiv",
    "abstract": "We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.",
    "title_zh": "去中心化自回归生成",
    "abstract_zh": "我们对自回归生成的去中心化进行了理论分析。通过将概率生成速度表示为专家流的线性组合，我们定义了去中心化离散流匹配目标。此外，我们还通过实验展示了在多种基准测试中，去中心化与中心化训练设置对于多模态语言模型的等价性。具体而言，我们比较了两种不同的范式：LLaVA 和 InternVL 2.5-1B，后者采用固定的 CLIP 视觉编码器，并在指令微调阶段对全部参数（ViT+MLP+LLM）进行微调。"
  },
  {
    "date": "2026-01-06",
    "title": "Gradient descent reliably finds depth- and gate-optimal circuits for generic unitaries",
    "authors": "Janani Gomathi, Alex Meiburg",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03123v1",
    "source": "arXiv",
    "abstract": "When the gate set has continuous parameters, synthesizing a unitary operator as a quantum circuit is always possible using exact methods, but finding minimal circuits efficiently remains a challenging problem. The landscape is very different for compiled unitaries, which arise from programming and typically have short circuits, as compared with generic unitaries, which use all parameters and typically require circuits of maximal size. We show that simple gradient descent reliably finds depth- and gate-optimal circuits for generic unitaries, including in the presence of restricted chip connectivity. This runs counter to earlier evidence that optimal synthesis required combinatorial search, and we show that this discrepancy can be explained by avoiding the random selection of certain parameter-deficient circuit skeletons.",
    "title_zh": "梯度下降能够可靠地为通用酉矩阵找到深度和门数最优的电路。",
    "abstract_zh": "当门集包含连续参数时，使用精确方法合成一个酉算符为量子电路始终是可行的，但高效地找到最小电路仍然是一个具有挑战性的问题。对于编译得到的酉算符（通常源于编程，具有较短的电路）与通用酉算符（使用所有参数，通常需要最大规模的电路）而言，情况截然不同。我们证明，简单的梯度下降法能够可靠地为通用酉算符找到深度和门数最优的电路，即使在受限芯片连通性的条件下也如此。这与早期认为最优合成必须依赖组合搜索的观点相悖，我们进一步表明，这种差异可以通过避免随机选择某些参数不足的电路结构来解释。"
  },
  {
    "date": "2026-01-06",
    "title": "Do LLMs Encode Functional Importance of Reasoning Tokens?",
    "authors": "Janvijay Singh, Dilek Hakkani-Tür",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03066v1",
    "source": "arXiv",
    "abstract": "Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased computational cost and reduced ability to isolate functionally relevant reasoning. Prior work on compact reasoning shortens such chains through probabilistic sampling, heuristics, or supervision from frontier models, but offers limited insight into whether models internally encode token-level functional importance for answer generation. We address this gap diagnostically and propose greedy pruning, a likelihood-preserving deletion procedure that iteratively removes reasoning tokens whose removal minimally degrades model likelihood under a specified objective, yielding length-controlled reasoning chains. We evaluate pruned reasoning in a distillation framework and show that students trained on pruned chains outperform a frontier-model-supervised compression baseline at matched reasoning lengths. Finally, our analysis reveals systematic pruning patterns and shows that attention scores can predict greedy pruning ranks, further suggesting that models encode a nontrivial functional importance structure over reasoning tokens.",
    "title_zh": "大语言模型是否编码了推理标记的功能重要性？",
    "abstract_zh": "大型语言模型通过生成长篇推理链来解决复杂任务，虽然能提升准确性，但同时也带来了更高的计算成本以及难以分离出功能相关推理部分的问题。以往的研究通过概率采样、启发式方法或前沿模型的监督来压缩推理链，但这些方法对模型内部是否编码了与答案生成相关的token级功能重要性缺乏深入洞察。为此，我们从诊断角度出发，提出一种“贪婪剪枝”（greedy pruning）方法——这是一种保持似然性的删除过程，通过迭代移除在特定目标下对模型似然影响最小的推理token，从而生成长度可控的推理链。我们在知识蒸馏框架中评估了剪枝后的推理链，结果表明，使用剪枝链训练的学生模型在相同推理长度下，性能优于由前沿模型监督的压缩基线。最后，我们的分析揭示了系统性的剪枝模式，并发现注意力分数能够有效预测贪婪剪枝的优先级，进一步表明模型在推理token上确实编码了非平凡的功能重要性结构。"
  },
  {
    "date": "2026-01-06",
    "title": "Few-shot learning for security bug report identification",
    "authors": "Muhammad Laiq",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02971v1",
    "source": "arXiv",
    "abstract": "Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.",
    "title_zh": "少样本学习在安全漏洞报告识别中的应用",
    "abstract_zh": "安全漏洞报告需要及时识别，以最大限度地缩小软件系统中的漏洞暴露时间。传统的机器学习（ML）技术在分类漏洞报告以识别安全漏洞报告时，严重依赖大量标注数据。然而，在实际应用中，安全漏洞报告的数据集往往十分稀缺，导致模型性能不佳，难以在真实场景中广泛应用。本研究提出一种基于少样本学习（few-shot learning）的技术，能够在标注数据有限的情况下有效识别安全漏洞报告。我们采用SetFit——一种先进的少样本学习框架，该框架结合了句子嵌入（sentence transformers）、对比学习以及参数高效的微调方法。模型在少量标注的漏洞报告数据集上进行训练，并评估其将报告分类为安全相关或非安全相关的能力。实验结果表明，我们的方法在所有测试数据集上均达到最高0.865的AUC值，优于传统机器学习方法（基线模型）。这凸显了SetFit在有效识别安全漏洞报告方面的潜力。基于SetFit的少样本学习为识别安全漏洞报告提供了一种有前景的替代方案，能够以极小的标注工作量实现高效模型开发，特别适用于标注数据稀缺的实际场景。"
  },
  {
    "date": "2026-01-06",
    "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning",
    "authors": "Xuan Yang, Furong Jia, Roy Xie, Xiong Xi, Hengwei Bian, Jian Li, Monica Agrawal",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02950v1",
    "source": "arXiv",
    "abstract": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.",
    "title_zh": "思维批：用于增强大语言模型推理的跨实例学习",
    "abstract_zh": "当前的大语言模型推理系统独立处理每个查询，忽略了诸如共享推理模式和一致性约束等宝贵的跨实例信号。我们提出了Batch-of-Thought（BoT）——一种无需训练的方法，通过联合处理相关查询来实现跨实例学习。通过在批次间进行对比分析，BoT能够识别高质量的推理模板，利用一致性检查发现错误，并分摊计算成本。我们将BoT集成到多智能体反思架构（BoT-R）中，其中“反思者”（Reflector）执行联合评估，从而挖掘出孤立处理无法获得的相互信息增益。在三个模型家族和六个基准测试上的实验表明，BoT-R在提升准确率和置信度校准的同时，将推理成本降低了高达61%。我们的理论与实验分析揭示了批处理感知推理为何以及在何种情况下能为大语言模型系统带来优势。"
  },
  {
    "date": "2026-01-06",
    "title": "Revisiting Data Compression with Language Modeling",
    "authors": "Chen-Han Tsai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02875v1",
    "source": "arXiv",
    "abstract": "In this report, we investigate the potential use of large language models (LLM's) in the task of data compression. Previous works have demonstrated promising results in applying LLM's towards compressing not only text, but also a wide range of multi-modal data. Despite the favorable performance achieved, there still remains several practical questions that pose a challenge towards replacing existing data compression algorithms with LLM's. In this work, we explore different methods to achieve a lower adjusted compression rate using LLM's as data compressors. In comparison to previous works, we were able to achieve a new state-of-the-art (SOTA) adjusted compression rate of around $18\\%$ on the enwik9 dataset without additional model training. Furthermore, we explore the use of LLM's in compressing non-English data, code data, byte stream sequences. We show that while LLM's excel in compressing data in text-dominant domains, their ability in compressing non-natural text sequences still remain competitive if configured in the right way.",
    "title_zh": "用语言模型重新审视数据压缩",
    "abstract_zh": "在本报告中，我们探讨了大型语言模型（LLM）在数据压缩任务中的潜在应用。以往的研究已证明，LLM 在压缩文本以及多种多模态数据方面均取得了令人瞩目的成果。尽管取得了良好的性能，但要将现有的数据压缩算法替换为 LLM 仍面临若干实际问题。在本研究中，我们探索了多种方法，以利用 LLM 实现更低的调整后压缩率。与先前工作相比，我们在无需额外模型训练的情况下，于 enwik9 数据集上实现了约 18% 的新状态最优（SOTA）调整后压缩率。此外，我们还研究了 LLM 在非英语数据、代码数据及字节流序列压缩中的应用。结果表明，虽然 LLM 在以文本为主导的数据领域表现出色，但在正确配置的前提下，其对非自然文本序列的压缩能力依然具有竞争力。"
  },
  {
    "date": "2026-01-06",
    "title": "Bounded Rewriting Induction for LCSTRSs",
    "authors": "Kasper Hagens, Cynthia Kop",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02803v1",
    "source": "arXiv",
    "abstract": "Rewriting Induction (RI) is a method to prove inductive theorems, originating from equational reasoning. By using Logically Constrained Simply-typed Term Rewriting Systems (LCSTRSs) as an intermediate language, rewriting induction becomes a tool for program verification, with inductive theorems taking the role of equivalence predicates. Soundness of RI depends on well-founded induction, and one of the core obstacles for obtaining a practically useful proof system is to find suitable well-founded orderings automatically. Using naive approaches, all induction hypotheses must be oriented within the well-founded ordering, which leads to very strong termination requirements. This, in turn, severely limits the proof capacity of RI. Here, we introduce Bounded RI: an adaption of RI for LCSTRSs where such termination requirements are minimized.",
    "title_zh": "LCSTRSs 的有界重写归纳法",
    "abstract_zh": "重写归纳（Rewriting Induction, RI）是一种用于证明归纳定理的方法，起源于等式推理。通过使用逻辑约束的简单类型项重写系统（Logically Constrained Simply-typed Term Rewriting Systems, LCSTRSs）作为中间语言，重写归纳成为程序验证的一种工具，其中归纳定理扮演等价谓词的角色。RI 的正确性依赖于良基归纳，而构建一个实际可用的证明系统的核心障碍之一，是能否自动找到合适的良基序关系。采用朴素方法时，所有归纳假设都必须在良基序中被定向，这导致了非常强的终止性要求，从而严重限制了 RI 的证明能力。本文提出了一种改进方法——有界重写归纳（Bounded RI），它是针对 LCSTRSs 的 RI 的一种适应性改进，旨在最大限度地降低此类终止性要求。"
  },
  {
    "date": "2026-01-06",
    "title": "The Path Ahead for Agentic AI: Challenges and Opportunities",
    "authors": "Nadia Sibai, Yara Ahmed, Serry Sibaee, Sawsan AlHalawani, Adel Ammar, Wadii Boulila",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02749v1",
    "source": "arXiv",
    "abstract": "The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.",
    "title_zh": "代理型人工智能的未来之路：挑战与机遇",
    "abstract_zh": "大型语言模型（LLMs）从被动的文本生成器演变为自主、目标驱动的系统，标志着人工智能领域的一次根本性转变。本章探讨了代理型人工智能系统（agentic AI systems）的兴起，这些系统通过整合规划、记忆、工具使用和迭代推理能力，在复杂环境中实现自主运行。我们追溯了从统计模型到基于Transformer的系统在架构上的演进过程，识别出促成代理行为的关键能力：长程推理、上下文感知以及自适应决策。本章提出三项贡献：（1）综合分析LLM能力如何通过“推理-行动-反思”循环向自主性延伸；（2）构建一个整合性框架，描述感知、记忆、规划与工具执行等核心组件如何将LLM与自主行为相连接；（3）对应用实践进行批判性评估，并深入探讨安全、对齐、可靠性与可持续性等方面的持续挑战。与现有综述不同，本文聚焦于从语言理解到自主行动的架构转型，强调在部署前必须解决的技术鸿沟。我们指出了若干关键研究方向，包括可验证的规划机制、可扩展的多智能体协同、持久化记忆架构以及治理框架。负责任的发展需要在技术鲁棒性、可解释性与伦理保障方面同步推进，以充分释放其潜力，同时防范对齐偏差与意外后果的风险。"
  },
  {
    "date": "2026-01-06",
    "title": "Time-Scaling Is What Agents Need Now",
    "authors": "Zhi Liu, Guangzhi Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02714v1",
    "source": "arXiv",
    "abstract": "Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on \"perception-representation,\" Reinforcement Learning on \"decision-making-behavior,\" and Symbolic AI on \"knowledge-reasoning.\" With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop \"perception-decision-action\" capabilities. Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency. This highlights the need for \"Time-Scaling\"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.",
    "title_zh": "时间缩放正是代理当前所需",
    "abstract_zh": "早期的人工智能范式呈现出认知功能的分离：神经网络专注于“感知-表征”，强化学习聚焦于“决策-行为”，而符号AI则致力于“知识-推理”。随着基于Transformer的大模型和世界模型的发展，这些范式正逐步融合，形成具备闭环“感知-决策-行动”能力的认知代理。人类在认知资源有限的情况下，通过时间化的序列推理来解决复杂问题；语言则依赖于对问题空间的搜索以实现深层语义推理。尽管早期的大语言模型（LLMs）能够生成流畅文本，却缺乏稳健的语义推理能力。提示技术如思维链（Chain-of-Thought, CoT）和思维树（Tree-of-Thought, ToT）通过显式化中间推理步骤，拓展了推理路径。近期模型如DeepSeek-R1通过显式的推理轨迹显著提升了性能。然而，这些方法在搜索的完备性与效率方面仍存在局限。这凸显出“时间缩放”（Time-Scaling）的迫切需求——即系统性地扩展与优化智能体在时间维度上展开推理的能力。时间缩放指的是通过引入延展的时间路径进行架构设计，使智能体能够更深入地探索问题空间，动态调整策略，并增强元认知控制，从而在认知约束下模拟人类的序列推理过程。它代表了提升深度推理与问题解决能力的关键前沿方向，且无需成比例增加静态模型参数。要推进智能代理能力的跃升，必须将时间缩放原则置于核心地位，将显式的时间推理管理作为基础性构建要素。"
  },
  {
    "date": "2026-01-06",
    "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents",
    "authors": "Chenglin Yu, Yuchen Wang, Songmiao Wang, Hongxia Yang, Ming Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03204v1",
    "source": "arXiv",
    "abstract": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent",
    "title_zh": "InfiAgent：一种面向通用自主代理的无限时域框架",
    "abstract_zh": "大型语言模型代理（LLM agents）具备推理和使用工具的能力，但在处理长期任务时，常常因上下文无限增长和错误累积而失效。常见的解决方案如上下文压缩或检索增强提示，往往在信息保真度与推理稳定性之间带来权衡。我们提出 InfiAgent，一种通用框架，通过将代理的持久状态外化为以文件为中心的状态抽象，确保无论任务持续时间多长，其推理上下文始终保持严格受限。在每一步中，代理从工作区的状态快照加上一个固定窗口内的近期操作来重建上下文。在 DeepResearch 和一项包含80篇论文的文献综述任务上的实验表明，无需针对特定任务进行微调，仅使用200亿参数的开源模型，InfiAgent 的表现已可媲美更大的专有系统，并且在长期任务覆盖方面显著优于基于上下文的基线方法。这些结果支持将显式状态外化作为构建稳定长期任务代理的实用基础。GitHub 仓库：https://github.com/ChenglinPoly/infiAgent"
  },
  {
    "date": "2026-01-06",
    "title": "CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation",
    "authors": "Peiding Wang, Li Zhang, Fang Liu, Chongyang Tao, Yinghao Zhu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02868v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.",
    "title_zh": "CodeMEM：面向仓库级迭代代码生成的AST引导自适应记忆机制",
    "abstract_zh": "大型语言模型（LLMs）通过交互式协作显著提升了代码仓库级代码生成中的开发效率。然而，随着交互的深入，必须持续维护和更新仓库上下文，以整合新验证的信息。与此同时，不断增长的会话历史也带来了认知负担，常常导致遗忘问题，甚至重复引入已解决的错误。现有的记忆管理方法虽具潜力，但仍受限于以自然语言为中心的表示方式。为克服这些局限，我们提出 CodeMEM——一种面向仓库级迭代代码生成的 AST 引导型动态记忆管理系统。具体而言，CodeMEM 引入了“代码上下文记忆”组件，通过基于抽象语法树（AST）引导的 LLM 操作，动态维护和更新仓库上下文；同时设计了“代码会话记忆”组件，构建以代码为核心的交互历史表示，并通过基于 AST 的分析显式检测并缓解遗忘现象。在指令遵循基准 CodeIF-Bench 和代码生成基准 CoderEval 上的实验结果表明，CodeMEM 达到了当前最优性能：单轮指令遵循能力提升 12.2%，会话级别提升 11.5%；交互轮次减少 2-3 轮，同时保持了具有竞争力的推理延迟与词元效率。"
  },
  {
    "date": "2026-01-06",
    "title": "SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation",
    "authors": "Hanqi Jiang, Junhao Chen, Yi Pan, Ling Chen, Weihang You, Yifan Zhou, Ruidong Zhang, Yohannes Abate, Tianming Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02744v1",
    "source": "arXiv",
    "abstract": "While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the \"Contextual Tunneling\" problem. Our code and data will be made publicly available upon acceptance.",
    "title_zh": "SYNAPSE：通过扩散激活实现情景-语义记忆，赋能大语言模型智能体",
    "abstract_zh": "尽管大型语言模型（LLMs）在通用推理方面表现出色，但传统的检索增强方法无法解决长期智能体记忆的碎片化问题。为弥合这一差距，我们提出了Synapse（协同关联式语义编码），一种统一的记忆架构，突破了静态向量相似性的局限。受认知科学启发，Synapse将记忆建模为一个动态图结构，其中相关性通过激活扩散而非预计算的连接来产生。通过引入侧向抑制和时间衰减机制，系统能够动态突出相关子图，同时有效过滤干扰信息。我们实现了一种三重混合检索策略，融合几何嵌入与基于激活的图遍历方法。在LoCoMo基准测试中的全面评估表明，Synapse在复杂的时序推理和多跳推理任务中显著优于当前最先进的方法，为“上下文隧道”问题提供了一个稳健的解决方案。我们的代码和数据将在论文被接受后公开发布。"
  },
  {
    "date": "2026-01-06",
    "title": "Mitigating Prompt-Induced Hallucinations in Large Language Models via Structured Reasoning",
    "authors": "Jinbo Hao, Kai Yang, Qingzhen Su, Yang Chen, Yifan Li, Chao Jiang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02739v1",
    "source": "arXiv",
    "abstract": "To address hallucination issues in large language models (LLMs), this paper proposes a method for mitigating prompt-induced hallucinations. Building on a knowledge distillation chain-style model, we introduce a code module to guide knowledge-graph exploration and incorporate code as part of the chain-of-thought prompt, forming an external knowledge input that provides more accurate and structured information to the model. Based on this design, we develop an improved knowledge distillation chain-style model and leverage it to analyze and constrain the reasoning process of LLMs, thereby improving inference accuracy. We empirically evaluate the proposed approach using GPT-4 and LLaMA-3.3 on multiple public datasets. Experimental results demonstrate that incorporating code modules significantly enhances the model's ability to capture contextual information and effectively mitigates prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 improve by 15.64%, 13.38%, and 13.28%, respectively. Moreover, the proposed method achieves HIT@1, HIT@3, and HIT@5 scores exceeding 95% across several evaluation settings. These results indicate that the proposed approach substantially reduces hallucination behavior while improving the accuracy and verifiability of large language models.",
    "title_zh": "通过结构化推理缓解大型语言模型中的提示诱导幻觉",
    "abstract_zh": "为解决大型语言模型（LLMs）中的幻觉问题，本文提出了一种缓解提示诱导型幻觉的方法。基于知识蒸馏链式模型，我们引入代码模块以引导知识图谱的探索，并将代码作为思维链提示的一部分，形成外部知识输入，为模型提供更准确、结构化的信息。在此设计基础上，我们构建了一个改进的知识蒸馏链式模型，并利用该模型分析和约束大语言模型的推理过程，从而提升推理准确性。我们在多个公开数据集上，使用GPT-4和LLaMA-3.3对所提方法进行了实证评估。实验结果表明，引入代码模块显著增强了模型捕捉上下文信息的能力，并有效缓解了提示诱导的幻觉现象。具体而言，HIT@1、HIT@3和HIT@5指标分别提升了15.64%、13.38%和13.28%。此外，在多个评估场景中，该方法在HIT@1、HIT@3和HIT@5上的得分均超过95%。这些结果表明，所提出的方法在显著降低幻觉行为的同时，有效提升了大语言模型的准确性与可验证性。"
  },
  {
    "date": "2026-01-06",
    "title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems",
    "authors": "Guibin Zhang, Haiyang Yu, Kaiming Yang, Bingli Wu, Fei Huang, Yongbin Li, Shuicheng Yan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02695v1",
    "source": "arXiv",
    "abstract": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.",
    "title_zh": "EvoRoute：基于经验驱动的自路由大模型智能体系统",
    "abstract_zh": "由大型语言模型（LLMs）、工具模块和记忆模块协同驱动的复杂代理型AI系统，在处理复杂、多轮任务方面已展现出卓越能力。然而，这种成功背后伴随着高昂的经济成本和严重的延迟问题，暴露出一个关键却未被充分探索的权衡难题。我们将其形式化为**代理系统三难困境**：在实现顶尖性能、最小化经济成本以及确保快速任务完成之间存在的内在矛盾。为破解这一三难困境，我们提出EvoRoute——一种自演化的模型路由范式，突破了静态预设模型分配的局限。EvoRoute利用不断扩展的历史经验知识库，动态选择每一步中帕累托最优的LLM骨干模型，在准确率、效率与资源消耗之间实现平衡，并通过环境反馈持续优化自身的选型策略。在GAIA和BrowseComp+等具有挑战性的代理基准测试中的实验表明，当EvoRoute集成到现成的代理系统中时，不仅能够维持甚至提升系统性能，还能将执行成本降低高达80%，延迟减少超过70%。"
  },
  {
    "date": "2026-1-6",
    "title": "Automatic programming via large language models with population self-evolution for dynamic fuzzy job shop scheduling problem_supp1-3650586.docx",
    "authors": "Xinyu Li",
    "publish": "N/A",
    "url": "https://doi.org/10.1109/tfuzz.2025.3650586/mm1",
    "source": "IEEE",
    "abstract": "Heuristic dispatching rules (HDRs) are widely used for solving the dynamic fuzzy job shop scheduling problem (DFJSSP). However, their performance is highly sensitive to specific scenarios and often necessitates expert customization. To overcome this, automated design methods like genetic programming (GP) and gene expression programming (GEP) have been proposed. Despite their success, these methods face challenges, such as high randomness in the search process. Recently, the combination of large language models (LLMs) with evolutionary algorithms has opened new possibilities for prompt engineering and automated algorithm design. To improve the ability of LLMs in automatic HDR design, this paper introduces a novel population self-evolutionary (SeEvo) framework, which draws inspiration from the self-reflective design strategies employed by human experts. Notably, this framework employs a novel teacher-student learning mechanism, allowing the LLM (student) to generate robust HDRs. Guided by a teacher model with complete knowledge of actual processing times, the student learns to infer fuzzy uncertainties from historical deviations, enabling it to effectively anticipate and adapt to fuzzy impacts. Experimental results demonstrate that SeEvo significantly outperforms GP, GEP, deep reinforcement learning (DRL) methods, and more than ten commonly used HDRs from the literature, particularly in previously unseen and dynamic scenarios.",
    "title_zh": "基于种群自进化的大语言模型自动编程在动态模糊作业车间调度问题中的应用_supp1-3650586.docx",
    "abstract_zh": "启发式调度规则（HDRs）被广泛应用于求解动态模糊作业车间调度问题（DFJSSP）。然而，其性能对具体场景高度敏感，通常需要专家进行定制化调整。为克服这一局限，研究人员提出了诸如遗传编程（GP）和基因表达编程（GEP）等自动化设计方法。尽管这些方法取得了一定成功，但仍面临搜索过程随机性高、稳定性不足等挑战。近年来，大型语言模型（LLMs）与进化算法的结合为提示工程和自动化算法设计开辟了新途径。为了提升LLM在自动HDR设计中的能力，本文提出一种新颖的群体自进化（SeEvo）框架，该框架受到人类专家自省式设计策略的启发。特别地，该框架引入了一种创新的师生学习机制：由具备完整实际加工时间知识的教师模型指导，LLM（学生）能够从历史偏差中推断出模糊不确定性，从而有效预测并适应模糊影响。实验结果表明，SeEvo在未见过的动态场景中显著优于GP、GEP、深度强化学习（DRL）方法以及文献中十余种常用HDRs，展现出卓越的适应性和鲁棒性。"
  },
  {
    "date": "2026-1-6",
    "title": "Research on the Application of AI Large Models in 6G Network Operations and Maintenance",
    "authors": "Zhou Sheng, Qu Jun, Wang Xi",
    "publish": "2025 IEEE 102nd Vehicular Technology Conference (VTC2025-Fall)",
    "url": "https://doi.org/10.1109/vtc2025-fall65116.2025.11310440",
    "source": "IEEE",
    "abstract": "With the rapid development of AI large model technology, the application of AI large models has put forward higher requirements for the design of network architectures. 6G networks can provide network performance with lower latency and higher reliability when AI large models perform complex real-time calculations. By exploring the application of AI large models in network operations and maintenance, this paper first designs a cloud-edge-terminal collaborative model deployment scheme in wireless networks to reduce the latency caused by model fine-tuning, training set inference. Then, based on the powerful semantic understanding ability, code generation ability, and capability orchestration ability of large language models, it builds functions such as intelligent question answering, data self-service, and report generation to support five major scenario applications including wireless professional maintenance, energy saving, optimization, complaint handling, and planning and construction. Finally, an operation and maintenance brain is created to achieve closed-loop management of the entire wireless operation and maintenance process, improve network operation and maintenance efficiency, and greatly guarantee network reliability.",
    "title_zh": "人工智能大模型在6G网络运维中的应用研究",
    "abstract_zh": "随着人工智能大模型技术的快速发展，AI大模型的应用对网络架构设计提出了更高要求。6G网络能够在AI大模型执行复杂实时计算时，提供更低时延和更高可靠性的网络性能。本文通过探索AI大模型在网络运维中的应用，首先设计了一种无线网络中云-边-端协同的模型部署方案，以降低模型微调、训练集推理带来的时延问题；随后，基于大语言模型强大的语义理解能力、代码生成能力及能力编排能力，构建了智能问答、数据自助服务、报告生成等功能，支撑无线专业维护、节能降耗、优化调整、投诉处理以及规划与建设等五大场景应用；最后，打造了运维大脑，实现无线运维全流程的闭环管理，显著提升网络运维效率，极大保障了网络可靠性。"
  },
  {
    "date": "2026-1-6",
    "title": "A Two-Stage Multi-Source Transfer Learning Approach for Software Defect Prediction",
    "authors": "Hao Li, Shuo Wang",
    "publish": "2025 IEEE 23rd International Conference on Industrial Informatics (INDIN)",
    "url": "https://doi.org/10.1109/indin64977.2025.11279473",
    "source": "IEEE",
    "abstract": "Multi-source transfer learning (MSTL) acquires knowledge from multiple source domains to improve learning in a target domain, offering higher generalization and robustness than single-source transfer. In software defect prediction, MSTL utilizes cross-project data for more accurate prediction. However, existing methods often neglect the scarcity of defect samples in the target domain. To address this, we propose Cross-Domain Consistency Multisource Transfer Learning (CDC-MSTL), a novel ensemble method based on MsTrAdaBoost. CDC-MSTL considers both domain similarity through two-stage instance transfer and the target dataset’s imbalance degree during training. Experiments on four software projects (23 datasets) show CDC-MSTL significantly outperforms state-of-the-art MSTL methods in Balanced Accuracy (BA) and is competitive in AUC, F-measure, and accuracy.",
    "title_zh": "一种两阶段多源迁移学习方法用于软件缺陷预测",
    "abstract_zh": "多源迁移学习（MSTL）通过从多个源域获取知识来提升目标域的学习效果，相较于单源迁移学习具有更高的泛化能力和鲁棒性。在软件缺陷预测中，MSTL利用跨项目数据实现更精准的预测。然而，现有方法往往忽视了目标域中缺陷样本稀缺的问题。为解决这一问题，本文提出一种基于MsTrAdaBoost的新型集成方法——跨域一致性多源迁移学习（CDC-MSTL）。该方法在训练过程中同时考虑了通过两阶段实例迁移所体现的域间相似性以及目标数据集的不平衡程度。在四个软件项目（共23个数据集）上的实验结果表明，CDC-MSTL在平衡准确率（BA）上显著优于当前最先进的MSTL方法，且在AUC、F-measure和准确率方面也表现出良好的竞争力。"
  },
  {
    "date": "2026-1-6",
    "title": "Generation of Synthesizable Verilog Code from Natural Language Specifications",
    "authors": "Daniil S. Yashchenko, Aleksandr Y. Romanov, Roman A. Solovyev, Dmitry V. Telpukhov, Artur A. Ziazetdinov",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2026.3651684",
    "source": "IEEE",
    "abstract": "This study presents a method for generating synthesizable Verilog code for digital integrated circuits directly from natural-language specifications. The approach combines large language models with parameter-efficient fine-tuning—specifically, Low-Rank Adaptation and Quantized Low-Rank Adaptation—together with a specialized corpus of specification–code pairs that covers common design patterns and varying task complexity. The pipeline includes automated compilation, simulation, and synthesizability checks to ensure that outputs are both syntactically correct and suitable for downstream tool flows. Evaluation is performed using the pass-at-k metric on the standardized VerilogEval benchmark. The fine-tuned models substantially improve functional correctness over untuned baselines, achieving pass-at-k of up to 0.88 while markedly reducing both syntax and logic errors. The results indicate that reliable Verilog code generation from natural language can be achieved under constrained compute budgets; in our setup, effective training and inference remained feasible on a single graphics processing unit. Beyond empirical gains, the method demonstrates practical value for design automation by shortening iteration cycles and lowering the effort needed to move from textual requirements to synthesizable hardware modules. Overall, the findings support the use of large language models, paired with targeted data and validation, as a viable pathway for Verilog code generation and for accelerating the development of complex digital devices.",
    "title_zh": "从自然语言规范生成可综合的Verilog代码",
    "abstract_zh": "本研究提出了一种直接从自然语言规范生成可综合Verilog代码的方法，用于数字集成电路设计。该方法结合了大语言模型与参数高效微调技术——具体为低秩适应（Low-Rank Adaptation）和量化低秩适应（Quantized Low-Rank Adaptation），并引入了一个专门构建的规范-代码配对语料库，涵盖常见设计模式及不同任务复杂度。整个流程包含自动编译、仿真以及可综合性的检查，以确保输出在语法上正确且适用于后续工具链。评估基于标准化的VerilogEval基准测试，采用pass-at-k指标进行衡量。经过微调的模型相较于未微调的基线显著提升了功能正确性，最高达到0.88的pass-at-k值，同时大幅减少了语法错误和逻辑错误。结果表明，在计算资源受限的情况下，仍可实现可靠的自然语言到Verilog代码的生成；在我们的实验设置中，有效的训练与推理过程可在单个图形处理器上完成。除了实证性能提升外，该方法在设计自动化方面展现出实际应用价值，能够缩短迭代周期，并降低从文本需求到可综合硬件模块的转化成本。总体而言，研究结果支持将大语言模型结合针对性数据与验证机制，作为Verilog代码生成的一种可行路径，从而加速复杂数字器件的研发进程。"
  },
  {
    "date": "2026-1-6",
    "title": "Industrial Large-language-model Based Standard-Enhanced Digital Twin System",
    "authors": "Xuehan Bai, Cailian Chen",
    "publish": "2025 IEEE 23rd International Conference on Industrial Informatics (INDIN)",
    "url": "https://doi.org/10.1109/indin64977.2025.11279674",
    "source": "IEEE",
    "abstract": "With the advancement of Industry 4.0, digital twins have become increasingly widespread in the industrial sector, providing real-time data acquisition, simulation, and optimization capabilities. However, ensuring standardized operations remains a significant challenge, particularly in complex environments where the absence of experienced engineers can lead to non-compliant designs or unsafe practices. To address this, digital twins can act as self-checking tools or enable manual querying of standards, ensuring compliance throughout the industrial process. This paper proposes a framework for the standardized operation of digital twins based on industrial large language models (LLMs), integrating industrial standards and dynamically detecting compliance during operations. Experiments on power system standardization tested different LLM approaches, demonstrating the framework’s ability to guide task-specific LLM selection. This research offers a robust solution for enhancing compliance, safety, and efficiency in industrial AI-powered digital twins.",
    "title_zh": "基于大语言模型的工业级标准增强型数字孪生系统",
    "abstract_zh": "随着工业4.0的推进，数字孪生在工业领域中的应用日益广泛，具备实时数据采集、仿真与优化能力。然而，在复杂环境中确保标准化操作仍面临重大挑战，尤其当缺乏经验丰富的工程师时，容易导致设计不合规或存在安全隐患。为应对这一问题，数字孪生可作为自我检查工具，或支持人工查询标准规范，从而在整个工业流程中保障合规性。本文提出一种基于工业大语言模型（LLMs）的数字孪生标准化操作框架，整合工业标准，并在运行过程中实现动态合规性检测。通过对电力系统标准化的实验，测试了多种LLM方法，验证了该框架在指导特定任务下LLM选择方面的有效性。本研究为提升工业人工智能驱动的数字孪生在合规性、安全性和效率方面的表现提供了可靠解决方案。"
  },
  {
    "date": "2026-1-6",
    "title": "Multi-Granular Information Flow Tracking in RISC-V Based SoCs",
    "authors": "Dhruvakumar Aklekar, Vineet Chadalavada, Nahush Tambe, Fareena Saqib",
    "publish": "2025 IEEE Physical Assurance and Inspection of Electronics (PAINE)",
    "url": "https://doi.org/10.1109/paine66113.2025.11320219",
    "source": "IEEE",
    "abstract": "In modern system-on-chip (SoC) designs, the use of proprietary third-party IPs (3PIPs) offers modularity and performance benefits, but may compromise on transparency and raise concerns on data confidentiality. Attacks such as buffer overflows targeting security-critical modules can bypass traditional protection and require robust runtime security mechanisms. In this work a Multi-Granular Information Flow Tracking (MG-IFT) framework is proposed that enforces secure data flow policies across different abstraction levels. The framework addresses the limitations of single-level IFT. The Multi-Granular IFT (MG-IFT) approach combines ISA-IFT for system-wide coverage and integrates Gate Level IFT (GLIFT) to the security-critical units, such as cryptographic modules. Performance achieves high precision without significant resource overhead. A hardware implementation of ISA-IFT is demonstrated in the PicoRV32 core. The results demonstrate the feasibility of deploying MG-IFT in real hardware, moving beyond conceptual models toward implementable security-enhanced SoCs.",
    "title_zh": "基于RISC-V的SoC中的多粒度信息流追踪",
    "abstract_zh": "在现代片上系统（SoC）设计中，使用专有的第三方知识产权（3PIPs）虽然带来了模块化和性能上的优势，但也可能影响透明度，并引发数据保密性方面的担忧。针对安全关键模块的缓冲区溢出等攻击能够绕过传统防护机制，因此需要具备强大能力的运行时安全机制。本文提出了一种多粒度信息流跟踪（MG-IFT）框架，能够在不同抽象层次上强制执行安全的数据流策略。该框架克服了单一层次信息流跟踪（IFT）的局限性。多粒度信息流跟踪（MG-IFT）方法结合了指令集架构级信息流跟踪（ISA-IFT），实现全系统的覆盖，并将门级信息流跟踪（GLIFT）集成到安全关键单元（如加密模块）中。该方法在保持高精度的同时，几乎不带来显著的资源开销。本文还展示了ISA-IFT在PicoRV32核心中的硬件实现。实验结果证明了MG-IFT在真实硬件中部署的可行性，推动了从概念模型向可实施的安全增强型SoC的迈进。"
  },
  {
    "date": "2026-1-6",
    "title": "Implementation of Machine Learning-Based Dynamic Logic Locking",
    "authors": "Nahush Tambe, Naseeruddin Lodge, Vineet Chadalavada, Fareena Saqib",
    "publish": "2025 IEEE Physical Assurance and Inspection of Electronics (PAINE)",
    "url": "https://doi.org/10.1109/paine66113.2025.11320174",
    "source": "IEEE",
    "abstract": "In today's technologically driven landscape, reverse engineering and intellectual property (IP) piracy pose serious threats to integrated circuits (ICs), especially as design and fabrication are increasingly outsourced across global supply chains. Logic locking has emerged as a promising defense by obfuscating circuit functionality through key-contingent logic. However, selecting and implementing a logic locking technique that balances security against modern attack vectors, as well as area and power efficiency, remains a challenge. State-of-the-art techniques often lack application-specific adaptability, rely on outdated strategies, and require manual key insertion. To address these challenges, we propose a novel machine learning (ML)-based framework that automates the prediction and implementation of locking mechanisms based on the architectural characteristics of hardware designs. We evaluate various ML classifiers to identify the most effective key-gate insertion strategy, and the Random Forest classifier achieves the best performance with 97% prediction accuracy. Beyond automation, the framework is enriched with Dynamic Partial Reconfiguration (DPR) to support real-time key-gate updates. By dynamically reconfiguring logic-locked key-gates at runtime, DPR introduces an additional post-deployment security layer that resists static and side-channel attacks. Experimental evaluation on ISCAS'85 benchmarks demonstrates that DPR integration delivers strong adaptability and enhanced resilience against IP piracy with minimal overhead, making it practical for security-critical applications.",
    "title_zh": "基于机器学习的动态逻辑锁定实现",
    "abstract_zh": "在当今技术驱动的背景下，逆向工程与知识产权（IP）盗用对集成电路（IC）构成了严重威胁，尤其是在设计和制造日益全球化、外包于全球供应链的形势下。逻辑锁机制作为一种有前景的防御手段，通过依赖密钥的逻辑门对电路功能进行混淆，从而提升安全性。然而，如何选择并实现一种既能抵御现代攻击手段，又兼顾面积与功耗效率的逻辑锁技术，仍是当前的一大挑战。现有先进方法普遍存在应用适配性差、依赖过时策略以及需手动插入密钥等问题。为应对这些挑战，我们提出了一种基于机器学习（ML）的新型框架，能够根据硬件设计的架构特征自动预测并实施相应的锁机制。我们评估了多种机器学习分类器，以确定最优的密钥门插入策略，其中随机森林（Random Forest）分类器表现最佳，预测准确率达到97%。除自动化外，该框架还引入了动态部分重配置（DPR）技术，支持密钥门的实时更新。通过在运行时动态重构逻辑锁密钥门，DPR在部署后增加了一层额外的安全防护，有效抵御静态攻击与侧信道攻击。在ISCAS'85基准测试集上的实验结果表明，DPR的集成不仅具备出色的适应能力，还能显著增强对IP盗用的抗性，且开销极小，因此非常适合应用于安全关键型场景。"
  },
  {
    "date": "2026-1-6",
    "title": "LLM En-powered UAV Networks Control for Efficient Wireless Communication",
    "authors": "Yue Zhang, Haibo Mei, Junyu Lai, Shuang Du",
    "publish": "2025 IEEE 102nd Vehicular Technology Conference (VTC2025-Fall)",
    "url": "https://doi.org/10.1109/vtc2025-fall65116.2025.11310529",
    "source": "IEEE",
    "abstract": "Currently, Large Language Model (LLM), as a typical Generative AI (GAI) implementation, has been deployed to lead to artificial general intelligence for numerous applications. Numbers of LLM releases, like ChatGPT, Llama, Gemini, ChatGLM, can online interact with users to give instructions against different kinds of queries. Moreover, LLMs can process multi-model data, like text, image, voice, video, to support the workings of robots, expert systems, and operation control systems. Due to its merits, LLM in theory can help the operation of Unmanned Aerial Vehicle (UAV) communication networks, where each UAV is constrained by its size, weight and power (SWAP) to provide limited data collection functions in the air. In this paper, we apply LLM to online control the operation of UAVs by optimizing the trajectory of UAV , to lead to UAV energy efficiency. We firstly provide well established air-ground wireless communication model within UAV and ground terminals (GTs) to provide API library to LLM to help its closed-loop reasoning. Further, we establish Chain-of-Thought (CoT) and generate optimizing instructions to support UAV communication via prompt engineering. In the end, such LLM en-powered UAV communication network will be validated via experiments. The numerical results validate that LLM can enable UAV communication networks to more adaptive to complicated environments to work with less energy consumption and higher energy efficiency.",
    "title_zh": "基于大语言模型赋能的无人机网络控制以实现高效无线通信",
    "abstract_zh": "目前，大型语言模型（LLM）作为生成式人工智能（GAI）的典型实现，已被广泛部署，推动了众多应用中的人工通用智能发展。诸如ChatGPT、Llama、Gemini、ChatGLM等大量LLM已上线，能够与用户在线交互，针对各类查询提供指导性响应。此外，LLM能够处理文本、图像、语音、视频等多种模态数据，支持机器人、专家系统及运行控制系统的工作。由于其诸多优势，理论上LLM可助力无人飞行器（UAV）通信网络的运行，其中每架UAV受限于尺寸、重量和功耗（SWAP），在空中仅能提供有限的数据采集功能。本文将LLM应用于UAV的在线控制，通过优化UAV轨迹以提升其能量效率。我们首先构建了UAV与地面终端（GTs）之间成熟的空-地无线通信模型，并为LLM提供API库，以支持其闭环推理。随后，通过提示工程（prompt engineering）建立思维链（Chain-of-Thought, CoT）并生成优化指令，以支持UAV通信。最终，通过实验验证该由LLM赋能的UAV通信网络。数值结果表明，LLM能够使UAV通信网络更适应复杂环境，在更低能耗下实现更高能量效率。"
  },
  {
    "date": "2026-1-6",
    "title": "AKP: Actionable Knowledge-Augmented Agent Planning with Large Language Models",
    "authors": "Hui Yu, Dongchen Zhu, Lei Wang, Jiamao Li",
    "publish": "2025 IEEE 23rd International Conference on Industrial Informatics (INDIN)",
    "url": "https://doi.org/10.1109/indin64977.2025.11279013",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable planning capabilities in complex language reasoning tasks. However, existing multi-step reasoning technologies easily introduces potential unreliable and inaccurate error accumulations over long-horizon action steps, and thereby making it exceedingly difficult to accurately explore the exponentially large search space. This deficiency primarily stems from the short-sighted greedy decoding of the next admissible action, and the lack of build-in actionable knowledge, which exacerbates the generation of hallucinatory or conflicting action sequences. To address these limitations, we propose Actionable Knowledge-augmented agent Planning (AKP), a novel LLM-based framework designed to enhance task planning of language agents by incorporating external actional knowledge. Specifically, we introduce a new knowledgeable policy called GVR by exploring future longer-term action paths, which leverages multiple domain foundation models (Verifier, Rewarder) trained on actional knowledge to jointly guide and calibrate more reasonable action generation. Additionally, AKP integrates the learned GVR into Monte Carlo Tree Search (MCTS) for deliberate planning, predicting various potential actions and iteratively refining alternative plans through lookahead and backtracking. Experimental results demonstrate that AKP significantly outperforms existing baselines, showcasing superior planning performance in tackling complex goal tasks across diverse embodied environments.",
    "title_zh": "AKP：基于大语言模型的可操作知识增强型智能体规划",
    "abstract_zh": "大型语言模型（LLMs）在复杂的语言推理任务中展现了卓越的规划能力。然而，现有的多步推理技术容易在长周期动作步骤中引入潜在的不可靠和不准确的误差累积，从而使得对指数级庞大的搜索空间进行精确探索变得极为困难。这一缺陷主要源于对下一步可接受动作的短视贪婪解码，以及缺乏内置的可操作性知识，这进一步加剧了幻觉或冲突动作序列的生成。为解决这些局限性，我们提出了一种名为“可操作知识增强型代理规划”（Actionable Knowledge-augmented agent Planning, AKP）的新型基于LLM的框架，旨在通过引入外部可操作知识来提升语言代理的任务规划能力。具体而言，我们提出了一种新的智能体策略——GVR（Goal-Verifiable Reasoning），通过探索更长远的未来动作路径，利用多个基于可操作知识训练的领域基础模型（验证器、奖励器）协同引导并校准更合理的动作生成。此外，AKP将学习到的GVR策略融入蒙特卡洛树搜索（MCTS）中，实现有意识的规划：预测多种潜在动作，并通过前瞻与回溯机制迭代优化备选方案。实验结果表明，AKP显著优于现有基线方法，在多样化具身环境中处理复杂目标任务时展现出卓越的规划性能。"
  },
  {
    "date": "2026-1-6",
    "title": "Large Language Model-Empowered Intent-Driven Network Configuration Generator",
    "authors": "Qiao Li, Chungang Yang, Yao Wang, Rongqian Fan, Pu Wang",
    "publish": "2025 IEEE 102nd Vehicular Technology Conference (VTC2025-Fall)",
    "url": "https://doi.org/10.1109/vtc2025-fall65116.2025.11310185",
    "source": "IEEE",
    "abstract": "With the advent of the sixth-generation (6G) era, the scale and complexity of communication networks have expanded dramatically, making conventional manual network management methods inefficient and error-prone. Intent-driven network (IDN) enables network operators to express high-level intents using natural language, which are then automatically translated into executable network configurations. However, the unstructured and ambiguous nature of intents poses challenges to achieving accurate intent-to-configuration translation. The emergence of a large language model (LLM) offers a promising solution to this problem. This paper proposes an LLM-empowered framework for generating IDN configurations, which integrates fine-tuning, retrieval-augmented generation, prompt engineering, and knowledge distillation techniques. We validate the effectiveness of the proposed framework through a network slicing use case implemented using open-source tools. Experimental results demonstrate that the proposed framework improves configuration generation time and accuracy by 24% and 25%, respectively, compared to the baseline schemes.",
    "title_zh": "大语言模型赋能的意图驱动网络配置生成器",
    "abstract_zh": "随着第六代（6G）时代的到来，通信网络的规模与复杂性急剧扩大，传统的手动网络管理方法已显得效率低下且容易出错。意图驱动网络（IDN）使网络运营商能够使用自然语言表达高层次的网络意图，这些意图随后可自动转化为可执行的网络配置。然而，意图本身具有非结构化和模糊性的特点，给准确实现“意图到配置”的转换带来了挑战。大型语言模型（LLM）的出现为解决这一问题提供了极具前景的方案。本文提出了一种基于LLM的IDN配置生成框架，该框架融合了微调、检索增强生成、提示工程以及知识蒸馏等多种技术。我们通过使用开源工具实现的一个网络切片应用场景验证了该框架的有效性。实验结果表明，相较于基线方案，所提出的框架在配置生成时间上提升了24%，准确率提高了25%。"
  },
  {
    "date": "2026-1-6",
    "title": "Identification of an Approach for the Execution of Skill-Based Domain-Specific Languages in the Area of Human-Robot Collaboration",
    "authors": "Lukas Buchner, Philipp Zallinger, Karin Nachbagauer, Alois Zoitl, Roman Froschauer",
    "publish": "2025 IEEE 23rd International Conference on Industrial Informatics (INDIN)",
    "url": "https://doi.org/10.1109/indin64977.2025.11279180",
    "source": "IEEE",
    "abstract": "Domain-specific languages (DSLs) can simplify the programming of human-robot collaboration (HRC) systems for non-experts. However, executing skill-based DSLs for HRC programming requires integrating various production resources such as robots, humans, and sensors. Therefore, domain overlapping requirements must be considered when choosing an appropriate execution approach. This paper identifies and explains key requirements such as flexible interfaces, dynamic capability discovery, task dependency resolution, spatial awareness, continuous monitoring, and user interfaces. Through a comprehensive literature review and gap analysis, we identify trade-offs in execution methodologies. Based on this, we propose an orchestration-based strategy that offers centralized coordination, extendable interface integration, and monitoring capabilities to address current limitations in executing DSLs in HRC applications.",
    "title_zh": "面向人机协作领域中基于技能的专用语言的执行方法识别",
    "abstract_zh": "领域特定语言（DSL）可以简化非专家编写人机协作（HRC）系统程序的过程。然而，执行基于技能的HRC DSL需要整合多种生产资源，如机器人、人类和传感器。因此，在选择合适的执行方式时，必须考虑各领域之间的重叠需求。本文识别并解释了若干关键需求，包括灵活的接口、动态能力发现、任务依赖关系解析、空间感知、持续监控以及用户界面。通过全面的文献回顾与差距分析，我们揭示了不同执行方法学之间的权衡。基于此，本文提出一种基于编排的策略，该策略提供集中式协调、可扩展的接口集成以及监控能力，以应对当前在HRC应用中执行DSL所面临的局限性。"
  },
  {
    "date": "2026-1-6",
    "title": "TeleMoM: Consensus-Driven Telecom Intelligence via Mixture of Models",
    "authors": "Xinquan Wang, Fenghao Zhu, Chongwen Huang, Zhaohui Yang, Zhaoyang Zhang, Sami Muhaidat, Chau Yuen, Mérouane Debbah",
    "publish": "2025 IEEE 102nd Vehicular Technology Conference (VTC2025-Fall)",
    "url": "https://doi.org/10.1109/vtc2025-fall65116.2025.11310195",
    "source": "IEEE",
    "abstract": "Large language models (LLMs) face significant challenges in specialized domains like telecommunication (Tele-com) due to technical complexity, specialized terminology, and rapidly evolving knowledge. Traditional methods, such as scaling model parameters or retraining on domain-specific corpora, are computationally expensive and yield diminishing returns, while existing approaches like retrieval-augmented generation, mixture of experts, and fine-tuning struggle with accuracy, efficiency, and coordination. To address this issue, we propose Telecom mixture of models (TeleMoM), a consensus-driven ensemble framework that integrates multiple LLMs for enhanced decision-making in Telecom. TeleMoM employs a two-stage process: proponent models generate justified responses, and an adjudicator finalizes decisions, supported by a quality-checking mechanism. This approach leverages strengths of diverse models to improve accuracy, reduce biases, and handle domain-specific complexities effectively. Evaluation results demonstrate that TeleMoM achieves a 9.7% increase in answer accuracy, highlighting its effectiveness in Telecom applications.",
    "title_zh": "TeleMoM：基于模型混合的共识驱动电信智能",
    "abstract_zh": "大型语言模型（LLMs）在电信等专业领域面临巨大挑战，原因在于技术复杂性、专业术语以及知识的快速迭代。传统的解决方案，如扩大模型参数规模或在领域特定语料上重新训练，不仅计算成本高昂，且收益递减；而现有的方法如检索增强生成、专家混合模型和微调，在准确性、效率和协同性方面仍存在不足。为解决这一问题，我们提出了一种名为Telecom混合模型（TeleMoM）的共识驱动集成框架，通过整合多个大语言模型，提升电信领域的决策能力。TeleMoM采用两阶段流程：提议模型生成有依据的响应，由仲裁者最终确定决策，并辅以质量检查机制。该方法充分发挥了不同模型的优势，有效提升了准确性，降低了偏差，并能高效应对领域特有的复杂性。评估结果表明，TeleMoM在答案准确率上提升了9.7%，充分展现了其在电信应用中的有效性。"
  },
  {
    "date": "2026-1-6",
    "title": "Towards Few-shot LLM-based Vulnerability Reproduction and Verification for Industrial Web Applications",
    "authors": "Gang Yang, Lin Ni, Shaofeng Lin, Tao Xia",
    "publish": "2025 IEEE 23rd International Conference on Industrial Informatics (INDIN)",
    "url": "https://doi.org/10.1109/indin64977.2025.11279514",
    "source": "IEEE",
    "abstract": "The inherent vulnerability of Web system poses a severe security threat to modern industrial systems. Nevertheless, automatically obtaining accurate and reproducible vulnerability intelligence in a time-acceptable manner remains a challenge. To address this issue, we propose VulRV, an end-to-end vulnerability reproduction and verification system for industrial Web applications. VulRV employs large language models (LLMs) with few-shot Chain-of-Thought (CoT) prompting to generate the environment configuration. And then VulRV utilizes tool-calling method to establish corresponding vulnerability reproduction and validation environment. To evaluate the effectiveness of the proposed method, we collected reports of 22 vulnerabilities of mainstream industrial Web application spanning four different categories. Experimental results demonstrate that our method can effectively extract information from the reports to construct Docker containers-based environments to automatically reproduce and validate all these 22 vulnerabilities. Thereby the results show VulRV are capable of supporting subsequent red-team testing and risk assessment of industrial Web applications.",
    "title_zh": "面向工业Web应用的少样本大模型漏洞复现与验证",
    "abstract_zh": "工业Web系统固有的脆弱性对现代工业系统构成了严重的安全威胁。然而，以可接受的时间成本自动获取准确且可复现的漏洞情报，仍面临巨大挑战。为解决这一问题，我们提出了VulRV——一种面向工业Web应用的端到端漏洞复现与验证系统。VulRV采用少量示例的思维链（Chain-of-Thought, CoT）提示技术，结合大语言模型（LLMs）生成环境配置；随后，通过工具调用方法构建相应的漏洞复现与验证环境。为评估所提方法的有效性，我们收集了涵盖四类主流工业Web应用的22个漏洞报告。实验结果表明，该方法能够有效从漏洞报告中提取信息，构建基于Docker容器的环境，自动复现并验证全部22个漏洞。结果证明，VulRV具备支持后续红队测试及工业Web应用风险评估的能力。"
  }
]