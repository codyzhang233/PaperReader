[
  {
    "date": "2025-12-02",
    "title": "The Evolutionary Ecology of Software: Constraints, Innovation, and the AI Disruption",
    "authors": "Sergi Valverde, Blai Vidiella, Salva Duran-Nebreda",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02953v1",
    "source": "arXiv",
    "abstract": "This chapter investigates the evolutionary ecology of software, focusing on the symbiotic relationship between software and innovation. An interplay between constraints, tinkering, and frequency-dependent selection drives the complex evolutionary trajectories of these socio-technological systems. Our approach integrates agent-based modeling and case studies, drawing on complex network analysis and evolutionary theory to explore how software evolves under the competing forces of novelty generation and imitation. By examining the evolution of programming languages and their impact on developer practices, we illustrate how technological artifacts co-evolve with and shape societal norms, cultural dynamics, and human interactions. This ecological perspective also informs our analysis of the emerging role of AI-driven development tools in software evolution. While large language models (LLMs) provide unprecedented access to information, their widespread adoption introduces new evolutionary pressures that may contribute to cultural stagnation, much like the decline of diversity in past software ecosystems. Understanding the evolutionary pressures introduced by AI-mediated software production is critical for anticipating broader patterns of cultural change, technological adaptation, and the future of software innovation.",
    "title_zh": "软件的进化生态学：约束、创新与人工智能的颠覆",
    "abstract_zh": "本章探讨了软件的演化生态，重点关注软件与创新之间的共生关系。约束、试错以及频率依赖性选择之间的相互作用，推动了这些社会技术系统复杂的演化轨迹。我们的研究方法结合了基于代理的建模与案例研究，运用复杂网络分析和演化理论，探索在新奇性生成与模仿的双重力量下，软件如何演进。通过考察编程语言的演变及其对开发者实践的影响，我们展示了技术物如何与社会规范、文化动态及人类互动共同演化并相互塑造。这一生态学视角还指导我们分析人工智能驱动开发工具在软件演化中日益凸显的作用。尽管大型语言模型（LLMs）为信息获取提供了前所未有的便利，但其广泛应用也引入了新的演化压力，可能引发类似过去软件生态系统中多样性衰退的文化停滞现象。理解由人工智能中介的软件生产所带来的演化压力，对于预测更广泛的文化变迁、技术适应趋势以及软件创新的未来具有重要意义。"
  },
  {
    "date": "2025-12-02",
    "title": "promptolution: A Unified, Modular Framework for Prompt Optimization",
    "authors": "Tom Zehle, Timo Heiß, Moritz Schlager, Matthias Aßenmacher, Matthias Feurer",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02840v1",
    "source": "arXiv",
    "abstract": "Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.",
    "title_zh": "promptolution：一种统一的、模块化的提示词优化框架",
    "abstract_zh": "提示优化已成为提升大型语言模型（LLMs）在各类任务中表现的关键手段。尽管众多研究论文展示了其有效性，但实际应用仍面临障碍，因为现有实现往往依赖于未维护且孤立的研究代码库。为解决这一问题，我们推出了 Promptolution——一个统一、模块化的开源框架，为从业者和研究人员提供 prompt 优化所需的所有组件，构建于一个可扩展的单一系统之中。该框架整合了多种前沿的离散提示优化器，同时对底层 LLM 实现保持无侵入性，具备高度灵活性与兼容性。"
  },
  {
    "date": "2025-12-02",
    "title": "Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents",
    "authors": "Zijie Lin, Qilin Cai, Liang Shen, Mingjun Xiao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02812v1",
    "source": "arXiv",
    "abstract": "Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\\% and 13\\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.",
    "title_zh": "通过无提示协作代理提升自动化论文复现",
    "abstract_zh": "自动化论文复现已成为加速科学研究的一项有前景的方法，通过多步骤工作流框架，系统地将学术论文转化为可执行代码。然而，现有框架通常缺乏在每一步生成过程中验证和优化输出的机制，或过度依赖人工设计的提示词进行自我改进，这限制了其适应性和可扩展性。为解决这些局限性，我们提出了一种无需提示词的协作智能体框架，能够自动提升从论文到代码生成的质量。该方法采用两个协作智能体：一个验证智能体，用于检查每一步的输出是否满足对应系统提示中规定的要求；另一个优化智能体，则根据识别出的问题对输出进行修正。与以往需要人类专家为每一步精心设计特定优化提示的方法不同，我们的框架仅利用原始系统提示即可实现自动验证与持续改进。我们将这一协作智能体集成到Paper2Code框架中，并在PaperBench Code-Dev和Paper2CodeBench数据集上进行了全面实验。实验结果表明，相较于未使用本框架的基线方法，我们的方法显著提升了复现代码的准确率和完整性，分别实现了约15%和13%的性能提升。此外，与Self-Refine方法的对比实验进一步验证了我们无提示词方法在不同数据集上的鲁棒性与一致性。"
  },
  {
    "date": "2025-12-02",
    "title": "Multi-Objective Agentic Rewrites for Unstructured Data Processing",
    "authors": "Lindsey Linxi Wei, Shreya Shankar, Sepanta Zeighami, Yeounoh Chung, Fatma Ozcan, Aditya G. Parameswaran",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02289v1",
    "source": "arXiv",
    "abstract": "One year ago, we open-sourced DocETL, a declarative system for LLM-powered data processing that, as of November 2025, has 3.2K GitHub stars and users across domains (e.g., journalism, law, medicine, policy, finance, and urban planning). In DocETL, users build pipelines by composing operators described in natural language, also known as semantic operators, with an LLM executing each operator's logic. However, due to complexity in the operator or the data it operates on, LLMs often give inaccurate results. To address this challenge, DocETL introduced rewrite directives, or abstract rules that guide LLM agents in rewriting pipelines by decomposing operators or data. For example, decomposing a single filter(\"is this email sent from an executive and discussing fraud?\") into the conjunction of two separate semantic filters may improve accuracy. However, DocETL only optimizes for accuracy, not cost. How do we optimize for both? We present MOAR (Multi-Objective Agentic Rewrites), a new optimizer for DocETL. To target cost optimization, we introduce two new categories of directives and extend all three existing categories with new ones, bringing the total to over 30 directives -- more than doubling what DocETL originally had. Moreover, since operators can interact with each other unpredictably due to LLM behavior, optimizing operators or sub-pipelines individually can yield suboptimal overall plans. Recognizing this, we design a new global search algorithm that explores rewrites in the context of entire pipelines. Since the space of rewrites is infinite -- pipelines can be rewritten in many ways, and each rewritten pipeline can itself be rewritten -- our algorithm adapts a multi-armed bandit framework to prioritize which pipelines to rewrite. Across six workloads, MOAR achieves 27% higher accuracy than ABACUS, the next-best optimizer, while matching its best accuracy at 55% of its cost.",
    "title_zh": "面向非结构化数据处理的多目标智能体重写技术",
    "abstract_zh": "一年前，我们开源了DocETL——一个由大语言模型（LLM）驱动的声明式数据处理系统。截至2025年11月，该系统已在GitHub上获得3.2万个星标，用户遍及新闻、法律、医疗、政策、金融和城市规划等多个领域。在DocETL中，用户通过组合以自然语言描述的“语义操作符”构建数据处理流水线，而每个操作符的逻辑由LLM执行。然而，由于操作符本身或其处理的数据过于复杂，LLM常常产生不准确的结果。为应对这一挑战，DocETL引入了“重写指令”（rewrite directives），即抽象规则，用于指导LLM代理通过分解操作符或数据来优化流水线。例如，将单一过滤器（“这封邮件是否由高管发送且涉及欺诈？”）拆分为两个独立的语义过滤器的合取形式，可能提升准确性。但此前的DocETL仅关注准确性，未考虑成本问题。如何同时优化准确性和成本？我们提出了MOAR（多目标智能体重写优化器），一种全新的DocETL优化器。\n\n为了实现成本优化，我们新增了两类重写指令，并对原有的三类指令进行了扩展，使总指令数量超过30种——是DocETL最初版本的两倍以上。此外，由于LLM行为的不确定性，各操作符之间可能产生不可预测的交互，单独优化某个操作符或子流水线往往难以获得全局最优方案。针对这一问题，我们设计了一种新的全局搜索算法，能够在整个流水线的上下文中探索各种重写策略。由于重写空间本质上是无限的——流水线可被多种方式重写，而每条重写后的流水线又可继续被重写——我们的算法借鉴了多臂老虎机（multi-armed bandit）框架，动态优先选择最具潜力的重写路径进行探索。\n\n在六个不同工作负载上的实验表明，MOAR在准确率上比当前最佳优化器ABACUS高出27%，同时在仅需其55%成本的情况下，达到了ABACUS的最佳准确率水平。"
  },
  {
    "date": "2025-12-02",
    "title": "Self-Improving AI Agents through Self-Play",
    "authors": "Przemyslaw Chojecki",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02731v1",
    "source": "arXiv",
    "abstract": "We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow. The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough. We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.",
    "title_zh": "通过自对弈实现自我提升的人工智能代理",
    "abstract_zh": "我们将心理测量学量表的模态理论框架拓展至动力系统领域。尽管以往研究已将AAI能力评分确立为代理表示空间上的静态泛函，本文则将代理形式化为由计算资源 $r$ 参数化的流 $ν_r$，该流受递归的生成-验证-更新（GVU）算子所支配。我们证明该算子在参数流形 $Θ$ 上生成一个向量场，并将自我改进系数 $κ$ 识别为能力泛函沿此流的李导数。本文的核心贡献在于推导出“方差不等式”——一种谱条件，该条件在适度正则性假设下足以保证自我改进的稳定性。我们进一步表明，$κ > 0$ 的一个充分条件是：在忽略曲率与步长效应的前提下，生成与验证过程的联合噪声必须足够小。随后，我们将这一形式化框架应用于统一近期关于语言自对弈（LSP）、自我修正以及合成数据自举的研究文献。我们证明，诸如STaR、SPIN、Reflexion、GANs和AlphaZero等架构，均为满足方差不等式的GVU算子的具体拓扑实现，其稳定性分别通过滤波机制、对抗性判别或在形式系统中的根基得以保障。"
  },
  {
    "date": "2025-12-02",
    "title": "G-PIFNN: A Generalizable Physics-informed Fourier Neural Network Framework for Electrical Circuits",
    "authors": "Ibrahim Shahbaz, Mohammad J. Abdel-Rahman, Eman Hammad",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02712v1",
    "source": "arXiv",
    "abstract": "Physics-Informed Neural Networks (PINNs) have advanced the data-driven solution of differential equations (DEs) in dynamic physical systems, yet challenges remain in explainability, scalability, and architectural complexity. This paper presents a Generalizable Physics-Informed Fourier Neural Network (G-PIFNN) framework that enhances PINN architectures for efficient and interpretable electrical circuit analysis. The proposed G-PIFNN introduces three key advancements: (1) improved performance and interpretability via a physics activation function (PAF) and a lightweight Physics-Informed Fourier Neural Network (PIFNN) architecture; (2) automated, bond graph (BG) based formulation of physics-informed loss functions for systematic differential equation generation; and (3) integration of intra-circuit and cross-circuit class transfer learning (TL) strategies, enabling unsupervised fine-tuning for rapid adaptation to varying circuit topologies. Numerical simulations demonstrate that G-PIFNN achieves significantly better predictive performance and generalization across diverse circuit classes, while significantly reducing the number of trainable parameters compared to standard PINNs.",
    "title_zh": "G-PIFNN：一种适用于电气电路的可推广物理信息傅里叶神经网络框架",
    "abstract_zh": "物理信息神经网络（PINNs）在动态物理系统中微分方程（DEs）的数据驱动求解方面取得了显著进展，但其在可解释性、可扩展性和架构复杂性方面仍面临挑战。本文提出了一种通用的物理信息傅里叶神经网络（G-PIFNN）框架，旨在提升PINN架构在高效且可解释的电路分析中的表现。所提出的G-PIFNN引入了三项关键改进：（1）通过物理激活函数（PAF）和轻量级物理信息傅里叶神经网络（PIFNN）架构，实现性能提升与可解释性的双重优化；（2）基于连接图（BG）自动生成物理信息损失函数，实现微分方程的系统化构建；（3）集成电路内部及跨电路类别迁移学习（TL）策略，支持无监督微调，从而快速适应不同电路拓扑结构。数值仿真结果表明，G-PIFNN在多种电路类别上均展现出显著更优的预测性能与泛化能力，同时相比标准PINNs大幅减少了可训练参数数量。"
  },
  {
    "date": "2025-12-02",
    "title": "Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks",
    "authors": "Zhiyuan He",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02677v1",
    "source": "arXiv",
    "abstract": "Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.",
    "title_zh": "探索大型语言模型在解决递归逻辑任务中的深度泛化能力",
    "abstract_zh": "大型语言模型在众多任务中展现了卓越的能力，但在处理递归推理问题时仍面临显著挑战，尤其是那些需要解决嵌套层次结构的问题。尽管以往研究已广泛探讨了长度泛化（即模型处理比训练期间更长序列的能力），我们则关注一个不同且尚未充分探索的局限性：深度泛化。这里的“深度”指的是层次问题中的嵌套层级数量，例如数学表达式中括号的层数，或布尔公式中逻辑子句的嵌套程度。我们的研究表明，标准的Transformer架构在面对训练过程中未遇到过的更深递归问题时表现不佳，即使它们在处理更长但非嵌套的序列时表现良好。这一局限性的根源在于其无法维持类似栈的行为——即追踪并解决多层嵌套依赖关系的能力。通过系统性分析，我们揭示了这种架构上的限制如何导致随着递归深度增加而迅速出现性能下降。为应对这一挑战，我们提出了一种新颖的循环式“定位-替换”处理流程，将递归问题分解为可管理的子组件。该方法采用两个专用模型：一个定位器用于识别可求解的子表达式，一个替换器则对这些组件进行求值，同时保持整体结构不变。我们在三个精心设计的领域中评估了该方法：布尔代数、递归算术和命题逻辑，每个领域均具备可控的递归深度。实验结果表明，当测试于分布外的递归深度时，我们的方法能有效缓解性能衰减问题。"
  },
  {
    "date": "2025-12-02",
    "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
    "authors": "Wenhao Yu, Zhenwen Liang, Chengsong Huang, Kishan Panaganti, Tianqing Fang, Haitao Mi, Dong Yu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02472v1",
    "source": "arXiv",
    "abstract": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
    "title_zh": "无需大量人工监督的引导式自进化大模型",
    "abstract_zh": "人工智能的自我演化长期以来被视为通向超级智能的一条路径，即模型能够自主地从自身学习经验中获取、优化并内化知识。然而在实践中，缺乏引导的自演化系统往往在训练初期便迅速达到瓶颈，甚至随着训练进程出现性能退化。这类失败主要源于概念漂移、多样性崩溃以及错误演化等问题，导致模型不断强化自身的偏见，并趋向于低熵的行为模式。为实现模型在低依赖人工监督的前提下稳定且可控的自我演化，我们提出了R-Few——一种通过上下文锚定与混合训练引入轻量级人类指导的引导式自对弈挑战者-求解器框架。在每一迭代周期中，挑战者（Challenger）会采样少量人工标注示例，以引导合成问题的生成；而求解器（Solver）则在在线、基于难度的课程机制下，联合训练于人类数据与合成数据之上。在数学推理和通用推理基准测试中，R-Few展现出持续且递进的性能提升。例如，Qwen3-8B-Base在数学任务上相较于R-Zero提升了+3.0分，其表现已与General-Reasoner相当，而后者所使用的高质量人工数据量是前者的20倍。消融实验验证了上下文锚定的挑战者训练与基于课程的求解器训练之间的互补作用，进一步分析表明，R-Few有效缓解了概念漂移问题，实现了更稳定、可控制的共演化动态。"
  },
  {
    "date": "2025-12-02",
    "title": "Cybersecurity AI: The World's Top AI Agent for Security Capture-the-Flag (CTF)",
    "authors": "Víctor Mayoral-Vilches, Luis Javier Navarrete-Lozano, Francesco Balassone, María Sanz-Gómez, Cristóbal R. J. Veas Chavez, Maite del Mundo de Torres, Vanesa Turiel",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02654v1",
    "source": "arXiv",
    "abstract": "Are Capture-the-Flag competitions obsolete? In 2025, Cybersecurity AI (CAI) systematically conquered some of the world's most prestigious hacking competitions, achieving Rank #1 at multiple events and consistently outperforming thousands of human teams. Across five major circuits-HTB's AI vs Humans, Cyber Apocalypse (8,129 teams), Dragos OT CTF, UWSP Pointer Overflow, and the Neurogrid CTF showdown-CAI demonstrated that Jeopardy-style CTFs have become a solved game for well-engineered AI agents. At Neurogrid, CAI captured 41/45 flags to claim the $50,000 top prize; at Dragos OT, it sprinted 37% faster to 10K points than elite human teams; even when deliberately paused mid-competition, it maintained top-tier rankings. Critically, CAI achieved this dominance through our specialized alias1 model architecture, which delivers enterprise-scale AI security operations at unprecedented cost efficiency and with augmented autonomy-reducing 1B token inference costs from $5,940 to just $119, making continuous security agent operation financially viable for the first time. These results force an uncomfortable reckoning: if autonomous agents now dominate competitions designed to identify top security talent at negligible cost, what are CTFs actually measuring? This paper presents comprehensive evidence of AI capability across the 2025 CTF circuit and argues that the security community must urgently transition from Jeopardy-style contests to Attack & Defense formats that genuinely test adaptive reasoning and resilience-capabilities that remain uniquely human, for now.",
    "title_zh": "网络安全AI：全球顶尖的网络安全夺旗赛（CTF）人工智能代理",
    "abstract_zh": "夺旗赛（Capture-the-Flag, CTF）是否已经过时？在2025年，网络安全人工智能（CAI）系统性地征服了全球最负盛名的黑客竞赛，连续在多项赛事中斩获第一名，并持续超越数千支人类参赛队伍。在五大主要赛事——HTB的人工智能 vs 人类赛、Cyber Apocalypse（参赛队伍达8,129支）、Dragos OT CTF、UWSP Pointer Overflow以及Neurogrid CTF对决——中，CAI证明了传统的“问答式”夺旗赛对经过精心设计的人工智能代理而言已基本成为“已解决”的问题。在Neurogrid比赛中，CAI成功夺取41/45个旗帜，赢得5万美元冠军奖金；在Dragos OT赛事中，其得分速度比顶尖人类团队快37%，达到10,000分；即使在比赛中被人为中断，它仍能保持顶级排名。关键在于，CAI的这一统治性表现得益于我们专有的alias1模型架构，该架构实现了企业级AI安全运营前所未有的成本效益与增强的自主性——将10亿次令牌推理成本从5,940美元降至仅119美元，首次使持续运行的安全智能体在经济上具备可行性。这些成果迫使我们直面一个令人不安的现实：如果如今的自主智能体已在几乎零成本的前提下主导了原本旨在甄别顶尖安全人才的竞赛，那么夺旗赛究竟在衡量什么？本文全面呈现了2025年夺旗赛电路中AI能力的实证数据，并强烈主张：安全界必须立即从传统的“问答式”竞赛转向“攻防对抗”（Attack & Defense）模式，唯有这种形式才能真正检验适应性推理与抗压韧性——这些能力目前仍为人类所独有。"
  },
  {
    "date": "2025-12-02",
    "title": "IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai",
    "authors": "Pengju Lu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02605v1",
    "source": "arXiv",
    "abstract": "This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.",
    "title_zh": "IACT：一种自组织递归模型，用于通用人工智能代理：关于kragent.ai架构的技术白皮书",
    "abstract_zh": "本文技术白皮书介绍了交互式代理调用树（IACT）——一种旨在解决静态、硬编码代理工作流局限性的计算模型。与传统系统需要预先定义图结构或专门编程不同，IACT作为一个通用的自主系统，完全由用户对话驱动。在给定高层次目标的前提下，该系统能够自主地逐步构建一个动态且递归的代理拓扑结构，其设计精准贴合问题本身的结构特征。这使得系统能够根据开放性任务的需求，灵活扩展其组织复杂度。为缓解单向函数调用固有的错误传播问题，IACT引入了交互冗余机制，将僵化的调用方式替换为双向、有状态的对话模式，从而实现在运行时对错误进行纠正和歧义的消解。本文详细阐述了该模型在kragent.ai系统中的架构设计、核心原则以及实际部署过程中的经验教训，并基于真实世界工作流的定性证据进行展示，而非依赖详尽的基准测试结果。"
  },
  {
    "date": "2025-12-02",
    "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
    "authors": "DeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenhao Xu, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Erhang Li, Fangqi Zhou, Fangyun Lin, Fucong Dai, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Li, Haofen Liang, Haoran Wei, Haowei Zhang, Haowen Luo, Haozhe Ji, Honghui Ding, Hongxuan Tang, Huanqi Cao, Huazuo Gao, Hui Qu, Hui Zeng, Jialiang Huang, Jiashi Li, Jiaxin Xu, Jiewen Hu, Jingchang Chen, Jingting Xiang, Jingyang Yuan, Jingyuan Cheng, Jinhua Zhu, Jun Ran, Junguang Jiang, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Kexin Huang, Kexing Zhou, Kezhao Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Wang, Liang Zhao, Liangsheng Yin, Lihua Guo, Lingxiao Luo, Linwang Ma, Litong Wang, Liyue Zhang, M. S. Di, M. Y Xu, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Panpan Huang, Peixin Cong, Peiyi Wang, Qiancheng Wang, Qihao Zhu, Qingyang Li, Qinyu Chen, Qiushi Du, Ruiling Xu, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runqiu Yin, Runxin Xu, Ruomeng Shen, Ruoyu Zhang, S. H. Liu, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaofei Cai, Shaoyuan Chen, Shengding Hu, Shengyu Liu, Shiqiang Hu, Shirong Ma, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, Songyang Zhou, Tao Ni, Tao Yun, Tian Pei, Tian Ye, Tianyuan Yue, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjie Pang, Wenjing Luo, Wenjun Gao, Wentao Zhang, Xi Gao, Xiangwen Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaokang Zhang, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xingyou Li, Xinyu Yang, Xinyuan Li, Xu Chen, Xuecheng Su, Xuehai Pan, Xuheng Lin, Xuwei Fu, Y. Q. Wang, Yang Zhang, Yanhong Xu, Yanru Ma, Yao Li, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Qian, Yi Yu, Yichao Zhang, Yifan Ding, Yifan Shi, Yiliang Xiong, Ying He, Ying Zhou, Yinmin Zhong, Yishi Piao, Yisong Wang, Yixiao Chen, Yixuan Tan, Yixuan Wei, Yiyang Ma, Yiyuan Liu, Yonglun Yang, Yongqiang Guo, Yongtong Wu, Yu Wu, Yuan Cheng, Yuan Ou, Yuanfan Xu, Yuduan Wang, Yue Gong, Yuhan Wu, Yuheng Zou, Yukun Li, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehua Zhao, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhixian Huang, Zhiyu Wu, Zhuoshu Li, Zhuping Zhang, Zian Xu, Zihao Wang, Zihui Gu, Zijia Zhu, Zilin Li, Zipeng Zhang, Ziwei Xie, Ziyi Gao, Zizheng Pan, Zongqing Yao, Bei Feng, Hui Li, J. L. Cai, Jiaqi Ni, Lei Xu, Meng Li, Ning Tian, R. J. Chen, R. L. Jin, S. S. Li, Shuang Zhou, Tianyu Sun, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xinnan Song, Xinyi Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Dongjie Ji, Jian Liang, Jianzhong Guo, Jin Chen, Leyi Xia, Miaojun Wang, Mingming Li, Peng Zhang, Ruyi Chen, Shangmian Sun, Shaoqing Wu, Shengfeng Ye, T. Wang, W. L. Xiao, Wei An, Xianzu Wang, Xiaowen Sun, Xiaoxiang Wang, Ying Tang, Yukun Zha, Zekai Zhang, Zhe Ju, Zhen Zhang, Zihua Qu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02556v1",
    "source": "arXiv",
    "abstract": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
    "title_zh": "DeepSeek-V3.2：推动开源大语言模型的前沿发展",
    "abstract_zh": "我们推出 DeepSeek-V3.2，这是一款在计算效率、推理能力与智能体表现之间实现卓越平衡的模型。DeepSeek-V3.2 的关键技术突破如下：  \n(1) **深度稀疏注意力机制（DeepSeek Sparse Attention, DSA）**：我们提出 DSA，一种高效的注意力机制，在显著降低计算复杂度的同时，仍能保持长序列场景下的模型性能。  \n(2) **可扩展的强化学习框架**：通过实施稳健的强化学习协议并大幅扩展后训练阶段的算力投入，DeepSeek-V3.2 的表现已接近 GPT-5 水平。尤为突出的是，我们的高算力版本——DeepSeek-V3.2-Speciale，在多项评测中超越 GPT-5，并展现出与 Gemini-3.0-Pro 相当的推理能力，成功在 2025 年国际数学奥林匹克竞赛（IMO）和国际信息学奥林匹克竞赛（IOI）中斩获金牌，充分体现了其顶尖的综合智能水平。  \n(3) **大规模智能体任务合成流水线**：为将推理能力有效融入工具使用场景，我们开发了一种创新的规模化数据合成流程，能够系统性地生成海量高质量训练数据。该方法支持可扩展的智能体后训练，显著提升了模型在复杂、交互式环境中的泛化能力与指令遵循鲁棒性。"
  },
  {
    "date": "2025-12-02",
    "title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning",
    "authors": "Songqiao Su, Xiaofei Sun, Xiaoya Li, Albert Wang, Jiwei Li, Chris Shum",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02551v1",
    "source": "arXiv",
    "abstract": "In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\\it cuBLAS}, {\\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\\% over {\\it torch.matmul} on average; +19.2\\% over {\\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\\% over {\\it cuBLASLt-heuristic}, which queries {\\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\\% over the most competitive {\\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\\%, +26.0\\%, +22.4\\%, and +15.9\\% for {\\it torch.matmul}, {\\it cuBLAS}, {\\it cuBLASLt-heuristic}, and {\\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2",
    "title_zh": "CUDA-L2：通过强化学习超越cuBLAS的矩阵乘法性能",
    "abstract_zh": "本文提出CUDA-L2，一个将大语言模型（LLMs）与强化学习（RL）相结合的系统，用于自动优化半精度通用矩阵乘法（HGEMM）CUDA内核。该系统以CUDA执行速度作为强化学习的奖励信号，能够自动在1,000种配置中对HGEMM内核进行优化。CUDA-L2在离线模式下系统性地超越了迄今为止主要的矩阵乘法基线，包括广泛使用的{\\it torch.matmul}以及最先进的Nvidia闭源库{\\it cuBLAS}和{\\it cuBLASLt}。\n\n在离线模式（即内核连续执行、无时间间隔）下，CUDA-L2相比{\\it torch.matmul}平均提升22.0%；相比使用最优布局配置（正常-正常NN和转置-正常TN）的{\\it cuBLAS}提升19.2%；相比基于启发式建议从{\\it cuBLASLt}库中选择算法的{\\it cuBLASLt-heuristic}提升16.8%；相比从{\\it cuBLASLt}建议中筛选出最多100个候选算法并选择最快方案的最先进模型{\\it cuBLASLt-AutoTuning}，仍实现11.4%的性能提升。\n\n在服务器模式（即内核以随机间隔执行，模拟真实推理场景）下，性能提升进一步扩大：分别达到{\\it torch.matmul}的+28.7%、{\\it cuBLAS}的+26.0%、{\\it cuBLASLt-heuristic}的+22.4%，以及{\\it cuBLASLt-AutoTuning}的+15.9%。\n\nCUDA-L2表明，即使对于像HGEMM这样已高度优化、性能至关重要的核心内核，也能通过LLM引导的强化学习自动化方法，在人类难以企及的规模上系统性探索配置空间，从而实现显著性能提升。项目与代码可在github.com/deepreinforce-ai/CUDA-L2获取。"
  },
  {
    "date": "2025-12-02",
    "title": "InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration",
    "authors": "Zhongyu Yang, Yingfang Yuan, Xuanming Jiang, Baoyi An, Wei Pang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02981v1",
    "source": "arXiv",
    "abstract": "Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness.",
    "title_zh": "InEx：通过内省与跨模态多智能体协作实现幻觉缓解",
    "abstract_zh": "幻觉仍然是大型语言模型（LLMs）面临的一个关键挑战，阻碍了可靠多模态大语言模型（MLLMs）的发展。现有解决方案通常依赖人工干预，或未能充分挖掘智能体自主缓解幻觉的潜力。为解决这些局限性，我们受到人类在现实世界中做出可靠决策方式的启发：人们首先通过内省式推理来降低不确定性并形成初步判断，随后借助来自不同视角的外部验证以达成最终决策。受这一认知范式的启发，我们提出了InEx——一种无需训练的多智能体框架，旨在自主缓解幻觉问题。InEx引入了基于熵的不确定性估计指导的内部内省推理机制，以提升决策智能体推理过程的可靠性。该智能体首先生成响应，随后通过与编辑智能体及自我反思智能体进行跨模态的多智能体协作，迭代地进行验证与优化，进一步增强结果的可靠性并有效抑制幻觉。大量实验表明，InEx持续优于现有方法，在通用任务和幻觉检测基准上分别实现了4%至27%的性能提升，并展现出强大的鲁棒性。"
  },
  {
    "date": "2025-12-02",
    "title": "Mapping code on Coarse Grained Reconfigurable Arrays using a SAT solver",
    "authors": "Cristian Tirelli, Laura Pozzi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02884v1",
    "source": "arXiv",
    "abstract": "Emerging low-powered architectures like Coarse-Grain Reconfigurable Arrays (CGRAs) are becoming more common. Often included as co-processors, they are used to accelerate compute-intensive workloads like loops. The speedup obtained is defined by the hardware design of the accelerator and by the quality of the compilation. State of the art (SoA) compilation techniques leverage modulo scheduling to minimize the Iteration Interval (II), exploit the architecture parallelism and, consequentially, reduce the execution time of the accelerated workload. In our work, we focus on improving the compilation process by finding the lowest II for any given topology, through a satisfiability (SAT) formulation of the mapping problem. We introduce a novel schedule, called Kernel Mobility Schedule, to encode all the possible mappings for a given Data Flow Graph (DFG) and for a given II. The schedule is used together with the CGRA architectural information to generate all the constraints necessary to find a valid mapping. Experimental results demonstrate that our method not only reduces compilation time on average but also achieves higher quality mappings compared to existing SoA techniques.",
    "title_zh": "使用SAT求解器在粗粒度可重构阵列上进行代码映射",
    "abstract_zh": "近年来，像粗粒度可重构阵列（CGRAs）这类低功耗架构正变得越来越普遍。它们通常作为协处理器使用，用于加速计算密集型任务，如循环运算。所获得的性能提升取决于加速器的硬件设计以及编译质量。当前最先进的（SoA）编译技术利用模调度（modulo scheduling）来最小化迭代间隔（II），充分挖掘架构的并行性，从而显著降低加速任务的执行时间。在本研究中，我们致力于改进编译过程，通过将映射问题建模为可满足性（SAT）问题，以针对任意给定拓扑找到最低的II。我们提出了一种新颖的调度策略，称为“核迁移调度”（Kernel Mobility Schedule），用于编码给定数据流图（DFG）和给定II下所有可能的映射方案。该调度与CGRA的架构信息结合，生成构建有效映射所需的所有约束条件。实验结果表明，我们的方法不仅平均显著缩短了编译时间，而且相比现有最先进的技术，能够生成更高品质的映射方案。"
  },
  {
    "date": "2025-12-02",
    "title": "SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures",
    "authors": "Cristian Tirelli, Lorenzo Ferretti, Laura Pozzi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02875v1",
    "source": "arXiv",
    "abstract": "Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration. We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \\textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved. Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.",
    "title_zh": "SAT-MapIt：一种基于SAT的粗粒度可重构架构调度映射方法",
    "abstract_zh": "粗粒度可重构阵列（CGRAs）是一种新兴的低功耗架构，旨在加速计算密集型应用循环。然而，CGRA所能提供的加速效果在很大程度上取决于映射质量，即循环在给定平台上的编译效率。当前最先进的编译技术通过模调度（modulo scheduling）实现映射，该策略旨在最小化执行循环所需的迭代间隔（II），通常借助经典的图算法（如最大团枚举）来完成。本文则采用一种基于SAT的建模方法来解决映射问题，从而比现有先进技术更有效地探索解空间。为构建SAT问题，我们提出了一种专用调度方案——**核移动调度**（Kernel Mobility Schedule, KMS），结合数据流图与CGRA的架构信息，生成一组布尔命题，以描述在特定II下映射必须满足的所有约束条件。随后，利用SAT求解器高效地搜索这一复杂空间。与其它先进方法类似，该过程也是迭代进行的：若对于当前II不存在有效映射，则增大II值，重新生成KMS及相应约束并再次求解。实验结果表明，在所测试的基准测试中，SAT-MapIt相较于现有先进方法在47.72%的情况下取得了更优结果：有时能够获得更低的II，有时甚至在以往无法找到有效映射的情况下成功找到了可行解。"
  },
  {
    "date": "2025-12-02",
    "title": "Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning",
    "authors": "Haonan Wang, Chao Du, Kenji Kawaguchi, Tianyu Pang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02874v1",
    "source": "arXiv",
    "abstract": "Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a \"majority\" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.",
    "title_zh": "思维并行，答案合一：用于开放性推理的逻辑平均法",
    "abstract_zh": "多数投票在闭合式问答中已被证明是有效的，通过聚合并行的推理轨迹来提升性能。然而，对于开放式推理任务（如代码生成和基于网络的深度研究），由于完整解决方案之间不存在明确的“多数”概念，该方法无法直接应用。我们提出了ThinkMerge——一种无需训练、即插即用的解码策略：它运行K条并行推理轨迹，在同步点对下一词的logits进行平均，从而生成一个连贯的单一输出。ThinkMerge可无缝集成至vLLM/SGLang，并与标准解码技术（如Top-p/Top-k）兼容。实验表明，ThinkMerge在AIME和GPQA任务上达到或超越了多数投票的表现，同时在开放式编程任务中持续带来显著提升：在LiveCodeBench（高难度）上，DeepCoder-14B-Preview的pass@1提升了+8.28%，Qwen3-8B提升了+7.58%。此外，ThinkMerge还显著改进了基于网络的深度研究代理（如WebSailor-7B/32B），在GAIA、BrowseComp-en/zh以及XbenchDeepSearch等多个基准上均取得优异表现。这些结果表明，通过并行的测试时扩展，可以在不依赖对完整输出进行投票的前提下，有效提升开放式推理能力。"
  },
  {
    "date": "2025-12-02",
    "title": "Network Self-Configuration based on Fine-Tuned Small Language Models",
    "authors": "Oscar G. Lira, Oscar M. Caicedo, Nelson L. S. Da Fonseca",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02861v1",
    "source": "arXiv",
    "abstract": "As modern networks grow in scale and complexity, manual configuration becomes increasingly inefficient and prone to human error. While intent-driven self-configuration using large language models has shown significant promise, such models remain computationally expensive, resource-intensive, and often raise privacy concerns because they typically rely on external cloud infrastructure. This work introduces SLM_netconfig, a fine-tuned small language model framework that uses an agent-based architecture and parameter-efficient adaptation techniques to translate configuration intents expressed as natural language requirements or questions into syntactically and semantically valid network configurations. The system is trained on a domain-specific dataset generated through a pipeline derived from vendor documentation, ensuring strong alignment with real-world configuration practices. Extensive evaluation shows that SLM_netconfig, when using its question-to-configuration model, achieves higher syntactic accuracy and goal accuracy than LLM-NetCFG while substantially reducing translation latency and producing concise, interpretable configurations. These results demonstrate that fine-tuned small language models, as implemented in SLM_netconfig, can deliver efficient, accurate, and privacy-preserving automated configuration generation entirely on-premise, making them a practical and scalable solution for modern autonomous network configuration.",
    "title_zh": "基于微调的小型语言模型的网络自配置",
    "abstract_zh": "随着现代网络规模和复杂性的不断增长，手动配置变得越来越低效且容易出错。尽管基于大语言模型的意图驱动自配置技术展现出巨大潜力，但这些模型通常计算成本高昂、资源消耗大，并因依赖外部云基础设施而引发隐私担忧。本文提出了一种名为SLM_netconfig的小型语言模型框架，该框架采用基于智能体的架构，并结合参数高效适配技术，能够将自然语言表达的配置意图（如需求或问题）转化为语法正确且语义合理的网络配置。系统在通过厂商文档构建的领域特定数据集上进行训练，确保其与实际配置实践高度一致。大量实验评估表明，当使用其“问题到配置”模型时，SLM_netconfig在语法准确率和目标达成率方面均优于LLM-NetCFG，同时显著降低翻译延迟，并生成简洁、可解释的配置结果。这些成果表明，如SLM_netconfig所实现的微调小型语言模型，能够在本地完全部署的情况下，提供高效、准确且保护隐私的自动化配置生成能力，为现代自主网络配置提供了一种实用且可扩展的解决方案。"
  },
  {
    "date": "2025-12-02",
    "title": "\"Can you feel the vibes?\": An exploration of novice programmer engagement with vibe coding",
    "authors": "Kiev Gama, Filipe Calegario, Victoria Jackson, Alexander Nolte, Luiz Augusto Morais, Vinicius Garcia",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02750v1",
    "source": "arXiv",
    "abstract": "Emerging alongside generative AI and the broader trend of AI-assisted coding, the term \"vibe coding\" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.",
    "title_zh": "“你能感受到氛围吗？”：对新手程序员与“ vibe 编码”互动的探索",
    "abstract_zh": "随着生成式人工智能及更广泛的AI辅助编程趋势的兴起，“氛围编码”（vibe coding）这一术语应运而生，指通过自然语言提示而非直接编写代码来创建软件。这种方法有望实现软件开发的民主化，但其教育意义仍待深入探讨。本文报告了一项为期一天的教育型黑客松活动的研究成果，旨在探究初学者程序员与经验混合团队如何参与氛围编码。我们在巴西一所公立大学组织了一场包容性活动，共有31名来自计算机与非计算机学科的本科生参与，分为九支队伍。通过观察、结项问卷调查以及半结构化访谈，我们考察了创作过程、工具使用模式、协作动态及学习成效。研究发现，氛围编码促进了快速原型设计和跨学科合作，参与者在时间限制内发展出提示工程技能，并成功交付了可运行的演示。然而，我们也观察到创意构思过早收敛、代码质量参差不齐需返工，以及对核心软件工程实践参与度不足等问题。各团队采用了结合多个AI工具的复杂工作流，以流水线形式协同运作，而人类判断在关键环节的优化中依然不可或缺。9小时的紧凑时长对新手而言有效提升了信心，同时也能容纳时间有限的参与者。我们得出结论：当辅以明确的支持机制——如促进发散性思维、批判性评估AI输出结果，以及对生产质量建立合理预期——氛围编码黑客松可成为极具价值的低风险学习环境。"
  },
  {
    "date": "2025-12-02",
    "title": "Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System",
    "authors": "Martin Weiss, Jesko Hecking-Harbusch, Jochen Quante, Matthias Woehrle",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02567v1",
    "source": "arXiv",
    "abstract": "The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes. We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables. Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.",
    "title_zh": "基于大语言模型的软件工程中的反馈回路与代码扰动：一个C到Rust翻译系统的案例研究",
    "abstract_zh": "强大的生成式人工智能对软件工程中的诸多任务产生了显著影响，例如代码修复、测试生成或语言翻译。尽管像 GitHub Copilot 这样的工具已在交互式场景中广泛使用，但自动化方法在工业实践中投入使用之前仍需具备更高的可靠性。本文聚焦于三个直接影响结果质量的方面：a）自动化反馈循环的影响；b）大型语言模型（LLM）的选择；c）保持行为不变的代码修改的影响。我们以自动化的 C 语言到 Rust 语言的代码转换系统为例，研究这三个变量的作用。由于 Rust 语言具有安全保证，C 到 Rust 的代码转换在工业界具有吸引力。该转换系统基于“生成-验证”模式：由 LLM 生成的 Rust 代码会自动检查其是否可编译，并与原始 C 代码在行为上是否等价。对于验证失败的情况，系统将通过反馈循环重新提示 LLM 以修复输出。这些验证机制也使我们能够评估并比较在改变上述三个变量时，翻译系统的成功率。我们的实验结果表明，在没有反馈循环的情况下，LLM 的选择对翻译成功率有显著影响；然而，当系统引入反馈循环后，不同模型之间的差异明显减小。这一现象在系统平均性能以及面对代码扰动时的鲁棒性方面均得到验证。最后，我们还发现，代码扰动带来的多样性甚至能提升系统的整体表现。"
  },
  {
    "date": "2025-12-02",
    "title": "Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior",
    "authors": "Marcus Kessel",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02795v1",
    "source": "arXiv",
    "abstract": "Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse",
    "title_zh": "面向观察湖仓：软件行为的活态、交互式档案",
    "abstract_zh": "代码生成大模型主要基于静态数据（如源代码、注释、规格说明）进行训练，很少接触运行时行为的实例化表现。因此，它们容易内化存在缺陷或标签错误的代码。由于非平凡的语义属性在一般情况下是不可判定的，获取真实功能的唯一可行方法是通过动态观察程序执行过程。在之前的工作中，我们提出了序列表（Sequence Sheets）、刺激-响应矩阵（SRMs）和刺激-响应立方体（SRCs），用于捕捉并比较不同测试、实现和上下文中的行为。这些结构使观测数据能够离线分析和重复使用，但它们本身并不具备持久化、演化能力或大规模交互式分析的支持。\n\n因此，在本文中，我们引入了“观测湖仓”（Observation Lakehouses），以实现持续的SRC操作：一个高且仅追加的观测表，存储每一次动作（刺激、响应、上下文）的完整记录，并通过SQL查询按需物化SRC切片。该湖仓基于Apache Parquet + Iceberg + DuckDB构建，可从受控流水线（LASSO）和CI流水线（如单元测试执行）中摄入数据，从而支持n版本评估、行为聚类以及共识预言机，而无需重新执行。在包含509个问题的基准测试中，我们摄入了约860万条观测行（小于51MiB），并在笔记本电脑上以不到100毫秒的时间重建SRM/SRC视图与聚类结果，证明了持续行为挖掘在无需分布式机器集群的情况下同样具有可行性。\n\n这使得行为真实信息成为与其它运行时数据同等重要的第一类数据，并为行为感知的评估与训练提供了基础设施路径。观测湖仓及其配套数据集已作为开源项目在GitHub上公开发布：https://github.com/SoftwareObservatorium/observation-lakehouse"
  },
  {
    "date": "2025-12-02",
    "title": "Decentralized Multi-Agent System with Trust-Aware Communication",
    "authors": "Yepeng Ding, Ahmed Twabi, Junwei Yu, Lingfeng Zhang, Tohru Kondo, Hiroyuki Sato",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02410v1",
    "source": "arXiv",
    "abstract": "The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.",
    "title_zh": "具有信任感知通信的去中心化多智能体系统",
    "abstract_zh": "大型语言模型（LLMs）的出现正迅速推动自主多智能体系统（MAS）的发展，为“智能体互联网”的实现铺平了道路。然而，传统的集中式多智能体架构面临诸多挑战，包括单点故障、易受审查、固有的可扩展性限制以及关键的信任问题。为此，我们提出一种新型的去中心化多智能体系统（DMAS）架构，旨在通过实现信任感知、可扩展且抗审查的自主智能体间交互，从根本上解决上述问题。我们的DMAS基于区块链架构，构建了一个去中心化的智能体运行时。我们形式化设计了一种信任感知的通信协议，该协议利用密码学原语和链上操作，提供了可验证的交互周期、通信完整性、身份真实性、不可否认性以及条件保密性等安全特性，并通过全面的安全分析进一步验证了其有效性。性能分析结果表明，DMAS是一种可扩展且高效的解决方案，适用于构建可信的多智能体系统。"
  },
  {
    "date": "2025-12-02",
    "title": "Process-Centric Analysis of Agentic Software Systems",
    "authors": "Shuyang Liu, Yang Chen, Rahul Krishna, Saurabh Sinha, Jatin Ganhotra, Reyhan Jabbarvand",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02393v1",
    "source": "arXiv",
    "abstract": "Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success. Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.",
    "title_zh": "面向过程的智能软件系统分析",
    "abstract_zh": "代理系统是现代软件系统：它们由协调的模块组成，提供接口，并部署在软件流水线中。与传统程序不同，其执行过程（即轨迹）本质上具有随机性，并能自适应地调整以应对所解决的问题。对这类系统的评估通常以结果为中心，仅根据最终步骤的成功或失败来判断性能。这种狭隘的关注忽略了对系统内部运作的深入洞察，无法解释代理如何推理、规划、行动或随时间改变策略。受传统软件系统以图结构表示的启发，我们提出了Graphectory，一种系统化编码此类软件系统中时序与语义关系的方法。Graphectory有助于设计以流程为中心的度量和分析方法，从而在不依赖最终成功的情况下评估代理工作流的质量。\n\n利用Graphectory，我们对两种主流代理编程工作流——SWE-agent和OpenHands——共4000条轨迹进行了分析，结合四种基础大型语言模型（LLMs），尝试解决SWE-bench Verified中的问题。我们的全自动化分析揭示了以下发现：(1) 使用更丰富提示或更强LLM的代理展现出更复杂的Graphectory结构，反映出更深入的探索、更广泛的上下文收集以及在提交补丁前更彻底的验证；(2) 代理的问题求解策略既受问题难度影响，也受底层LLM的影响——对于已解决的问题，策略通常遵循连贯的“定位-修复-验证”步骤；而对于未解决的问题，则表现出混乱、重复或回溯等行为；(3) 即使最终成功，代理编程系统也常常表现出低效的过程，导致轨迹不必要的延长。"
  },
  {
    "date": "2025-12-02",
    "title": "SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification",
    "authors": "Zhendong Tan, Xingjun Zhang, Chaoyi Hu, Junjie Peng, Kun Xia",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02337v1",
    "source": "arXiv",
    "abstract": "Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation.",
    "title_zh": "SpecPV：通过部分验证提升自推测解码在长上下文生成中的性能",
    "abstract_zh": "随着代码生成、深度推理和长文档理解等任务需求的不断增长，长上下文生成已成为大型语言模型（LLMs）的一项关键能力。推测解码（Speculative Decoding）是加速生成过程最直接且高效的方法之一。它采用“草案-验证”范式：由一个轻量级的草案模型提出若干候选词元，再由目标模型进行验证。然而，我们发现随着上下文长度的增加，验证阶段逐渐成为主要瓶颈。为在长上下文生成中进一步提升推测解码的速度，我们提出了 SpecPV——一种自推测解码方法，通过使用部分键值状态（KV）实现快速验证，并定期执行完整验证以消除累积误差。我们在多个长上下文基准测试和模型上验证了 SpecPV 的有效性，包括 LLaMA-3.1-8B-Instruct 和 Qwen3 系列模型。实验结果表明，与标准自回归解码相比，SpecPV 可实现高达 6 倍的解码速度提升，同时仅带来微小的性能下降。"
  },
  {
    "date": "2025-12-02",
    "title": "Invasive Context Engineering to Control Large Language Models",
    "authors": "Thomas Rivasseau",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.03001v1",
    "source": "arXiv",
    "abstract": "Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.",
    "title_zh": "入侵式上下文工程以控制大型语言模型",
    "abstract_zh": "当前关于大语言模型（LLM）操作控制的研究通过在偏好示例、提示词设计以及输入/输出过滤等方面进行训练，提升了模型对对抗攻击和异常行为的鲁棒性。尽管取得了良好效果，LLM 仍容易被滥用，且随着上下文长度的增加，越狱（jailbreak）概率显著上升。在长上下文场景下，亟需具备可靠的 LLM 安全保障机制。为此，我们提出将控制语句插入 LLM 上下文，以实现“侵入式上下文工程”（Invasive Context Engineering），部分解决该问题。我们进一步建议，该技术可推广至思维链（Chain-of-Thought）过程，以防止模型策划规避行为。与依赖模型训练的方法不同，侵入式上下文工程无需额外训练，从而避免了在长上下文情境中因数据不足而导致的训练困境。"
  },
  {
    "date": "2025-12-02",
    "title": "LeechHijack: Covert Computational Resource Exploitation in Intelligent Agent Systems",
    "authors": "Yuanhe Zhang, Weiliu Wang, Zhenhong Zhou, Kun Wang, Jie Zhang, Li Sun, Yang Liu, Sen Su",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02321v1",
    "source": "arXiv",
    "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in reasoning, planning, and tool usage. The recently proposed Model Context Protocol (MCP) has emerged as a unifying framework for integrating external tools into agent systems, enabling a thriving open ecosystem of community-built functionalities. However, the openness and composability that make MCP appealing also introduce a critical yet overlooked security assumption -- implicit trust in third-party tool providers. In this work, we identify and formalize a new class of attacks that exploit this trust boundary without violating explicit permissions. We term this new attack vector implicit toxicity, where malicious behaviors occur entirely within the allowed privilege scope. We propose LeechHijack, a Latent Embedded Exploit for Computation Hijacking, in which an adversarial MCP tool covertly expropriates the agent's computational resources for unauthorized workloads. LeechHijack operates through a two-stage mechanism: an implantation stage that embeds a benign-looking backdoor in a tool, and an exploitation stage where the backdoor activates upon predefined triggers to establish a command-and-control channel. Through this channel, the attacker injects additional tasks that the agent executes as if they were part of its normal workflow, effectively parasitizing the user's compute budget. We implement LeechHijack across four major LLM families. Experiments show that LeechHijack achieves an average success rate of 77.25%, with a resource overhead of 18.62% compared to the baseline. This study highlights the urgent need for computational provenance and resource attestation mechanisms to safeguard the emerging MCP ecosystem.",
    "title_zh": "LeechHijack：智能代理系统中的隐蔽计算资源劫持",
    "abstract_zh": "基于大语言模型（LLM）的智能体在推理、规划和工具使用方面展现了卓越的能力。近期提出的模型上下文协议（Model Context Protocol, MCP）已成为将外部工具集成到智能体系统中的统一框架，推动了由社区共建功能的繁荣开放生态的发展。然而，MCP所具备的开放性与可组合性虽然令人向往，也引入了一个关键却常被忽视的安全假设——对第三方工具提供者的隐式信任。在本研究中，我们识别并形式化了一类新型攻击，这类攻击利用该信任边界，在不违反显式权限的前提下实施恶意行为。我们称这种新型攻击向量为“隐性毒性”（implicit toxicity），即恶意行为完全发生于允许的权限范围内。为此，我们提出了LeechHijack：一种用于计算劫持的潜在嵌入式漏洞利用技术。在此攻击中，恶意的MCP工具会隐蔽地窃取智能体的计算资源，用于执行未经授权的工作负载。LeechHijack采用两阶段机制：第一阶段为植入阶段，将看似无害的后门嵌入工具；第二阶段为利用阶段，当满足预设触发条件时，后门激活并建立命令与控制通道。通过该通道，攻击者可注入额外任务，使智能体如同执行正常工作流一般执行这些任务，从而实质上寄生用户所分配的计算预算。我们在四大主流LLM家族上实现了LeechHijack。实验结果表明，该攻击平均成功率高达77.25%，相较于基线仅带来18.62%的资源开销。本研究凸显了迫切需要建立计算溯源与资源认证机制，以保护新兴的MCP生态系统免受此类威胁。"
  },
  {
    "date": "2025-12-02",
    "title": "When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers",
    "authors": "Jack Lu, Ryan Teehan, Jinran Jin, Mengye Ren",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02304v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.",
    "title_zh": "验证何时见效？深入探讨大语言模型作为解决方案验证者的应用",
    "abstract_zh": "大型语言模型（LLMs）既可以作为问题求解者，也可以作为解决方案的验证者。验证者通过从候选答案中筛选高质量结果，能够提升求解器的性能。然而，以往关于求解器与验证者之间交互的研究较为有限，主要集中在自验证（self-verification）上，很少探讨验证者如何评估来自同一模型家族或不同模型家族的输出。此外，现代大模型在训练后还经历了大量的后训练过程，但其对验证效果的影响尚不明确。本文对跨越37个模型、涵盖多个模型家族、规模以及基础模型与后训练版本的系统性研究进行了全面分析，这些模型在9个基准测试上进行了评估，覆盖逻辑推理、结构化谜题、符号计算、数学、常识判断、事实记忆和领域知识等多个任务类型。我们比较了自验证、同家族验证以及跨家族验证的效果。为支持这一研究，我们提出了“验证者增益”（verifier gain）这一新指标，并通过实证验证了其预测基于测试时验证者拒绝采样带来的性能提升能力。我们进一步分析了验证者增益、误报率等指标随模型规模和后训练程度的变化规律，并刻画了不同数据集的可验证性特征。研究发现：跨家族验证尤其有效；后训练虽削弱了自验证的改进效果，却显著增强了跨家族验证的提升能力；而数学与逻辑类任务具有最高的内在可验证性。"
  },
  {
    "date": "2025-12-02",
    "title": "Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits",
    "authors": "Pedro Orvalho, Marta Kwiatkowska, Mikoláš Janota, Vasco Manquinho",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02898v1",
    "source": "arXiv",
    "abstract": "Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults. This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).",
    "title_zh": "基于模型的诊断与多观测：C软件和布尔电路的统一方法",
    "abstract_zh": "调试是软件开发和电路设计中最为耗时且成本高昂的任务之一。尽管已提出多种基于公式的故障定位（FBFL）方法，但它们无法保证在所有失败测试用例中都获得一组完整的诊断结果，或可能产生非最小子集的冗余诊断，尤其是在存在多个故障的程序/电路中表现尤为明显。本文提出了一种名为CFaults的新颖故障定位工具，适用于具有多个故障的C语言软件和布尔电路。CFaults采用基于模型的诊断（MBD）技术，结合多组观测数据，并将所有失败的测试用例整合为一个统一的极大可满足性（MaxSAT）公式。因此，该方法确保了不同观测之间的一致性，同时简化了故障定位过程。在三个基准测试集上的实验结果表明：对于两个C程序基准（TCAS和C-Pack-IPAs），CFaults在定位C语言软件中的故障方面比其他FBFL方法（如BugAssist、SNIPER和HSD）更快；在布尔电路基准ISCAS85上，CFaults的运行速度通常慢于HSD，但仅在6%的电路中未能完成故障定位，显示出其在该领域仍具有较强的竞争力。此外，CFaults仅生成最小子集意义下的故障语句诊断，而其他方法往往会产生大量冗余诊断（例如BugAssist和SNIPER）。"
  },
  {
    "date": "2025-12-02",
    "title": "S3C2 SICP Summit 2025-06: Vulnerability Response Summit",
    "authors": "Anna Lena Rotthaler, Simon Oberthür, Juraj Somorovsky, Kirsten Thommes, Simon Trang, Yasemin Acar, Michel Cukier, William Enck, Alexandros Kapravelos, Christian Kästner, Dominik Wermke, Laurie Williams",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02600v1",
    "source": "arXiv",
    "abstract": "Recent years have shown increased cyber attacks targeting less secure elements in the software supply chain and causing significant damage to businesses and organizations. The US and EU governments and industry are equally interested in enhancing software security, including supply chain and vulnerability response. On June 26, 2025, researchers from the NSF-supported Secure Software Supply Chain Center (S3C2) and the Software Innovation Campus Paderborn (SICP) conducted a Vulnerability Response Summit with a diverse set of 9 practitioners from 9 companies. The goal of the Summit is to enable sharing between industry practitioners having practical experiences and challenges with software supply chain security, including vulnerability response, and helping to form new collaborations. We conducted five panel discussions based on open-ended questions regarding experiences with vulnerability reports, tools used for vulnerability discovery and management, organizational structures to report vulnerability response and management, preparedness and implementations for Cyber Resilience Act1 (CRA) and NIS22, and bug bounties. The open discussions enabled mutual sharing and shed light on common challenges that industry practitioners with practical experience face when securing their software supply chain, including vulnerability response. In this paper, we provide a summary of the Summit. Full panel questions can be found in the appendix.",
    "title_zh": "S3C2 SICP 2025年6月峰会：漏洞响应峰会",
    "abstract_zh": "近年来，针对软件供应链中安全性较弱环节的网络攻击日益增多，给企业和组织造成了重大损失。美国和欧盟政府以及产业界均高度重视提升软件安全，包括供应链安全和漏洞响应能力。2025年6月26日，由美国国家科学基金会（NSF）支持的可信软件供应链中心（S3C2）与帕德博恩软件创新园区（SICP）的研究人员共同举办了一场“漏洞响应峰会”，邀请来自9家企业的9位行业实践者参与。此次峰会旨在促进具有实际经验的行业从业者在软件供应链安全（包括漏洞响应）方面进行交流，推动建立新的合作机制。\n\n峰会围绕五个专题小组讨论展开，议题基于开放式问题，涵盖：漏洞报告的实际经验、漏洞发现与管理所使用的工具、漏洞响应与管理的组织架构、对《网络安全韧性法案》（Cyber Resilience Act, CRA）和《NIS2指令》（NIS2）的准备与实施情况，以及漏洞赏金计划。开放式的对话促进了各方之间的相互学习，揭示了行业实践者在保障软件供应链安全、特别是漏洞响应方面所面临的共性挑战。\n\n本文对本次峰会进行了总结。完整的小组讨论问题详见附录。"
  },
  {
    "date": "2025-12-02",
    "title": "Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic",
    "authors": "Muyu Pan, Dheeraj Kodakandla, Mahfuza Farooque",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02987v1",
    "source": "arXiv",
    "abstract": "Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.",
    "title_zh": "用于逻辑翻译的微调大型语言模型：通过Lang2Logic减少幻觉",
    "abstract_zh": "近年来，自然语言处理（NLP）领域，尤其是大型语言模型（LLMs）的进展，推动了无需人工干预即可将自然语言陈述自动翻译为形式逻辑的研究。这一技术使得自动化推理成为可能，有助于软件系统中的调试、循环不变量的发现以及对规范的遵循。然而，LLMs产生的“幻觉”——即不准确的输出——在需要高度精确性的逻辑翻译任务中仍是一个重大挑战。本文提出了一种新颖的框架：输入英文句子，将其转换为逻辑表达式，并进一步转化为合取范式（CNF），以供可满足性求解使用。该框架结合了经典NLP技术、自定义语法、符号计算库以及经过微调的语言模型，有效降低了幻觉现象的发生。在初步实验中，我们发现，经过不同语法设置训练的微调模型能够有意识地纠正原始模型所犯的同类错误，从而实现了可靠且一致的CNF生成。"
  },
  {
    "date": "2025-12-02",
    "title": "Probabilistic energy profiler for statically typed JVM-based programming languages",
    "authors": "Joel Nyholm, Wojciech Mostowski, Christoph Reichenbach",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02738v1",
    "source": "arXiv",
    "abstract": "Energy consumption is a growing concern in several fields, from mobile devices to large data centers. Developers need detailed data on the energy consumption of their software to mitigate consumption issues. Previous approaches have a broader focus, such as on specific functions or programs, rather than source code statements. They primarily focus on estimating the CPU's energy consumption using point estimates, thereby disregarding other hardware effects and limiting their use for statistical reasoning and explainability. We developed a novel methodology to address the limitations of measuring only the CPU's consumption and using point estimates, focusing on predicting the energy usage of statically typed JVM-based programming languages, such as Java and Scala. We measure the energy consumption of Bytecode patterns, the translation from the programming language's source code statement to their Java Bytecode representation. With the energy measurements, we construct a statistical model using Bayesian statistics, which allows us to predict the energy consumption through statistical distributions and analyze individual factors. The model includes three factors we obtain statically from the code: data size, data type, operation, and one factor about the hardware platform the code executes on: device. To validate our methodology, we implemented it for Java and evaluated its energy predictions on unseen programs. We observe that all four factors are influential, notably that two devices of the same model may differ in energy consumption and that the operations and data types cause consumption differences. The experiments also show that the energy prediction of programs closely follows the program's real energy consumption, validating our approach. Our work presents a methodology for constructing an energy model that future work, such as verification tools, can use for their energy estimates.",
    "title_zh": "面向静态类型JVM编程语言的概率能效分析器",
    "abstract_zh": "能源消耗在多个领域正成为一个日益突出的问题，从移动设备到大型数据中心皆是如此。开发者需要对其软件的能耗进行详细分析，以应对能耗问题。以往的方法多聚焦于特定函数或程序，而非源代码语句本身；且主要依赖点估计来估算CPU的能耗，忽略了其他硬件组件的影响，限制了其在统计推断和可解释性方面的应用。为此，我们提出了一种新方法，旨在克服仅测量CPU能耗以及使用点估计所带来的局限性，专注于预测静态类型JVM语言（如Java和Scala）的能耗。我们通过测量字节码模式的能耗——即编程语言源代码语句转换为Java字节码表示时的能耗——来实现这一目标。基于这些能耗数据，我们采用贝叶斯统计构建了一个统计模型，能够通过概率分布预测能耗，并对各个影响因素进行分析。该模型包含四个因素：三个来自代码的静态信息——数据大小、数据类型和操作类型，以及一个与硬件平台相关的因素——设备类型。为验证该方法的有效性，我们针对Java实现了该模型，并在未见过的程序上评估了其能耗预测能力。实验结果表明，所有四个因素均具有显著影响，尤其值得注意的是，同一型号的两个设备在能耗上可能存在差异，而操作类型和数据类型也会导致明显的能耗变化。此外，实验还显示，该模型对程序能耗的预测值与实际能耗高度吻合，验证了本方法的可靠性。我们的工作为未来研究（如验证工具）提供了一种构建能耗模型的方法论基础。"
  },
  {
    "date": "2025-12-02",
    "title": "In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs",
    "authors": "Vishnu Sarukkai, Asanshay Gupta, James Hong, Michaël Gharbi, Kayvon Fatahalian",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02543v1",
    "source": "arXiv",
    "abstract": "The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\\textbf{2.5$\\times$ lower cost}$, reducing per-episode costs from \\$0.059 to \\$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \\$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.",
    "title_zh": "上下文蒸馏与自洽级联：一种简单、无需训练的降低大语言模型代理成本的方法",
    "abstract_zh": "当前，人们对于如何利用新型大语言模型（LLM）代理已有大量构想，开发者也迫切希望快速原型化并测试新的代理设计。然而，使用高容量LLM在大规模上执行代理会产生高昂的推理成本。我们提出了一种简单有效的方法，在不引入微调（如漫长的训练周期、反复调整超参数）或手动提示工程（耗时费力的试错过程）所带来的开发摩擦成本的前提下，显著降低LLM代理的推理开销。\n\n最重要的是，我们提出了**上下文内蒸馏**（in-context distillation）这一新概念，将知识蒸馏的思想——即用低成本的学生模型模仿高成本的教师模型——应用于上下文学习场景中。我们的方法在每个代理步骤中检索相关的教师示范，并将其作为上下文示例提供给学生模型，使学生能够实时模仿教师的行为。同时，我们将该方法与已有的**自一致性级联**（self-consistency cascades）思想相结合，以动态判断何时应信任学生模型的输出。\n\n这种自适应策略既实现了模型专业化带来的成本优势，又保留了使用冻结模型的高效开发体验。在多步具身推理基准ALFWorld上，我们的方法在达到教师模型水平的准确率的同时，推理成本降低了**2.5倍**，单个任务的平均成本从0.059美元降至0.024美元。仅需843个episode，前期示范成本即可摊销完毕，在部署规模达100万episode时，累计节省超过34,900美元。\n\n在AppWorld这一需要复杂多步API工作流的代理基准测试中，我们在保持相同准确率的前提下，实现了**2倍的成本降低**，成功推动了性能-成本权衡曲线的前移。\n\n通过在维持快速实验迭代能力的同时大幅降低运营成本，我们的方法使先进的代理系统在更广泛的应用场景中具备了经济可行性。"
  },
  {
    "date": "2025-12-02",
    "title": "AtomGraph: Tackling Atomicity Violation in Smart Contracts using Multimodal GCNs",
    "authors": "Xiaoqi Li, Zongwei Li, Wenkai Li, Zeng Zhang, Lei Xie",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02399v1",
    "source": "arXiv",
    "abstract": "Smart contracts are a core component of blockchain technology and are widely deployed across various scenarios. However, atomicity violations have become a potential security risk. Existing analysis tools often lack the precision required to detect these issues effectively. To address this challenge, we introduce AtomGraph, an automated framework designed for detecting atomicity violations. This framework leverages Graph Convolutional Networks (GCN) to identify atomicity violations through multimodal feature learning and fusion. Specifically, driven by a collaborative learning mechanism, the model simultaneously learns from two heterogeneous modalities: extracting structural topological features from the contract's Control Flow Graph (CFG) and uncovering deep semantics from its opcode sequence. We designed an adaptive weighted fusion mechanism to dynamically adjust the weights of features from each modality to achieve optimal feature fusion. Finally, GCN detects graph-level atomicity violation on the contract. Comprehensive experimental evaluations demonstrate that AtomGraph achieves 96.88% accuracy and 96.97% F1 score, outperforming existing tools. Furthermore, compared to the concatenation fusion model, AtomGraph improves the F1 score by 6.4%, proving its potential in smart contract security detection.",
    "title_zh": "AtomGraph：基于多模态图卷积网络解决智能合约中的原子性违规问题",
    "abstract_zh": "智能合约是区块链技术的核心组成部分，广泛应用于各种场景中。然而，原子性违规问题已成为潜在的安全风险。现有的分析工具往往缺乏足够的精度来有效检测此类问题。为应对这一挑战，我们提出了AtomGraph——一种用于检测原子性违规的自动化框架。该框架利用图卷积网络（GCN）通过多模态特征学习与融合来识别原子性违规。具体而言，在协同学习机制的驱动下，模型同时从两种异构模态中进行学习：一是从合约的控制流图（CFG）中提取结构拓扑特征，二是从操作码序列中挖掘深层语义信息。我们设计了一种自适应加权融合机制，动态调整各模态特征的权重，以实现最优的特征融合。最终，GCN在合约的图级别上检测原子性违规。全面的实验评估表明，AtomGraph实现了96.88%的准确率和96.97%的F1分数，优于现有工具。此外，相较于传统的拼接融合模型，AtomGraph将F1分数提升了6.4%，充分证明了其在智能合约安全检测方面的巨大潜力。"
  },
  {
    "date": "2025-12-02",
    "title": "Pushing Tensor Accelerators Beyond MatMul in a User-Schedulable Language",
    "authors": "Yihong Zhang, Derek Gerstmann, Andrew Adams, Maaz Bin Safeer Ahmad",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02371v1",
    "source": "arXiv",
    "abstract": "Tensor accelerators now represent a growing share of compute resources in modern CPUs and GPUs. However, they are hard to program, leading developers to use vendor-provided kernel libraries that support tensor accelerators. As a result, the usage of tensor accelerators is limited to the provided interface, mainly designed for traditional ML and scientific computing workloads. In this paper, we show that tensor accelerators can improve the performance of applications beyond simple variants of MatMul. For example, many image processing pipelines are linear transformations over matrices in disguise and can therefore utilize such specialized hardware. This is nonetheless hindered by the difficulties in programming tensor accelerators. We tackle this problem with compiler-based techniques. We use the Halide user-schedulable language and express operations as Halide algorithms succinctly. To this end, we implement a flexible tensor instruction selector based on equality saturation. The tensor instruction selector supports both CPU- and GPU-attached tensor accelerators and works with existing scheduling operations (e.g., producer-consumer fusion). Together, this enables developers to write diverse accelerator-leveraging applications in a few dozen lines. Using our system, we demonstrate the potential of tensor accelerators beyond their traditional domains. We implement several image processing pipelines (e.g., filtering, resampling, and denoising) in our system and evaluate them against non-accelerator-leveraging baselines. We show that these pipelines can achieve significant speedups. For example, a downsampling routine is sped up by $6.1\\times$ by utilizing Tensor Cores on an Nvidia RTX 4070 GPU.",
    "title_zh": "在用户可调度语言中将张量加速器的应用超越矩阵乘法",
    "abstract_zh": "张量加速器如今在现代CPU和GPU中占据了日益增长的计算资源份额。然而，它们难以编程，导致开发者不得不依赖厂商提供的内核库来支持张量加速器。结果，张量加速器的使用被限制在厂商提供的接口范围内，这些接口主要针对传统的机器学习和科学计算工作负载。本文表明，张量加速器能够提升远超简单矩阵乘法（MatMul）变体的应用性能。例如，许多图像处理流水线本质上是矩阵上的线性变换，因此可以利用这类专用硬件。然而，这种潜力受到编程张量加速器困难的制约。为此，我们采用基于编译器的技术来解决该问题。我们使用Halide这一用户可调度的语言，以简洁的方式表达算法操作。为此，我们实现了一个基于等式饱和（equality saturation）的灵活张量指令选择器。该指令选择器同时支持附着于CPU和GPU的张量加速器，并能与现有的调度操作（如生产者-消费者融合）协同工作。由此，开发者仅需几十行代码即可编写出多种利用加速器的应用程序。通过我们的系统，我们展示了张量加速器在传统应用领域之外的巨大潜力。我们在系统中实现了多个图像处理流水线（如滤波、重采样和去噪），并与不使用加速器的基线方法进行了对比评估。结果表明，这些流水线能够获得显著的速度提升。例如，利用Nvidia RTX 4070 GPU上的张量核心，一个下采样例程的执行速度提升了6.1倍。"
  },
  {
    "date": "2025-12-02",
    "title": "Towards autonomous normative multi-agent systems for Human-AI software engineering teams",
    "authors": "Hoa Khanh Dam, Geeta Mahala, Rashina Hoda, Xi Zheng, Cristina Conati",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02329v1",
    "source": "arXiv",
    "abstract": "This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.",
    "title_zh": "面向人机协同软件工程团队的自主规范多智能体系统",
    "abstract_zh": "本文构想了一种软件工程领域的范式变革：人工智能以完全自主的智能体形式，成为核心软件开发活动的主要驱动力。我们提出一类新型的软件工程智能体，这些智能体基于大型语言模型，具备信念、欲望、意图和记忆能力，从而实现类人的推理。这些智能体与人类及其他智能体协同工作，以远超当前软件开发流程的速度、可靠性和适应性，完成软件系统的设计、实现、测试与部署。它们的协调与协作由以道义模态（即承诺、义务、禁止与许可）表达的行为规范所引导，用以规范交互并确保合规性。这些创新共同构建了一个可扩展、透明且值得信赖的未来人机协同软件工程框架。"
  },
  {
    "date": "2025-12-02",
    "title": "Lumos: Let there be Language Model System Certification",
    "authors": "Isha Chaudhary, Vedaant Jain, Avaljot Singh, Kavya Sachdeva, Sayan Ranu, Gagandeep Singh",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02966v1",
    "source": "arXiv",
    "abstract": "We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.",
    "title_zh": "Lumos：让语言模型系统认证成为现实",
    "abstract_zh": "我们提出了首个原则性框架Lumos，用于指定并形式化验证语言模型系统（LMS）的行为。Lumos是一种基于图的命令式概率编程领域特定语言（DSL），其语法结构可生成独立同分布的提示（prompts）以供LMS使用。它通过图结构对提示分布进行系统化建模，从采样的子图中构造出随机提示。Lumos通过与统计验证工具集成，支持对任意提示分布下的LMS进行形式化验证。我们为Lumos提供了混合语义（操作语义与指称语义），从而提供了一种严谨的方式来解释其规范。仅需少量可组合的构造模块，Lumos即可编码现有的LMS规范，包括复杂的关联关系与时间序列规范。同时，它也支持新性质的定义——我们首次利用Lumos构建了自动驾驶场景下视觉-语言模型（VLMs）的安全性规范。基于这些规范，我们发现当前最先进的VLM模型Qwen-VL在雨天右转场景中，至少以90%的概率产生错误且不安全的响应，暴露出显著的安全隐患。Lumos的模块化设计使得规范易于修改，使LMS的验证能力能够紧跟快速演进的威胁环境。此外，我们进一步证明，用Lumos编写的规范程序能够有效定位出现于前沿LMS中的具体失效案例。Lumos是首个系统化且可扩展的语言级框架，用于指定和验证LMS行为，为LMS验证技术的广泛应用铺平了道路。"
  },
  {
    "date": "2025-12-02",
    "title": "Belobog: Move Language Fuzzing Framework For Real-World Smart Contracts",
    "authors": "Wanxu Xia, Ziqiao Kong, Zhengwei Li, Yi Lu, Pan Li, Liqun Yang, Yang Liu, Xiapu Luo, Shaohua Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02918v1",
    "source": "arXiv",
    "abstract": "Move is a research-oriented programming language design for secure and verifiable smart contract development and has been widely used in managing billions of digital assets in blockchains, such as Sui and Aptos. Move features a strong static type system and explicit resource semantics to enforce safety properties such as the prevention of data races, invalid asset transfers, and entry vulnerabilities. However, smart contracts written in Move may still contain certain vulnerabilities that are beyond the reach of its type system. It is thus essential to validate Move smart contracts. Unfortunately, due to its strong type system, existing smart contract fuzzers are ineffective in producing syntactically or semantically valid transactions to test Move smart contracts. This paper introduces the first fuzzing framework, Belobog, for Move smart contracts. Belobog is type-aware and ensures that all generated and mutated transactions are well-typed. More specifically, for a target Move smart contract, Belobog first constructs a type graph based on Move's type system, and then generates or mutates a transaction based on the graph trace derived from the type graph. In order to overcome the complex checks in Move smart contracts, we further design and implement a concolic executor in Belobog. We evaluated Belobog on 109 real-world Move smart contract projects. The experimental results show that Belobog is able to detect 100\\% critical and 79\\% major vulnerabilities manually audited by human experts. We further selected two recent notorious incidents in Move smart contracts, i.e., Cetus and Nemo. Belobog successfully reproduced full exploits for both of them, without any prior knowledge.",
    "title_zh": "贝洛博格：面向现实世界智能合约的移动语言模糊测试框架",
    "abstract_zh": "Move 是一种面向研究的编程语言设计，专为安全且可验证的智能合约开发而生，在区块链中被广泛用于管理数十亿数字资产，例如 Sui 和 Aptos。Move 具有强大的静态类型系统和显式的资源语义，能够强制实施诸如防止数据竞争、无效资产转移以及入口点漏洞等安全属性。然而，尽管如此，用 Move 编写的智能合约仍可能包含超出其类型系统检测能力的某些漏洞。因此，对 Move 智能合约进行验证至关重要。遗憾的是，由于其严格的类型系统，现有的智能合约模糊测试工具难以生成语法或语义上有效的交易来测试 Move 智能合约。\n\n本文提出了首个针对 Move 智能合约的模糊测试框架——Belobog。Belobog 具备类型感知能力，确保所有生成和变异的交易均为合法类型。具体而言，对于目标 Move 智能合约，Belobog 首先基于 Move 的类型系统构建类型图，然后根据从该类型图导出的图遍历路径生成或变异交易。为了克服 Move 智能合约中复杂的检查机制，我们进一步在 Belobog 中设计并实现了一个符号执行器（concolic executor）。我们在 109 个真实世界的 Move 智能合约项目上评估了 Belobog 的表现。实验结果表明，Belobog 能够检测到 100% 的关键漏洞以及 79% 的由人工专家手动审计出的重大漏洞。此外，我们还选取了近期两个著名的 Move 智能合约事件——Cetus 和 Nemo，Belobog 成功在无任何先验知识的情况下复现了完整的攻击过程。"
  },
  {
    "date": "2025-12-02",
    "title": "Synthetic Error Injection Fails to Elicit Self-Correction In Language Models",
    "authors": "David X. Wu, Shreyas Kapur, Anant Sahai, Stuart Russell",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.02389v1",
    "source": "arXiv",
    "abstract": "Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.",
    "title_zh": "合成错误注入无法激发语言模型的自我修正能力",
    "abstract_zh": "强化学习已成为激发大语言模型推理与自我修正能力的主导范式，但其高昂的计算成本促使人们探索替代方案。受自动驾驶和机器人技术的启发，我们研究了在语言模型中通过带合成错误注入的监督学习是否能够诱导出自我修正能力。我们的方法是在推理链中插入人为制造的错误，对这些错误进行掩码，并监督模型识别并纠正这些错误。尽管该方法具有直观吸引力，但我们发现，即使在多个模型上针对简单的合成任务，其性能提升也并不显著。此外，即便模型能够察觉自身错误，它也常常只是重复原始错误。我们发现，合成错误分布与策略内错误（on-policy errors）之间的分布偏移会显著削弱微调后模型的纠错能力，即使合成错误覆盖了良好的策略内错误分布也是如此。我们的研究结果有助于解释为何基于策略内强化学习的方法在激发自我修正能力方面表现尤为有效。"
  },
  {
    "date": "2025-12-2",
    "title": "Shield Broken: Black-Box Adversarial Attacks on LLM-Based Vulnerability Detectors",
    "authors": "Yuan Jiang, Shan Huang, Christoph Treude, Xiaohong Su, Tiantian Wang",
    "publish": "IEEE Transactions on Software Engineering",
    "url": "https://doi.org/10.1109/tse.2025.3638998",
    "source": "IEEE",
    "abstract": "Vulnerability detection is critical for ensuring software security. Although deep learning (DL) methods, particularly those employing large language models (LLMs), have shown strong performance in automating vulnerability identification, they remain susceptible to adversarial examples, which are carefully crafted inputs with subtle perturbations designed to evade detection. Existing adversarial attack methods often require access to model architectures or confidence scores, making them impractical for real-world black-box systems. In this paper, we propose SVulAttack, a novel label-only adversarial attack framework targeting LLM-based vulnerability detectors. Our key innovation lies in a similarity-based strategy that estimates statement importance and model confidence, thereby enabling more effective selection of semantic-preserving code perturbations. SVulAttack combines this strategy with a transformation component and a search component, based on either greedy or genetic algorithms, to effectively identify and apply optimal combinations of transformations. We evaluate SVulAttack on open-source models (LineVul, StagedVulBERT, Code Llama, Deepseek-Coder) and closed-source models (GPT-5 nano, GPT-4o, GPT-4o-mini, Claude Sonnet 4). Results show that SVulAttack significantly outperforms existing label-only black-box attack methods. For example, against LineVul, our method with genetic algorithm achieves an attack success rate of 49.0%, improving over DIP and CODA by 150.0% and 240.3%, respectively.",
    "title_zh": "盾牌破碎：基于大语言模型的漏洞检测器的黑盒对抗攻击",
    "abstract_zh": "漏洞检测对于保障软件安全至关重要。尽管深度学习（DL）方法，尤其是采用大语言模型（LLMs）的方法，在自动化漏洞识别方面表现出色，但它们仍容易受到对抗样本的攻击——这些样本是经过精心设计、带有细微扰动的输入，旨在逃避检测。现有的对抗攻击方法通常需要访问模型架构或置信度分数，这使得它们在实际的黑盒系统中难以应用。本文提出了一种名为SVulAttack的新颖标签仅限（label-only）对抗攻击框架，专门针对基于LLM的漏洞检测器。我们的核心创新在于一种基于相似性的策略，该策略能够估计代码语句的重要性及模型置信度，从而更有效地选择保持语义不变的代码扰动。SVulAttack将这一策略与一个变换组件和一个搜索组件相结合，搜索组件可基于贪婪算法或遗传算法，以有效识别并应用最优的变换组合。我们在开源模型（LineVul、StagedVulBERT、Code Llama、Deepseek-Coder）和闭源模型（GPT-5 nano、GPT-4o、GPT-4o-mini、Claude Sonnet 4）上对SVulAttack进行了评估。结果表明，SVulAttack显著优于现有的标签仅限黑盒攻击方法。例如，在针对LineVul的攻击中，使用遗传算法的本方法实现了49.0%的攻击成功率，相较于DIP和CODA分别提升了150.0%和240.3%。"
  },
  {
    "date": "2025-12-2",
    "title": "Enhancing SAP Predictive Maintenance and Industrial Analytics with LLM-Driven Automation",
    "authors": "Ashwini Chandrakumaran",
    "publish": "2025 IEEE Madhya Pradesh Section Conference (MPCON)",
    "url": "https://doi.org/10.1109/mpcon66082.2025.11256577",
    "source": "IEEE",
    "abstract": "The emergence of Industry 4.0 has revolutionized real-time process monitoring, predictive maintenance, and industrial analytics, with SAP playing a key role in digital transformation. However, implementing AI-driven analytics in SAP S/4HANA, SAP BTP, and SAP Predictive Maintenance & Service (PdMS) remains a challenge due to the complexity of machine learning tools and the need for programming expertise. To address this, I introduce SAP-LLM Analytica, an AI-powered framework that integrates Large Language Models (LLMs) to automate fault detection, process optimization, and predictive maintenance within SAP environments. By leveraging pre-trained AI models, iterative prompting strategies, and 60+ expert-designed analytics modules, SAP-LLM Analytica enhances decision-making in manufacturing, supply chain, and asset management. The system seamlessly integrates with SAP Digital Manufacturing Cloud (DMC) and SAP IoT to improve efficiency, reliability, and product quality through real-time anomaly detection and process recommendations. Empirical evaluations using real-world SAP industrial datasets demonstrate improved KPI performance, reduced downtime, and enhanced automation of analytics workflows. This research highlights how SAP-LLM Analytica can accelerate smart factory initiatives and digital twin development, transforming industrial analytics within the SAP ecosystem.",
    "title_zh": "利用大语言模型驱动的自动化提升SAP预测性维护与工业分析",
    "abstract_zh": "工业4.0的兴起彻底改变了实时过程监控、预测性维护和工业分析，而SAP在数字化转型中发挥了关键作用。然而，由于机器学习工具的复杂性以及对编程技能的需求，将人工智能驱动的分析应用于SAP S/4HANA、SAP BTP及SAP预测性维护与服务（PdMS）仍面临挑战。为应对这一难题，本文提出SAP-LLM Analytica——一种基于大语言模型（LLMs）的人工智能框架，可自动实现故障检测、流程优化和预测性维护，无缝集成于SAP生态系统中。该框架利用预训练AI模型、迭代提示策略以及60多个专家设计的分析模块，显著提升制造、供应链和资产管理领域的决策能力。系统与SAP数字制造云（DMC）和SAP IoT深度集成，通过实时异常检测与流程优化建议，有效提高效率、可靠性和产品质量。基于真实世界SAP工业数据集的实证评估表明，该系统在关键绩效指标（KPI）表现、停机时间减少以及分析工作流自动化方面均取得显著改善。本研究揭示了SAP-LLM Analytica在加速智能工厂建设与数字孪生开发方面的巨大潜力，正推动SAP生态体系内工业分析的全面革新。"
  },
  {
    "date": "2025-12-2",
    "title": "Power and Area Efficient Processing Element (PE) of CNN Accelerator for Object Detection",
    "authors": "Abinash Kumar Pala, Swastik Behera, Raghunandan Swain, Dinesh Kumar Dash, Sandipan Mallik",
    "publish": "2025 IEEE 6th India Council International Subsections Conference (INDISCON)",
    "url": "https://doi.org/10.1109/indiscon66021.2025.11251525",
    "source": "IEEE",
    "abstract": "The proposed Research presents the architecture of an ASIC-based Processing Element (PE) designed to support and increasingly advance the computational needs of Convolutional Neural Networks (CNNs) as they extend to real-time applications of image processing, object recognition, and autonomous systems. The PE is specifically developed for convolution operations and has a pipelined datapath composed of a multiplier; three-layer convolution engine; first-in-first-out (FIFO) buffer; and an output register. The control elements of the component are computed using D-Flip Flops and include overflow detection, set-bounds logic, a counter and a MUX to enable step-level control and dynamic limits on the computation limits. The novel features ensure synchronized access and perform data transformations through convolution, pooling, and activation functions efficiently. It is a modular and scalable implementation, and can be integrated into larger CNN accelerators capable of performing multiple convolution operations in parallel across multiple PE’s. Synthesis and simulation results substantiate that the architecture works effectively and shows significant gains in power, area, and performance when compared to conventional GPU-based systems.",
    "title_zh": "用于目标检测的卷积神经网络加速器中的高效能与高面积效率处理单元（PE）",
    "abstract_zh": "所提出的研究所设计的是一种基于ASIC的处理单元（PE）架构，旨在支持并持续推动卷积神经网络（CNN）在实时图像处理、目标识别及自主系统等应用中的计算需求。该PE专为卷积运算而设计，其流水线数据通路包含乘法器、三层卷积引擎、先进先出（FIFO）缓冲器以及输出寄存器。该组件的控制逻辑通过D-触发器实现，包含溢出检测、边界设定逻辑、计数器以及多路选择器（MUX），以实现逐级控制和动态计算范围限制。这些创新特性确保了访问的同步性，并高效完成卷积、池化及激活函数等数据变换操作。该架构具有模块化与可扩展性，可集成至更大规模的CNN加速器中，实现多个PE并行执行多组卷积运算。综合合成与仿真结果表明，该架构运行有效，在功耗、面积和性能方面相较于传统GPU系统均表现出显著优势。"
  },
  {
    "date": "2025-12-2",
    "title": "ASIC Flow of AXI Interface for ADPLL Module using SCL180nm",
    "authors": "M.L.N Charyulu, R. Mounika, Mohd Ziauddin Jahangir, Vivek Singh Kushwah",
    "publish": "2025 IEEE Madhya Pradesh Section Conference (MPCON)",
    "url": "https://doi.org/10.1109/mpcon66082.2025.11256534",
    "source": "IEEE",
    "abstract": "In today's technologically advanced world, microelectronics plays a crucial role in nearly every aspect of daily life. As a result, the demand for microelectronic components has significantly increased. This surge in demand can lead to longer production times and a higher likelihood of failures in final products. Therefore, it is essential to adopt methods that enhance both the efficiency and effectiveness of hardware design and verification processes. The Advanced Microcontroller Bus Architecture (AMBA) is an open standard for on-chip communication interfaces. It defines a set of rules that enable structured and efficient communication within a System on Chip (SoC). One of the prominent protocols within the AMBA family is the Advanced extensible Interface (AXI). AXI is designed to facilitate high-speed data transfers between IP cores using a master-slave configuration. Known for supporting high-frequency and high-performance system architectures, AXI is well-suited for designs that require low latency and high bandwidth. Additionally, AXI maintains compatibility with other AMBA protocols such as APB (Advanced Peripheral Bus) and AHB (Advanced High-performance Bus). A key feature of the AXI protocol is its separation of address, data, and control phases, which contributes to its flexibility and efficiency.",
    "title_zh": "使用SCL180nm工艺的ADPLL模块中AXI接口的ASIC设计流程",
    "abstract_zh": "在当今技术高度发达的世界中，微电子技术几乎渗透到日常生活的方方面面，发挥着至关重要的作用。因此，对微电子元器件的需求显著增加。这种需求的激增可能导致生产周期延长，并提高最终产品出现故障的可能性。为此，采用能够提升硬件设计与验证过程效率和效果的方法显得尤为重要。\n\n高级微控制器总线架构（AMBA）是一种开放标准的片上通信接口，它定义了一套规则，使系统级芯片（SoC）内部的通信更加结构化和高效。AMBA家族中一个突出的协议是高级可扩展接口（AXI）。AXI旨在通过主从架构实现IP核之间的高速数据传输。由于其支持高频和高性能系统架构，AXI特别适用于对低延迟和高带宽有严格要求的设计。此外，AXI还与其它AMBA协议（如APB，即高级外设总线；AHB，即高级高性能总线）保持良好的兼容性。\n\nAXI协议的一个关键特性是将地址、数据和控制阶段分离，这一设计大大增强了其灵活性和效率。"
  },
  {
    "date": "2025-12-2",
    "title": "Leveraging LLMs for Detecting Security Threats in Virtual Network Functions",
    "authors": "Ibrahim Khalil Mejri, Sara Sutton",
    "publish": "2025 IEEE 12th International Conference on Cyber Security and Cloud Computing (CSCloud)",
    "url": "https://doi.org/10.1109/cscloud66326.2025.00031",
    "source": "IEEE",
    "abstract": "Securing dynamic virtual network environments poses a significant challenge for traditional intrusion detection systems. This paper explores the use of Large Language Models (LLMs) to more effectively identify security threats in these complex settings. We introduce two complementary LLM-based approaches. The first, a “deep inspection” method, performs a highly detailed analysis of network traffic, achieving up to 97% accuracy in detecting malicious activity. The second, an “aggregated analysis” workflow, is designed for high-throughput environments and maintains 86.2% accuracy by analyzing statistical trends in traffic data. Evaluated on the VNFCYBERDATA dataset, our fine-tuned models outperform traditional methods. A combined model that dynamically switches between the two LLM workflows achieves a <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{9 1. 2 \\%}$</tex> overall accuracy, surpassing the performance of a traditional DNN baseline while effectively managing real-time processing demands. These findings highlight the potential of LLMs to enhance security in virtualized networks, augmenting the capabilities of traditional systems.",
    "title_zh": "利用大语言模型检测虚拟网络功能中的安全威胁",
    "abstract_zh": "在动态虚拟网络环境中保障安全对传统的入侵检测系统构成了重大挑战。本文探讨了利用大型语言模型（LLMs）更有效地识别此类复杂环境中的安全威胁。我们提出了两种互补的基于LLM的方法。第一种是“深度检测”方法，对网络流量进行高度细致的分析，能够实现高达97%的恶意活动检测准确率。第二种是“聚合分析”工作流，专为高吞吐量环境设计，通过分析流量数据的统计趋势，保持86.2%的准确率。在VNFCYBERDATA数据集上的评估表明，我们微调后的模型优于传统方法。一种能够根据需求动态切换两种LLM工作流的组合模型，实现了91.2%的整体准确率，不仅超越了传统DNN基线的表现，还能有效应对实时处理的需求。这些发现突显了LLM在增强虚拟化网络安全方面的潜力，进一步提升了传统系统的防护能力。"
  },
  {
    "date": "2025-12-2",
    "title": "Code Search with RAG: A Systematic Comparison of Retrieval Performance",
    "authors": "Rama Satya, Raghu Kisore. N",
    "publish": "2025 International Conference on Emerging Techniques in Computational Intelligence (ICETCI)",
    "url": "https://doi.org/10.1109/icetci67340.2025.11257897",
    "source": "IEEE",
    "abstract": "Buffer overflow attacks remain a critical threat to software security, with existing defenses like ASLR and stack canaries proving insufficient against sophisticated exploits. Nvariant systems, which execute diverse program variants to thwart coordinated attacks, offer a promising solution. However, manually generating such variants is costly and unscalable. To address this, we propose an AI-driven pipeline that automates the creation of N-secure code variants by leveraging programming expertise embedded in existing code repositories. As a foundational step, this work benchmarks the ability of Retrieval-Augmented Generation (RAG) models to identify semantically similar code snippets from a repository that match the coding constructs of a given human-written code. We evaluate multiple RAG architectures on their retrieval accuracy, diversity of returned snippets, and ability to surface distinct coding expertise (e.g., security-aware implementations). Our analysis reveals how RAG models can systematically curate programming variants that preserve functionality while differing in structure, libraries, or security practices. The retrieved snippets will later train a modelfree reinforcement learning (RL) agent to generate N diverse variants, introducing security by diversity against buffer overflow attacks. By rigorously assessing RAG models now, we ensure the subsequent RL phase has high-quality, relevant exemplars that are critical for generating robust variants. We evaluate two different RAG models: Code specific RAG model and a Generalized RAG model. L2 distance and cosine similarity metrics are used to measure the ability of RAG model identify code segments from a given human code repository that capture diverse coding human expertise. We believe our work bridges AI-powered code retrieval and generative security, offering a scalable path to N-variant systems. Our proposed scalable path to N-variant systems involves feeding the output of RAG to a model free Reinforcement Learning algorithm.",
    "title_zh": "基于RAG的代码搜索：检索性能的系统性比较",
    "abstract_zh": "缓冲区溢出攻击仍然是软件安全领域的一大威胁，而现有的防御机制（如ASLR和栈金丝雀）在面对复杂攻击时仍显不足。Nvariant系统通过执行多个程序变体来抵御协同攻击，为解决这一问题提供了有前景的方案。然而，手动生成这些变体成本高昂且难以扩展。为此，我们提出一种基于人工智能的自动化流水线，利用现有代码仓库中嵌入的编程知识，自动创建N个安全的代码变体。\n\n作为基础步骤，本研究评估了检索增强生成（RAG）模型从代码库中识别与给定人工编写的代码在语义上相似的代码片段的能力，这些片段需匹配特定的编程结构。我们在多个RAG架构上进行了评估，考察其在检索准确性、返回代码片段多样性以及揭示不同编程专长（例如注重安全性的实现方式）方面的能力。分析结果表明，RAG模型能够系统性地筛选出在功能保持一致的前提下，在结构、所用库或安全实践上存在差异的编程变体。\n\n这些检索到的代码片段将用于训练一个无模型强化学习（RL）代理，以生成N个多样化的代码变体，从而通过“安全多样性”策略抵御缓冲区溢出攻击。通过严格评估当前RAG模型的表现，我们确保后续强化学习阶段所使用的示例质量高、相关性强，这对生成鲁棒的代码变体至关重要。\n\n我们对两种不同的RAG模型进行了评估：专用代码RAG模型和通用RAG模型。采用L2距离和余弦相似度等指标，衡量RAG模型从人类代码仓库中提取能体现多样化编程经验的代码片段的能力。\n\n我们认为，本工作打通了AI驱动的代码检索与生成式安全之间的桥梁，为构建可扩展的Nvariant系统开辟了新路径。所提出的可扩展Nvariant系统方案，是将RAG模型的输出输入至无模型强化学习算法中，实现高效、多样且安全的代码变体生成。"
  },
  {
    "date": "2025-12-2",
    "title": "An Open-Source Structural Coverage Tool for DO-178C Compliance",
    "authors": "Wentao Zhang, Andrew Oppelt, Ikjun Jeon, Minji Park, Steven H. VanderLeest, Chuck Wolber",
    "publish": "2025 AIAA DATC/IEEE 44th Digital Avionics Systems Conference (DASC)",
    "url": "https://doi.org/10.1109/dasc66011.2025.11257174",
    "source": "IEEE",
    "abstract": "This paper presents enhancements to the opensource LLVM-Cov structural coverage tool to meet DO-178C Software Level A objectives in the Linux kernel for assuring safety-critical aerospace software. We contributed new features to Linux and made improvements to LLVM-Cov for measuring unique-cause Modified Condition/Decision Coverage (MC/DC) of privileged, complex, optimized code (e.g., the Linux kernel). We innovated on the collection of trace data and improved several coverage mechanisms. With our efforts, LLVM-Cov achieves a robust mapping of coverage to the source code with the support of compiler optimizations, measuring line, branch, and MC/DC coverage. We present the first comprehensive coverage results (including MC/DC) for the Linux kernel. The MC/DC coverage using the current Linux test suites is low (7 %). By contributing our work back to the open-source community, we enable test suites to be enhanced to improve MC/DC coverage over time. Moreover, we describe DO-330 qualification. We certified our usage of LLVM-Cov to meet Criterion 3 of the tool impact criteria in DO-178C §12.2.2. at a Tool Qualification Level of 5. We discuss major milestones of our LLVM-Cov tool qualification, including a Tool Qualification Plan, Tool Operational Requirements, Tool Operational Verification and Validation Cases and Procedures, and Tool Accomplishment Summary.",
    "title_zh": "用于DO-178C合规的开源结构覆盖工具",
    "abstract_zh": "本文提出对开源LLVM-Cov结构化覆盖工具的改进，以满足航空航天安全关键软件在Linux内核中实现DO-178C软件等级A目标的要求。我们为Linux内核贡献了新功能，并对LLVM-Cov进行了优化，使其能够测量特权、复杂且高度优化代码（如Linux内核）的唯一原因修正条件/判定覆盖（MC/DC）。我们在追踪数据采集方法上进行了创新，并改进了多种覆盖度量机制。通过我们的工作，LLVM-Cov实现了在编译器优化支持下对源代码的稳健覆盖率映射，能够准确测量行覆盖、分支覆盖以及MC/DC覆盖。本文首次提供了Linux内核全面的覆盖度量结果（包括MC/DC）。当前Linux测试套件下的MC/DC覆盖率仅为7%。通过将我们的成果回馈开源社区，我们使测试套件得以持续改进，从而逐步提升MC/DC覆盖率。此外，本文还介绍了DO-330认证过程。我们已成功完成对LLVM-Cov的认证，确认其使用符合DO-178C第12.2.2节中准则3的工具影响标准，达到工具资格等级5。文中详细讨论了LLVM-Cov工具资格认证的主要里程碑，包括工具资格计划、工具运行需求、工具运行验证与确认案例及流程，以及工具完成情况总结。"
  },
  {
    "date": "2025-12-2",
    "title": "Enhancing Web Security Through Machine Learning-Based Feature Selection for Cross-Site Scripting (XSS) Attacks Classification",
    "authors": "Charchit Gharai, Sunil Kumar Mohapatra, Somanath Parida, Rajkumar Mohanta, Sujata Chakravarty, Dayal Sankar Ghosh",
    "publish": "2025 IEEE 6th India Council International Subsections Conference (INDISCON)",
    "url": "https://doi.org/10.1109/indiscon66021.2025.11251743",
    "source": "IEEE",
    "abstract": "Cross-Site Scripting (XSS) attacks fall under the broad classification of web security vulnerabilities. It enables attackers to inject harmful scripts into trusted sites that compromise sensitive user data and system integrity. The primary objective of our work is to classify cross-scripting attacks to enhance web security. This proposed approach categorizes the XSS attacks into three types of payloads: reflected, stored, and DOM-based. Each entry contains an encoded URL or script that exploits a weakness in web applications. A benchmark dataset is accessed from the IEEE Dataport repository to train, test, and performance analysis of the model. As per the earlier study, the machine learning model plays a crucial role in XSS attack detection and classification. However, these models lack in terms of proper feature selection techniques. So in this research, we have applied the widely used ML algorithms such as Random Forest, Decision tree, Support vector classifier, KNN, and Naive Bayes approach, along with the most preferred feature selection techniques to design the model. The model outcome shows that Random Forest’s testing accuracy is 0.9908 without feature selection. Accordingly, using feature selection techniques for Random Forest, the SelectKBest Chi<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> method achieved an accuracy of 0.8933, while SelectKBest Mutual Information reached 0.9167. Using the wrapper-based approach (RFE), the highest accuracy was 0.9789 with Random Forest, followed by KNN with 0.9433. Then the embedded method resulted in an accuracy of 0.9100. Though the Random Forest model without using feature selection holds the highest testing accuracy, it is suspected to be overfitting. Therefore, feature selection methods are utilized to minimize overfitting and increase the ability of the model to generalize. Therefore, in this research work, we have primarily focused on the feature selection techniques to mitigate this issue.",
    "title_zh": "通过基于机器学习的特征选择提升Web安全：用于跨站脚本（XSS）攻击分类",
    "abstract_zh": "跨站脚本（XSS）攻击属于广义的Web安全漏洞类别。此类攻击使攻击者能够向可信网站注入恶意脚本，从而威胁用户敏感数据的安全性及系统完整性。本文的主要目标是实现对跨脚本攻击的有效分类，以提升Web安全性。所提出的方案将XSS攻击分为三类载荷：反射型、存储型和基于DOM的攻击。每条记录包含一个经过编码的URL或脚本，利用Web应用程序中的安全缺陷进行攻击。研究采用来自IEEE Dataport资源库的基准数据集，用于模型的训练、测试及性能分析。\n\n根据先前的研究，机器学习模型在XSS攻击检测与分类中发挥着关键作用。然而，这些模型在特征选择技术方面仍存在不足。因此，在本研究中，我们采用了多种广泛应用的机器学习算法，包括随机森林（Random Forest）、决策树（Decision Tree）、支持向量机分类器（Support Vector Classifier）、K近邻（KNN）以及朴素贝叶斯（Naive Bayes），并结合当前最常用的特征选择方法来构建模型。\n\n实验结果表明，随机森林模型在未使用特征选择的情况下，测试准确率达到0.9908。而通过引入特征选择技术后，使用SelectKBest与卡方检验（Chi²）方法时，准确率为0.8933；采用SelectKBest互信息法时，准确率提升至0.9167。基于包装器方法（RFE）的评估中，随机森林达到最高准确率0.9789，KNN则为0.9433。嵌入式方法（Embedded Method）的最终准确率为0.9100。\n\n尽管不使用特征选择的随机森林模型在测试中表现出最高的准确率，但其可能存在过拟合现象。因此，本研究重点引入特征选择技术，旨在降低过拟合风险，增强模型的泛化能力。综上所述，本研究的核心工作聚焦于优化特征选择策略，以有效缓解过拟合问题，提升模型在实际应用中的鲁棒性与可靠性。"
  },
  {
    "date": "2025-12-2",
    "title": "Coverage-Driven Verification of Ethernet MAC Using System Verilog and UVM",
    "authors": "Beulah Grace Nelam, P. Sharmila, D.P.S. Praveenya, N. Vedajhna, N.V.S. Nagalakshmi, G. Gayathri, M. Durga Prakash",
    "publish": "2025 IEEE 6th India Council International Subsections Conference (INDISCON)",
    "url": "https://doi.org/10.1109/indiscon66021.2025.11253760",
    "source": "IEEE",
    "abstract": "In modern high-speed networking systems, the Ethernet MAC layer is essential for ensuring dependable and high-performance data transfer across network systems. This project presents the design and functional verification of an Ethernet MAC module, focusing on compliance with the IEEE 802. 3 standard. The verification environment was developed using SystemVerilog and Universal Verification Methodology (UVM), ensuring a modular, reusable, and scalable platform for constrained-random and coverage-driven verification. The environment incorporates key UVM components such as interface, transaction, driver, monitor, agent, scoreboard, and environment. A master-slave interface was created to facilitate MIIM (Management Interface) register access, supporting both read and write operations. To simulate physical layer communication, a PHY interface was implemented, and the MII (Media Independent Interface) was used to manage data exchange between the MAC and PHY layers. The testbench supported a range of test scenarios, including TX data path, RX data path, MIIM register write, MIIM register read, and PHY clock variation. Functional coverage metrics were captured to ensure verification completeness, and a scoreboard was employed for data comparison and result validation. This project delivers a robust and reusable verification environment tailored specifically for validating the Ethernet MAC design.",
    "title_zh": "基于System Verilog和UVM的以太网MAC覆盖率驱动验证",
    "abstract_zh": "在现代高速网络系统中，以太网MAC层对于确保网络系统间可靠且高性能的数据传输至关重要。本项目展示了符合IEEE 802.3标准的以太网MAC模块的设计与功能验证。验证环境采用SystemVerilog和通用验证方法学（UVM）构建，确保了平台的模块化、可重用性和可扩展性，支持约束随机测试与覆盖率驱动的验证。该环境集成了UVM的核心组件，包括接口（interface）、事务（transaction）、驱动器（driver）、监视器（monitor）、代理（agent）、评分卡（scoreboard）以及验证环境（environment）。为实现MIIM（管理接口）寄存器的访问，设计了主从接口，支持读写操作。为了模拟物理层通信，实现了PHY接口，并采用MII（媒体独立接口）来管理MAC与PHY之间的数据交换。测试平台支持多种测试场景，包括发送数据路径（TX）、接收数据路径（RX）、MIIM寄存器写入、MIIM寄存器读取以及PHY时钟变化等。通过捕获功能覆盖率指标，确保验证的完整性，并利用评分卡进行数据比对与结果验证。本项目成功构建了一个强大且可复用的验证环境，专门用于以太网MAC设计的功能验证。"
  },
  {
    "date": "2025-12-2",
    "title": "Assessing the capabilities of MBERT And Flan-T5 in detecting of multilingual suspicious messages",
    "authors": "Mohammed Akber Ali, M.S. Qaseem, Mohammed Mahmood Ali, Tamkanath Farooqui, Iram Fatima, Ayesha Mirza",
    "publish": "2025 International Conference on Emerging Techniques in Computational Intelligence (ICETCI)",
    "url": "https://doi.org/10.1109/icetci67340.2025.11258067",
    "source": "IEEE",
    "abstract": "These days, the increasing prevalence of internet threats such as phishing, spam, dangerous intent, fraud, and other related threats have posed many risks to individuals and organizations across the world. Previous research in this scope primarily focused on rule-based or keyword-driven approaches, which were mostly limited to single-language datasets and lacked the adaptability required for multilingual contexts. The need of detecting these messages multilingually arises from the growing number of digital dangers. This paper explores the use of LLM (Large Language Models), for identifying potentially suspicious content across multiple language contexts. MBERT model utilizes a transformer-based bidirectional attention mechanism to extract the meaning of words in multilingual messages and offers reliable detection of risks across diverse languages. Flan-T5 is fine-tuned using a broad set of task-specific instructions, improving its performance on unseen tasks. Moreover, approaches like zero-shot learning allow the system to get used to work on/adapt to different languages, on which it is not trained. Both our models show promising results with synthetic multilingual datasets. The overall accuracies are 82.3% and 83.33% for MBERT and Flan T5 respectively. The results obtained are helpful in dealing with messages involving cyber threats.",
    "title_zh": "评估MBERT与Flan-T5在检测多语言可疑消息方面的能力",
    "abstract_zh": "如今，网络威胁（如网络钓鱼、垃圾邮件、恶意意图、欺诈等）日益普遍，对全球个人和组织构成了诸多风险。以往在该领域的研究主要采用基于规则或关键词驱动的方法，这些方法大多局限于单语数据集，难以适应多语言环境的需求。随着数字威胁的不断增多，实现多语言环境下消息检测的需求愈发迫切。本文探讨了大型语言模型（LLM）在多语言背景下识别潜在可疑内容的应用。MBERT模型利用基于Transformer的双向注意力机制，能够有效提取多语言消息中词汇的语义信息，从而在多种语言中实现可靠的威胁检测。Flan-T5则通过大量特定任务指令进行微调，显著提升了其在未见过任务上的表现。此外，零样本学习等方法使系统能够在未受训的语言上快速适应和工作。在合成多语言数据集上的实验结果表明，两种模型均表现出良好的性能：MBERT的整体准确率为82.3%，Flan-T5为83.33%。这些成果有助于应对涉及网络威胁的消息处理问题。"
  },
  {
    "date": "2025-12-2",
    "title": "Contemporary Directions in Cybersecurity Avionics Risk Analysis",
    "authors": "Erik Blasch, Victor Murray, Mario Werthwein, Jeffrey S. Chavis, Jan Leuchter, Aloke Roy, James Lyke, Carlos C. Insaurralde, Giancarmine Fasano",
    "publish": "2025 AIAA DATC/IEEE 44th Digital Avionics Systems Conference (DASC)",
    "url": "https://doi.org/10.1109/dasc66011.2025.11257187",
    "source": "IEEE",
    "abstract": "With the increase in the congested airspace that incorporates signals, commands, and policies aggregated over multi-domain operations, cybersecurity risk is increasing. In this paper, the IEEE AESS Cyber Avionics Security Panel highlights contemporary methods discussed in aviation cybersecurity. As focused on the awareness and risk, the paper identifies directions for(1) next generation threats in aerospace cybersecurity, (2) artificial intelligence and machine learning in support or aerospace cyber defense including large language models, (3) applying zero trust and blockchains to avionics, (4) advanced edge computing implications to avionics cybersecurity, (5) cybersecurity in air and space traffic management (A-STM), including securing the internet of things (IoT) as a sensor in the airspace infrastructure, and (6) methods of cyber-security risk analysis that includes hardware, software, and data. Three focused advances include discussions on large language models (LLMs), zero-trust policies, and risk scoring.",
    "title_zh": "网络安保航空电子风险分析的当代发展方向",
    "abstract_zh": "随着涵盖信号、指令和跨域作战政策的空域日益拥挤，网络安全隐患正不断加剧。本文中，IEEE AESS航空网络安全小组重点探讨了当前航空网络安全领域的前沿方法。围绕意识与风险问题，论文指出了以下六个方向：（1）航空航天网络安全中的下一代威胁；（2）人工智能与机器学习在支持航空航天网络防御中的应用，包括大型语言模型；（3）零信任架构与区块链技术在航空电子系统中的应用；（4）先进边缘计算对航空网络安全的影响；（5）空中与空间交通管理（A-STM）中的网络安全问题，包括将物联网（IoT）作为空域基础设施传感器的安全保障；（6）涵盖硬件、软件和数据的网络安全风险分析方法。三项重点进展分别涉及大型语言模型（LLMs）、零信任策略以及风险评分机制的讨论。"
  },
  {
    "date": "2025-12-2",
    "title": "LLM-Driven Approach to Automated Sustainability of IoT Systems",
    "authors": "Nenad Petrović, Dragana Krstić, Suad Suljović, Dario Javor",
    "publish": "2025 IEEE 34th International Conference on Microelectronics (MIEL)",
    "url": "https://doi.org/10.1109/miel66332.2025.11261122",
    "source": "IEEE",
    "abstract": "Due to huge number and heterogeneity of devices within Internet of Things (IoT)-based architectures, scalability and maintainability of such systems rise many challenges when it comes to long-term sustainability, especially regarding scalability and maintainability aspects. In this paper, we explore state-of-art Large Language Model (LLM)-based techniques as well as Agentic AI approaches in order to automate these aspects and reduce the cognitive load and need for human intervention. As intermediary step for system representation, we leverage Model-Driven Engineering (MDE) approach in order to ensure verifiability. As outcome, two case studies relying on GPT-40 model are presented.",
    "title_zh": "基于大语言模型的物联网系统自动化可持续性方法",
    "abstract_zh": "由于基于物联网（IoT）架构中的设备数量庞大且类型多样，这些系统在长期可持续性方面面临着诸多挑战，尤其是在可扩展性和可维护性方面。本文探讨了前沿的大语言模型（LLM）技术以及代理型人工智能（Agentic AI）方法，旨在自动化这些方面，降低认知负担并减少对人工干预的需求。作为系统表示的中间步骤，我们采用模型驱动工程（MDE）方法以确保系统的可验证性。最终，本文提出了两个基于GPT-40模型的案例研究。"
  },
  {
    "date": "2025-12-2",
    "title": "Functional Coverage Analysis of APB RAM Verification Using System Verilog",
    "authors": "Beulah Grace Nelam, N.V.S. Nagalakshmi, G. Gayathri, N. Vedajhna, D.P.S. Praveenya, P. Sharmila, M. Durga Prakash",
    "publish": "2025 IEEE 6th India Council International Subsections Conference (INDISCON)",
    "url": "https://doi.org/10.1109/indiscon66021.2025.11251590",
    "source": "IEEE",
    "abstract": "This project primarily aims to design and validate the APB (Advanced Peripheral Bus) RAM using System Verilog by generating user defined testcases and transactions thereby obtaining cent percent functional coverage. The design is for providing simple, faster data rate, speedy read-write transactions, energy-efficient and budget-friendly approach for interfacing with low-speed peripherals like Real-Time Clock (RTC), Temperature Sensors, Watchdog Timers, Touchscreen Controllers, Light Sensors, Proximity Sensors, Keypad Interfaces, EEPROM by using APB protocol. This mechanism is intended to improve inter-chip communication and memory read/write efficiency. The proposed architecture is developed for communication between master and slave devices. Multiple read write transactions are implemented using multiple agents by making use of semaphore for selecting required agent based on semaphore keys. The memory simulation and verification is performed using the GVIM Editor and Questa Sim simulator. The test bench is further enhanced by implementing advanced features like concurrent task execution and comprehensive functional coverage analysis.",
    "title_zh": "基于SystemVerilog的APB RAM验证功能覆盖率分析",
    "abstract_zh": "本项目主要旨在通过生成用户自定义的测试用例和事务，利用System Verilog设计并验证APB（高级外设总线）RAM，从而实现百分之百的功能覆盖率。该设计旨在为与低速外设（如实时时钟RTC、温度传感器、看门狗定时器、触摸屏控制器、光传感器、接近传感器、键盘接口、EEPROM等）的接口提供一种简单、高速率、快速读写操作、节能且成本低廉的解决方案，采用APB协议实现。该机制旨在提升芯片间通信效率以及内存读写性能。所提出的架构专为实现主设备与从设备之间的通信而设计。通过使用多个代理（agent）并结合信号量（semaphore）机制，根据信号量键值选择所需代理，实现了多组读写事务的并行执行。内存的仿真与验证工作通过GVIM编辑器和Questa Sim仿真器完成。此外，测试平台还进一步集成了并发任务执行和全面的功能覆盖率分析等高级特性，以增强验证能力。"
  },
  {
    "date": "2025-12-2",
    "title": "LLM-WOA: Large Language Model Assisted Whale Optimization Algorithm",
    "authors": "Jiguang Yang, Jiuyuan Huo",
    "publish": "2025 10th International Conference on Computer and Information Processing Technology (ISCIPT)",
    "url": "https://doi.org/10.1109/iscipt67144.2025.11264957",
    "source": "IEEE",
    "abstract": "In view of the limitations of the traditional Whale Optimization Algorithm (WOA), such as fixed parameter adjustment rules, strategy switching dependent on random probability, and tendency to get trapped in local optima, this paper proposes a Whale Optimization Algorithm assisted by a Large Language Model (LLM), named LLM-WOA. The DeepSeek-V3.1 model is integrated into the core iterative process of WOA to achieve adaptive parameter adjustment and intelligent strategy adjustment. Specifically, LLM-WOA is designed with two core mechanisms: 1) The LLM-assisted parameter and strategy decision-making module, which enables LLM to dynamically output the contraction factor a, spiral coefficient b, and exploration/exploitation strategy probability based on the real-time optimization status through precise Prompt engineering; 2) The LLM-guided local optimal escape mechanism, which generates population perturbation strategies to restore search diversity when the algorithm stagnates. To validate the algorithm’s performance, a comparative experiment was conducted with the traditional WOA in the CEC2022 test set. The results show that LLM-WOA outperforms the traditional WOA in eleven test functions, indicating that the introduction of LLM can effectively solve the insufficient adaptive ability of the traditional WOA and provide a more efficient solution for complex optimization problems. The source code of this research is available for download at https://github.com/Drleach/LLMWOA.",
    "title_zh": "LLM-WOA：大语言模型辅助的鲸鱼优化算法",
    "abstract_zh": "针对传统鲸鱼优化算法（WOA）存在的参数调整规则固定、策略切换依赖随机概率以及易陷入局部最优等局限性，本文提出一种由大语言模型（LLM）辅助的鲸鱼优化算法，命名为LLM-WOA。将DeepSeek-V3.1模型嵌入WOA的核心迭代过程，实现参数的自适应调整与策略的智能化切换。具体而言，LLM-WOA设计了两个核心机制：1）基于LLM的参数与策略决策模块，通过精准的提示工程（Prompt engineering），使LLM能够根据实时优化状态动态输出收缩因子a、螺旋系数b以及探索/开发策略的概率；2）LLM引导的局部最优逃逸机制，在算法陷入停滞时生成种群扰动策略，以恢复搜索多样性。为验证算法性能，本文在CEC2022测试集上与传统WOA进行了对比实验。结果表明，LLM-WOA在11个测试函数中均优于传统WOA，证明引入LLM可有效解决传统WOA自适应能力不足的问题，为复杂优化问题提供了更高效的求解方案。本研究的源代码可在 https://github.com/Drleach/LLMWOA 下载获取。"
  },
  {
    "date": "2025-12-2",
    "title": "Quantum-Aware Large Language Models: Taxonomy, Architectures, and Future Applications",
    "authors": "Reyhan Dede",
    "publish": "2025 International Conference on Electrical, Communication and Computer Engineering (ICECCE)",
    "url": "https://doi.org/10.1109/icecce67514.2025.11257941",
    "source": "IEEE",
    "abstract": "The convergence of quantum computing and large language models (LLMs) represents a major milestone in computational intelligence, introducing novel paradigms for information processing, complexity modeling, and scientific advancement. This survey provides a comprehensive overview of the emerging interdisciplinary field that integrates quantum mechanics with natural language processing. We begin by presenting the foundational principles of quantum computing and LLMs, then examine their integration through automated quantum code generation, hybrid transformer architectures, and quantum natural language processing (QNLP) frameworks. State-of-the-art models, such as quantum-augmented attention mechanisms and circuit embedding strategies, are evaluated across domain-specific datasets and benchmarks. We also investigate the role of LLMs as quantum research assistants, educational tools, and simulation agents, particularly in resource-constrained environments. Furthermore, the survey explores challenges related to scalability, interpretability, and quantum data encoding, and outlines future research directions-especially the vision for fully quantum-native LLMs. Our goal is to guide researchers through the current landscape and to encourage continued exploration at the intersection of quantum technologies and advanced language models.",
    "title_zh": "量子感知大型语言模型：分类、架构与未来应用",
    "abstract_zh": "量子计算与大型语言模型（LLMs）的融合标志着计算智能领域的一个重要里程碑，为信息处理、复杂性建模以及科学进步引入了全新的范式。本综述全面概述了这一新兴的跨学科领域，即量子力学与自然语言处理的结合。我们首先介绍量子计算和大型语言模型的基础原理，随后探讨它们通过自动化量子代码生成、混合Transformer架构以及量子自然语言处理（QNLP）框架实现的集成。对当前最先进的模型——如量子增强型注意力机制和电路嵌入策略——在特定领域数据集和基准测试中的表现进行了评估。此外，我们还研究了大型语言模型作为量子研究助手、教育工具和模拟代理的作用，尤其是在资源受限环境中的应用。本综述还深入探讨了可扩展性、可解释性以及量子数据编码等方面的挑战，并展望了未来的研究方向，特别是对完全原生量子大型语言模型的愿景。我们的目标是引导研究人员了解当前的发展格局，并鼓励持续探索量子技术与先进语言模型交汇处的前沿领域。"
  },
  {
    "date": "2025-12-2",
    "title": "Leveraging Large Language Models and Database Integration for Compensating Micro-Anomaly in Automated SoC Verification",
    "authors": "Harsh Pandey, Gaurav Lodhi, Rajan Kumar Jha, Rajiv Kapoor",
    "publish": "2025 IEEE 6th India Council International Subsections Conference (INDISCON)",
    "url": "https://doi.org/10.1109/indiscon66021.2025.11253942",
    "source": "IEEE",
    "abstract": "When System-on-Chip (SoC) projects move from concept to implementation, design intent is usually written in plain language by senior architects and then interpreted by several downstream teams. In practice, that hand-off is errorprone: small omissions or wording shifts “micro-anomalies” propagate into RTL code, where they become costly late-stage bugs. We present a verification pipeline that tackles the problem at its linguistic source. A first Large Language Model (LLM) layer converts free-form requirements into concrete artifacts such as module lists, interface definitions, and schematic fragments. A second LLM layer then validates every artifact against a knowledge base that combines relational tables, semantic embeddings, and historical metrics. Detected inconsistencies trigger an automatic feedback loop that refines the artifacts until they align with the stored golden specifications. Experiments across ten representative SoC subsystems show that the framework identifies $95 \\%$ of requirement deviations versus $85 \\%$ for a simulation only baseline and shortens verification turnaround by $33 \\%$. In a case study on a 12-bit $250 \\mathrm{MS} / \\mathrm{s}$ ADC, the system uncovered twelve timing violations five missed by conventional flows while halving manual review effort. Engineers reported clearer traceability and faster convergence when requirements changed mid-project.These results indicate that language-aware, database backed reasoning can shift verification from a postdesign activity to a continuous, intent-preserving process, improving both reliability and schedule predictability for complex SoC developments.",
    "title_zh": "利用大语言模型与数据库集成补偿自动化SoC验证中的微小异常",
    "abstract_zh": "当系统级芯片（SoC）项目从概念阶段进入实现阶段时，设计意图通常由资深架构师以自然语言形式撰写，随后由多个下游团队进行解读。在实际操作中，这种交接过程极易出错：微小的遗漏或措辞变化——即“微观异常”——会传播至RTL代码中，最终演变为代价高昂的后期缺陷。我们提出了一种验证流程，从语言源头解决这一问题。第一层采用大型语言模型（LLM），将自由格式的需求转化为具体的可执行产物，如模块列表、接口定义和原理图片段；第二层LLM则利用融合了关系型表格、语义嵌入及历史度量数据的知识库，对每个产物进行校验。一旦发现不一致，系统将触发自动反馈循环，持续优化产物，直至其与存储的“黄金规格”完全对齐。在十个具有代表性的SoC子系统上的实验表明，该框架能够识别出95%的需求偏差，优于仅依赖仿真的基线方法（85%），同时将验证周期缩短了33%。在一项针对12位250 MS/s模数转换器（ADC）的案例研究中，该系统发现了十二处传统流程遗漏的时序违规，同时将人工审查工作量减半。工程师反馈称，在项目中期需求变更时，系统显著提升了需求可追溯性，并加快了收敛速度。这些结果表明，基于语言感知、数据库驱动的推理机制，能够将验证从传统的后期活动转变为持续进行、保持设计意图一致性的过程，从而显著提升复杂SoC开发的可靠性与进度可预测性。"
  },
  {
    "date": "2025-12-2",
    "title": "Hybridized PSO-ACO Algorithm for Optimized VLSI Placement: Focusing on Area and Wirelength",
    "authors": "Beena Snehal Uphale, G. R. Vijayshankar, Guntaj J, Bhupendra Ku. Gupta, Deepak Minhas, Premkumar R",
    "publish": "2025 IEEE Madhya Pradesh Section Conference (MPCON)",
    "url": "https://doi.org/10.1109/mpcon66082.2025.11256542",
    "source": "IEEE",
    "abstract": "This work presents a PSACO algorithm optimized approach of chip placement where the impartial is to diminish area and wirelength. A sub-step of physical design, placement assigns cells to fixed locations in a netlist while fulfilling timing and connectivity constraints. PSO is adaptive in global search while ACO is efficient in local search; the use of both addresses’ layout design as a multi-objective problem. In comparison with MCNC and GSRC circuits, the proposed method achieves area utilization improvement of 5.41% and wirelength improvement of 32.73% over standalone PSO for MCNC benchmarks. Smaller areas were squeezed more tightly within the chip leading to a slight degradation in area utilization by 2.94%, but significantly improving the wirelength by 2.06% as compared with their corresponding GSRC benchmarks. These results confirm the competitiveness of PSACO in solving NP-hard problems in VLSI design, providing a more harmonious trade-off between conflicting objectives.",
    "title_zh": "混合粒子群-蚁群算法在VLSI布局优化中的应用：聚焦于面积与线长",
    "abstract_zh": "本文提出了一种基于PSACO算法优化的芯片布局方法，其目标是减小芯片面积和布线长度。作为物理设计的一个子步骤，布局（placement）需将电路网表中的单元分配到固定位置，同时满足时序和连接性约束。粒子群优化（PSO）在全局搜索中具有自适应性，而蚁群优化（ACO）在局部搜索中效率较高，结合两者可将布局设计视为一个多目标优化问题。与MCNC和GSRC基准电路相比，所提方法在MCNC基准上相较于独立使用PSO，实现了5.41%的面积利用率提升和32.73%的布线长度改善。对于GSRC基准，虽然芯片面积被更紧密地压缩，导致面积利用率略有下降（2.94%），但布线长度显著改善了2.06%。这些结果验证了PSACO在解决VLSI设计中NP难问题方面的竞争力，能够在相互冲突的目标之间提供更为协调的权衡方案。"
  },
  {
    "date": "2025-12-2",
    "title": "Signal Integrity Analysis of DDR4 Design in Avionics Systems: A Comparative Study of Simulation-Measurement Correlation Between EDA Tools",
    "authors": "Soazig LE Bihan, Tristan Dubois, Benjamin Ducombs, Marc Gatti, Jean-Baptiste Begueret, Adil EL Abbazi",
    "publish": "2025 AIAA DATC/IEEE 44th Digital Avionics Systems Conference (DASC)",
    "url": "https://doi.org/10.1109/dasc66011.2025.11257145",
    "source": "IEEE",
    "abstract": "Signal integrity (SI) analysis becomes necessary as data rates, frequencies, density, and Printed Circuit Board (PCB) complexity grow. Double Data Rate (DDR) 4th generation memory is widely used in avionics systems, and each generation brings new design challenges. Recent developments in Electronic Design Automation (EDA) tools from the Ansys, Siemens, and Keysight suites enable the simulation and optimization of DDR signals to ensure compliance with Joint Electron Device Engineering Council (JEDEC) standards. Many vendors have developed DDR wizards that automate signal integrity analysis to reduce simulation setup time. On the other hand, simulation tools have their limitations, and the reliability of their results has yet to be validated on a real avionic board design. Research in this study aims to compare the DDR wizards from various simulation tools available on the market and correlate the results with measurements. This will help build confidence in the selected tools, developing a simulation methodology that allows design times to be reduced by relying solely on simulations, without the need for further measurements or test vehicles for future avionic designs.",
    "title_zh": "航空电子系统中DDR4设计的信号完整性分析：EDA工具间仿真-测量相关性对比研究",
    "abstract_zh": "随着数据速率、频率、集成度以及印刷电路板（PCB）复杂性的不断提高，信号完整性（SI）分析变得愈发重要。第四代双倍数据速率（DDR4）内存广泛应用于航空电子系统中，而每一代新产品的推出都带来了新的设计挑战。近年来，Ansys、西门子和Keysight等厂商推出的电子设计自动化（EDA）工具，能够对DDR信号进行仿真与优化，以确保符合联合电子器件工程委员会（JEDEC）标准。许多供应商已开发出DDR向导工具，可自动完成信号完整性分析，从而大幅缩短仿真设置时间。然而，仿真工具仍存在局限性，其结果在真实航空电子板设计中的可靠性尚未得到充分验证。本研究旨在对比市场上各类仿真工具提供的DDR向导功能，并将仿真结果与实测数据进行关联分析。这一研究有助于增强对所选工具的信心，建立一种可靠的仿真方法论，使未来航空电子设计仅依赖仿真即可完成，无需再进行额外的实测或制作测试样机，从而显著缩短设计周期。"
  }
]