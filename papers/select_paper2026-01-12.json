[
  {
    "date": "2026-01-12",
    "title": "Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models",
    "authors": "Pranav Kallem",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07245v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) achieve strong aver- age performance yet remain unreliable at the instance level, with frequent hallucinations, brittle failures, and poorly calibrated confidence. We study reliability through the lens of multi-model consensus: given responses from several heterogeneous LLMs, can we learn which answer is most likely correct for a given query? We introduce a Multi-Model Consensus Reasoning Engine that treats the set of LLM outputs as input to a supervised meta-learner. The system maps natural language responses into structured features using semantic embeddings, pairwise similarity and clustering statistics, lexical and structural cues, reasoning-quality scores, confidence estimates, and model-specific priors, and then applies gradient-boosted trees, listwise ranking, and graph neural networks over similarity graphs of answers. Using three open-weight LLMs evaluated on compact, resource- constrained subsets of GSM8K, ARC-Challenge, HellaSwag, and TruthfulQA, our best graph-attention-based consensus model improves macro-average accuracy by 4.6 percentage points over the strongest single LLM and by 8.1 points over majority vote, while also yielding lower Brier scores and fewer TruthfulQA hal- lucinations. Ablation and feature-importance analyses show that semantic agreement and clustering features are most influential, with reasoning-quality and model-prior features providing com- plementary gains, suggesting supervised multi-model consensus is a practical route toward more reliable LLM behavior, even in a modest single-machine setup.",
    "title_zh": "学会信任群体：一种用于大型语言模型的多模型共识推理引擎",
    "abstract_zh": "大型语言模型（LLMs）在平均性能上表现强劲，但在个体实例层面仍不可靠，常出现幻觉、脆弱性失败以及置信度校准不佳等问题。本文从多模型共识的视角研究可靠性：给定多个异构LLM对同一问题的响应，我们能否学习到最可能正确的答案？为此，我们提出一种多模型共识推理引擎，将多个LLM的输出作为输入，交由一个监督式元学习器进行处理。该系统通过语义嵌入、成对相似性与聚类统计、词汇与结构线索、推理质量评分、置信度估计以及模型特异性先验等手段，将自然语言响应映射为结构化特征；随后利用梯度提升树、列表排序（listwise ranking）以及基于答案相似性图的图神经网络进行建模。我们在三个开源权重的LLM上，针对GSM8K、ARC-Challenge、HellaSwag和TruthfulQA四个数据集的紧凑、资源受限子集进行了评估。结果显示，我们最优的基于图注意力机制的共识模型，在宏平均准确率上比最强的单个LLM提升了4.6个百分点，相比多数投票方法更是提高了8.1个百分点，同时Brier得分更低，TruthfulQA中的幻觉现象也显著减少。消融实验与特征重要性分析表明，语义一致性与聚类特征最具影响力，而推理质量与模型先验特征则提供了互补性增益。这表明，即使在资源有限的单机环境下，监督式多模型共识仍是实现更可靠LLM行为的一条切实可行路径。"
  },
  {
    "date": "2026-01-12",
    "title": "Can Large Language Models Understand, Reason About, and Generate Code-Switched Text?",
    "authors": "Genta Indra Winata, David Anugraha, Patrick Amadeus Irawan, Anirban Das, Haneul Yoo, Paresh Dashore, Shreyas Kulkarni, Ruochen Zhang, Haruki Sakajo, Frederikus Hudi, Anaelia Ovalle, Syrielle Montariol, Felix Gaschi, Michael Anugraha, Rutuj Ravindra Puranik, Zawad Hayat Ahmed, Adril Putra Merin, Emmanuele Chersoni",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07153v1",
    "source": "arXiv",
    "abstract": "Code-switching is a pervasive phenomenon in multilingual communication, yet the robustness of large language models (LLMs) in mixed-language settings remains insufficiently understood. In this work, we present a comprehensive evaluation of LLM capabilities in understanding, reasoning over, and generating code-switched text. We introduce CodeMixQA a novel benchmark with high-quality human annotations, comprising 16 diverse parallel code-switched language-pair variants that span multiple geographic regions and code-switching patterns, and include both original scripts and their transliterated forms. Using this benchmark, we analyze the reasoning behavior of LLMs on code-switched question-answering tasks, shedding light on how models process and reason over mixed-language inputs. We further conduct a systematic evaluation of LLM-generated synthetic code-switched text, focusing on both naturalness and semantic fidelity, and uncover key limitations in current generation capabilities. Our findings reveal persistent challenges in both reasoning and generation under code-switching conditions and provide actionable insights for building more robust multilingual LLMs. We release the dataset and code as open source.",
    "title_zh": "大型语言模型能否理解、推理并生成代码转换文本？",
    "abstract_zh": "代码切换是多语言交流中普遍存在的现象，然而大型语言模型（LLM）在混合语言环境下的鲁棒性仍缺乏充分理解。本文系统评估了大型语言模型在理解、推理和生成代码切换文本方面的能力。我们提出了一个名为CodeMixQA的新基准，该基准包含高质量的人工标注数据，涵盖16种多样化的并行代码切换语言对，覆盖多个地理区域和不同的代码切换模式，并包含原始文字及对应的转写形式。基于此基准，我们分析了大型语言模型在代码切换问答任务中的推理行为，揭示了模型如何处理和推理混合语言输入。此外，我们还对大型语言模型生成的合成代码切换文本进行了系统的评估，重点关注其自然度与语义保真度，发现了当前生成能力中的关键局限性。研究结果表明，在代码切换条件下，模型在推理与生成方面仍面临持续挑战，并为构建更具鲁棒性的多语言大型语言模型提供了可操作的洞见。我们已将数据集和代码开源发布。"
  },
  {
    "date": "2026-01-12",
    "title": "Thinking Before Constraining: A Unified Decoding Framework for Large Language Models",
    "authors": "Ngoc Trinh Hung Nguyen, Alonso Silva, Laith Zumot, Liubov Tupikina, Armen Aghasaryan, Mehwish Alam",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07525v1",
    "source": "arXiv",
    "abstract": "Natural generation allows Language Models (LMs) to produce free-form responses with rich reasoning, but the lack of guaranteed structure makes outputs difficult to parse or verify. Structured generation, or constrained decoding, addresses this drawback by producing content in standardized formats such as JSON, ensuring consistency and guaranteed-parsable outputs, but it can inadvertently restrict the model's reasoning capabilities. In this work, we propose a simple approach that combines the advantages of both natural and structured generation. By allowing LLMs to reason freely until specific trigger tokens are generated, and then switching to structured generation, our method preserves the expressive power of natural language reasoning while ensuring the reliability of structured outputs. We further evaluate our approach on several datasets, covering both classification and reasoning tasks, to demonstrate its effectiveness, achieving a substantial gain of up to 27% in accuracy compared to natural generation, while requiring only a small overhead of 10-20 extra tokens.",
    "title_zh": "思考在约束之前：大语言模型的统一解码框架",
    "abstract_zh": "自然生成使语言模型（LMs）能够产生富含推理能力的自由格式回应，但缺乏结构保证使得输出难以解析或验证。结构化生成（或受限解码）通过生成如JSON等标准化格式的内容来解决这一缺陷，确保输出的一致性和可解析性，但可能无意中限制了模型的推理能力。在本研究中，我们提出一种简单有效的方法，结合了自然生成与结构化生成的优势。该方法允许大语言模型在生成过程中自由推理，直到特定触发标记被生成，随后切换至结构化生成模式。这种方法既保留了自然语言推理的表达力，又确保了输出结果的可靠性。我们在多个数据集上对所提方法进行了评估，涵盖分类和推理任务，结果表明其有效性显著：相比自然生成，准确率最高提升达27%，且仅需增加10-20个额外token的开销。"
  },
  {
    "date": "2026-01-12",
    "title": "KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning",
    "authors": "Qitan Lv, Tianyu Liu, Qiaosheng Zhang, Xingcheng Xu, Chaochao Lu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07430v1",
    "source": "arXiv",
    "abstract": "Despite the impressive performance of large language models (LLMs) pretrained on vast knowledge corpora, advancing their knowledge manipulation-the ability to effectively recall, reason, and transfer relevant knowledge-remains challenging. Existing methods mainly leverage Supervised Fine-Tuning (SFT) on labeled datasets to enhance LLMs' knowledge manipulation ability. However, we observe that SFT models still exhibit the known&incorrect phenomenon, where they explicitly possess relevant knowledge for a given question but fail to leverage it for correct answers. To address this challenge, we propose KALE (Knowledge-Aware LEarning)-a post-training framework that leverages knowledge graphs (KGs) to generate high-quality rationales and enhance LLMs' knowledge manipulation ability. Specifically, KALE first introduces a Knowledge-Induced (KI) data synthesis method that efficiently extracts multi-hop reasoning paths from KGs to generate high-quality rationales for question-answer pairs. Then, KALE employs a Knowledge-Aware (KA) fine-tuning paradigm that enhances knowledge manipulation by internalizing rationale-guided reasoning through minimizing the KL divergence between predictions with and without rationales. Extensive experiments on eight popular benchmarks across six different LLMs demonstrate the effectiveness of KALE, achieving accuracy improvements of up to 11.72% and an average of 4.18%.",
    "title_zh": "KALE：通过知识感知学习增强大语言模型中的知识操作",
    "abstract_zh": "尽管大规模语言模型（LLMs）在海量知识语料库上进行了预训练，表现出令人瞩目的性能，但提升其知识操作能力——即有效回忆、推理和迁移相关知识的能力——仍面临挑战。现有方法主要依赖在标注数据集上的监督微调（SFT）来增强LLMs的知识操作能力。然而，我们观察到，SFT模型仍然存在“已知却错误”现象：它们虽然明确掌握了解答特定问题所需的相关知识，却无法正确利用这些知识得出正确答案。为解决这一难题，我们提出了KALE（Knowledge-Aware LEarning）——一种基于知识图谱（KGs）生成高质量推理过程的后训练框架，以增强LLMs的知识操作能力。具体而言，KALE首先引入一种知识引导（KI）的数据合成方法，高效地从知识图谱中提取多跳推理路径，从而为问答对生成高质量的推理过程。随后，KALE采用一种知识感知（KA）的微调范式，通过最小化有无推理过程时预测结果之间的KL散度，使模型能够内化基于推理过程的思维路径，从而强化知识操作能力。在六个不同LLM上针对八个主流基准的大量实验表明，KALE具有显著有效性，准确率最高提升达11.72%，平均提升4.18%。"
  },
  {
    "date": "2026-01-12",
    "title": "Resolution of Erdős Problem #728: a writeup of Aristotle's Lean proof",
    "authors": "Nat Sothanaphan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07421v1",
    "source": "arXiv",
    "abstract": "We provide a writeup of a resolution of Erdős Problem #728; this is the first Erdos problem (a problem proposed by Paul Erdős which has been collected in the Erdos Problems website) regarded as fully resolved autonomously by an AI system. The system in question is a combination of GPT-5.2 Pro by OpenAI and Aristotle by Harmonic, operated by Kevin Barreto. The final result of the system is a formal proof written in Lean, which we translate to informal mathematics in the present writeup for wider accessibility. The proved result is as follows. We show a logarithmic-gap phenomenon regarding factorial divisibility: For any constants $0<C_1<C_2$ there exist infinitely many triples $(a,b,n)\\in\\mathbb N^3$ such that \\[ a!\\,b!\\mid n!\\,(a+b-n)!\\qquad\\text{and}\\qquad C_1\\log n < a+b-n < C_2\\log n. \\] The argument reduces this to a binomial divisibility $\\binom{m+k}{k}\\mid\\binom{2m}{m}$ and studies it prime-by-prime. By Kummer's theorem, $ν_p\\binom{2m}{m}$ translates into a carry count for doubling $m$ in base $p$. We then employ a counting argument to find, in each scale $[M,2M]$, an integer $m$ whose base-$p$ expansions simultaneously force many carries when doubling $m$, for every prime $p\\le 2k$, while avoiding the rare event that one of $m+1,\\dots,m+k$ is divisible by an unusually high power of $p$. These \"carry-rich but spike-free\" choices of $m$ force the needed $p$-adic inequalities and the divisibility. The overall strategy is similar to results regarding divisors of $\\binom{2n}{n}$ studied earlier by Erdős and by Pomerance.",
    "title_zh": "埃爾多斯問題 #728 的解決：亞里士多德的 Lean 證明筆記",
    "abstract_zh": "我们提供对埃尔德什问题 #728 的一个解答说明；这是首个由人工智能系统独立、完整解决的埃尔德什问题（即由保罗·埃尔德什提出并被收录于“埃尔德什问题网站”的问题）。该系统由 OpenAI 的 GPT-5.2 Pro 与 Harmonic 公司开发的 Aristotle 系统组合而成，由凯文·巴雷托（Kevin Barreto）操作。系统的最终成果是一份用 Lean 编写的严格形式化证明，本文将该证明转化为非形式化的数学语言，以便更广泛地理解。\n\n所证明的结果如下：我们揭示了一个关于阶乘整除性的对数间隙现象——对于任意常数 $0 < C_1 < C_2$，存在无穷多组三元组 $(a, b, n) \\in \\mathbb{N}^3$，使得  \n$$\na!\\,b! \\mid n!\\,(a + b - n)!\n\\quad\\text{且}\\quad\nC_1 \\log n < a + b - n < C_2 \\log n.\n$$\n\n该问题的论证思路将其归约为一个二项式系数的整除性问题：$\\binom{m + k}{k} \\mid \\binom{2m}{m}$，并逐个素数进行分析。根据库默尔定理（Kummer's Theorem），$ν_p\\binom{2m}{m}$ 可以转化为在 $p$ 进制下将 $m$ 加倍时产生的进位次数。我们随后采用计数论证方法，在每个尺度区间 $[M, 2M]$ 中寻找一个整数 $m$，使其在所有满足 $p \\leq 2k$ 的素数 $p$ 下，其 $p$ 进制表示在加倍时均产生大量进位，同时避免出现罕见事件：即 $m+1, \\dots, m+k$ 中某一个被某个素数 $p$ 的异常高次幂整除。\n\n这些“进位丰富但无尖峰”（carry-rich but spike-free）的 $m$ 值，能够强制满足所需的 $p$-进制不等式，从而保证整体的整除关系成立。\n\n整体策略与早期埃尔德什及波默朗斯（Pomerance）研究 $\\binom{2n}{n}$ 的因子结构的工作有相似之处。"
  },
  {
    "date": "2026-01-12",
    "title": "Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection",
    "authors": "Mariana Costa, Alberlucia Rafael Soarez, Daniel Kim, Camila Ferreira",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07780v1",
    "source": "arXiv",
    "abstract": "While Chain-of-Thought (CoT) prompting advances LLM reasoning, challenges persist in consistency, accuracy, and self-correction, especially for complex or ethically sensitive tasks. Existing single-dimensional reflection methods offer insufficient improvements. We propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel methodology employing structured multi-perspective reflection. After initial CoT, PR-CoT guides the LLM to self-assess its reasoning across multiple predefined angles: logical consistency, information completeness, biases/ethics, and alternative solutions. Implemented purely via prompt engineering, this process refines the initial CoT into a more robust and accurate final answer without model retraining. Experiments across arithmetic, commonsense, ethical decision-making, and logical puzzles, using GPT-three point five and GPT-four models, demonstrate PR-CoT's superior performance. It significantly outperforms traditional CoT and existing reflection methods in logical consistency and error correction, with notable gains in nuanced domains like ethical decision-making. Ablation studies, human evaluations, and qualitative analyses further validate the contribution of each reflection perspective and the overall efficacy of our poly-reflective paradigm in fostering more reliable LLM reasoning.",
    "title_zh": "通过多视角反思提升大型语言模型的自我修正能力",
    "abstract_zh": "尽管思维链（Chain-of-Thought, CoT）提示技术推动了大语言模型（LLM）推理能力的发展，但在复杂或涉及伦理敏感性的任务中，仍存在一致性、准确性及自我修正方面的挑战。现有的单维度反思方法改进有限。为此，我们提出一种名为“MyGO多视角反思思维链”（Poly-Reflective Chain-of-Thought, PR-CoT）的新方法，采用结构化的多角度反思机制。在完成初始思维链推理后，PR-CoT引导大语言模型从多个预设维度对自身推理过程进行自我评估：逻辑一致性、信息完整性、偏见与伦理考量，以及替代解决方案。该方法完全通过提示工程实现，无需模型重训练，即可将初始思维链优化为更稳健、更准确的最终答案。在算术、常识推理、伦理决策和逻辑谜题等任务上，基于GPT-3.5和GPT-4模型的实验表明，PR-CoT显著优于传统CoT及现有反思方法，在逻辑一致性和错误修正方面表现突出，尤其在伦理决策等细微领域展现出显著提升。消融实验、人工评估与定性分析进一步验证了各反思维度的贡献，充分证明了多视角反思范式在提升大语言模型推理可靠性方面的有效性。"
  },
  {
    "date": "2026-01-12",
    "title": "OODEval: Evaluating Large Language Models on Object-Oriented Design",
    "authors": "Bingxu Xiao, Yunwei Dong, Yiqi Tang, Manqing Zhang, Yifan Zhou, Chunyan Ma, Yepang Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07602v1",
    "source": "arXiv",
    "abstract": "Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.",
    "title_zh": "OODEval：评估大语言模型在面向对象设计方面的表现",
    "abstract_zh": "近年来，大型语言模型（LLMs）的快速发展推动了其在软件工程领域的广泛评估。然而，以往的研究大多集中于代码层面的任务，而对软件设计能力的探索仍显不足。为填补这一空白，我们开展了一项全面的实证研究，评估29个大型语言模型在面向对象设计（OOD）任务上的表现。由于缺乏标准化的基准和评价指标，我们提出了OODEval——一个手工构建的基准，包含50个难度各异的OOD任务；以及OODEval-Human，这是首个由人类评分的OOD基准，收录了940份由本科生提交的类图，并由教师进行评分。此外，我们还提出了CLUE（Class Likeness Unified Evaluation）——一种统一的评估指标体系，用于同时衡量类图生成的整体正确性与细粒度的设计质量。\n\n基于这些基准和指标，我们围绕五个核心研究问题展开分析：整体正确性、与人类表现的对比、模型规模的影响、任务特征的影响，以及错误案例的深入分析。研究结果表明，尽管LLMs在语法层面表现出较高的准确性，但在语义层面存在显著缺陷，尤其是在方法和关系生成方面。在所评估的模型中，Qwen3-Coder-30B表现最佳，其性能可与DeepSeek-R1和GPT-4o相媲美；而Gemma3-4B-IT虽参数量较小，却优于GPT-4o-Mini。尽管顶尖LLMs的平均表现已接近本科生水平，但仍远低于最优秀的人类设计师。进一步分析显示，模型参数规模、代码专精程度以及指令微调对性能有显著影响；而设计复杂度增加和需求可读性降低则会明显削弱模型表现。错误案例分析揭示了常见的失败模式，包括关键词误用、类或关系缺失、方法遗漏等问题。"
  },
  {
    "date": "2026-01-12",
    "title": "PROTEA: Securing Robot Task Planning and Execution",
    "authors": "Zainab Altaweel, Mohaiminul Al Nahian, Jake Juettner, Adnan Siraj Rakin, Shiqi Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07186v1",
    "source": "arXiv",
    "abstract": "Robots need task planning methods to generate action sequences for complex tasks. Recent work on adversarial attacks has revealed significant vulnerabilities in existing robot task planners, especially those built on foundation models. In this paper, we aim to address these security challenges by introducing PROTEA, an LLM-as-a-Judge defense mechanism, to evaluate the security of task plans. PROTEA is developed to address the dimensionality and history challenges in plan safety assessment. We used different LLMs to implement multiple versions of PROTEA for comparison purposes. For systemic evaluations, we created a dataset containing both benign and malicious task plans, where the harmful behaviors were injected at varying levels of stealthiness. Our results provide actionable insights for robotic system practitioners seeking to enhance robustness and security of their task planning systems. Details, dataset and demos are provided: https://protea-secure.github.io/PROTEA/",
    "title_zh": "PROTEA：保障机器人任务规划与执行",
    "abstract_zh": "机器人需要任务规划方法来生成复杂任务的动作序列。近期关于对抗攻击的研究揭示了现有机器人任务规划器存在显著的安全漏洞，尤其是基于基础模型构建的规划器。本文旨在通过引入PROTEA——一种基于大语言模型（LLM）作为裁判的防御机制，来评估任务计划的安全性，以应对这些安全挑战。PROTEA专门针对计划安全性评估中的维度复杂性和历史依赖性难题而设计。我们采用不同的大语言模型实现了多个版本的PROTEA，以便进行对比分析。为实现系统性评估，我们构建了一个包含良性与恶意任务计划的数据集，其中恶意行为以不同隐蔽程度被注入。实验结果为希望提升任务规划系统鲁棒性与安全性的机器人系统从业者提供了切实可行的洞见。详细信息、数据集及演示视频请访问：https://protea-secure.github.io/PROTEA/"
  },
  {
    "date": "2026-01-12",
    "title": "Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure",
    "authors": "Nicolas Tacheny",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07342v1",
    "source": "arXiv",
    "abstract": "Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models, where failures propagate across physical and logical components and affect multiple customers. Traditional approaches to root cause analysis(RCA) rely on hard-coded graph traversal algorithms or rule-based correlation engines, which are costly to maintain and tightly coupled to the infrastructure model. In this work, we introduce an agentic diagnostic framework where a Large Language Model (LLM) performs step-wise investigation using a constrained tool space exposed through the Model Context Protocol (MCP). Instead of embedding causal logic or traversal algorithms into the application, the agent autonomously navigates the infrastructure model by invoking tools for service lookup, dependency retrieval, structured and unstructured data, and event analysis, and impact discovery. We define an investigation protocol that structures the agent's reasoning and ensures grounding, reproducibility, and safe handling of missing or ambiguous information. This work lays the foundation for autonomous incident resolution and change impact mitigation. Future systems will not only diagnose and remediate infrastructure failures, but also predict the impact of planned changes on services and customers, enabling operators to mitigate risks before executing maintenance operations.",
    "title_zh": "面向电信与数据中心基础设施的自主诊断推理",
    "abstract_zh": "大规模电信和数据中心基础设施依赖于多层服务与资源模型，其中故障会跨物理和逻辑组件传播，并影响多个客户。传统的根因分析（RCA）方法依赖于硬编码的图遍历算法或基于规则的相关性引擎，这些方法维护成本高，且与基础设施模型紧密耦合。在本研究中，我们提出了一种代理式诊断框架，其中大型语言模型（LLM）通过模型上下文协议（MCP）暴露的受限工具空间，执行逐步调查。与将因果逻辑或遍历算法嵌入应用不同，该代理能够自主导航基础设施模型，调用工具进行服务查询、依赖关系获取、结构化与非结构化数据检索、事件分析以及影响发现。我们定义了一套调查协议，规范了代理的推理过程，确保其推理具有依据性、可复现性，并能安全处理缺失或模糊信息。本工作为实现自主故障修复和变更影响缓解奠定了基础。未来的系统不仅能够诊断并修复基础设施故障，还能预测计划内变更对服务和客户的影响，使运维人员能够在执行维护操作前主动规避风险。"
  },
  {
    "date": "2026-01-12",
    "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
    "authors": "Wei Fang, James Glass",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07782v1",
    "source": "arXiv",
    "abstract": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
    "title_zh": "超越单次调用：通过查询规划实现多步工具检索",
    "abstract_zh": "基于庞大且动态的工具库运行的大语言模型（LLM）代理依赖于高效的检索机制，然而传统的单次密集检索方法在处理复杂请求时表现不佳。这类失败主要源于用户抽象目标与技术文档之间的语义脱节，以及固定尺寸嵌入表示在建模工具组合的组合性方面能力有限。为解决上述挑战，我们提出TOOLQP——一种轻量级框架，将检索过程建模为迭代式查询规划。与传统的单次匹配不同，TOOLQP将指令分解为子任务，并动态生成查询以与检索器交互，通过精准聚焦于完成组合所需的特定子任务，有效弥合了语义鸿沟。我们采用合成查询轨迹进行训练，并通过可验证奖励的强化学习（RLVR）进行优化。实验结果表明，TOOLQP达到了当前最优性能，在零样本泛化能力、对多种检索器的鲁棒性以及下游代理执行效率方面均有显著提升。"
  },
  {
    "date": "2026-01-12",
    "title": "Examining the Effectiveness of Transformer-Based Smart Contract Vulnerability Scan",
    "authors": "Emre Balci, Timucin Aydede, Gorkem Yilmaz, Ece Gelal Soyak",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07334v1",
    "source": "arXiv",
    "abstract": "Smart contract technology facilitates self-executing agreements on the blockchain, eliminating dependency on an external trusted authority. However, smart contracts may expose vulnerabilities that can lead to financial losses and disruptions in decentralized applications. In this work, we evaluate deep learning-based approaches for vulnerability scanning of Ethereum smart contracts. We propose VASCOT, a Vulnerability Analyzer for Smart COntracts using Transformers, which performs sequential analysis of Ethereum Virtual Machine (EVM) bytecode and incorporates a sliding window mechanism to overcome input length constraints. To assess VASCOT's detection efficacy, we construct a dataset of 16,469 verified Ethereum contracts deployed in 2022, and annotate it using trace analysis with concrete validation to mitigate false positives. VASCOT's performance is then compared against a state-of-the-art LSTM-based vulnerability detection model on both our dataset and an older public dataset. Our findings highlight the strengths and limitations of each model, providing insights into their detection capabilities and generalizability.",
    "title_zh": "基于Transformer的智能合约漏洞扫描有效性研究",
    "abstract_zh": "智能合约技术能够在区块链上实现自动执行的协议，从而消除对第三方可信机构的依赖。然而，智能合约可能暴露漏洞，导致去中心化应用中出现财务损失和系统中断。本文评估了基于深度学习的以太坊智能合约漏洞扫描方法。我们提出了VASCOT（基于Transformer的智能合约漏洞分析器），该模型对以太坊虚拟机（EVM）字节码进行序列化分析，并引入滑动窗口机制以克服输入长度限制。为评估VASCOT的检测效果，我们构建了一个包含16,469个经验证的2022年部署的以太坊合约的数据集，并通过追踪分析与实际验证相结合的方式进行标注，以减少误报。随后，我们将VASCOT在我们的数据集以及一个较早的公开数据集上与当前最先进的基于LSTM的漏洞检测模型进行了对比。研究结果揭示了两种模型各自的优缺点，为它们的检测能力与泛化性能提供了深入见解。"
  },
  {
    "date": "2026-01-12",
    "title": "VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing",
    "authors": "Guanyuan Pan, Yugui Lin, Tiansheng Zhou, Pietro Liò, Shuai Wang, Yaqi Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07315v1",
    "source": "arXiv",
    "abstract": "Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches often underutilize circuit schematics and lack the explainability required for industry adoption. To tackle these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-starting from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance, achieving a 100% success rate in optimizing an amplifier with a complementary input and a class-AB output stage, while maintaining total runtime under 43 minutes across all experiments.",
    "title_zh": "VLM-CAD：面向模拟电路尺寸优化的VLM增强型协作智能体设计流程",
    "abstract_zh": "模拟混合信号电路尺寸设计涉及高维设计空间中的复杂权衡。现有的自动模拟电路尺寸设计方法通常未能充分利用电路原理图信息，且缺乏工业应用所必需的可解释性。为应对这些挑战，我们提出了一种基于视觉语言模型优化的协同代理设计工作流（VLM-CAD），该工作流能够分析电路、优化直流工作点、基于推理进行尺寸估算，并执行外部尺寸优化。我们引入Image2Net对电路原理图进行标注，并生成结构化的JSON描述，以便视觉语言模型能够精确解析。此外，我们提出了一种可解释的信任区域贝叶斯优化方法（ExTuRBO），该方法通过代理生成的初始种子实现协同预热，并提供双粒度灵敏度分析，用于外部尺寸优化，支持生成全面的最终设计报告。在采用180nm、90nm和45nm预测技术模型的放大器尺寸设计任务实验中，VLM-CAD有效平衡了功耗与性能，在优化具有互补输入和AB类输出级的放大器时实现了100%的成功率，且所有实验的总运行时间均控制在43分钟以内。"
  },
  {
    "date": "2026-01-12",
    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
    "authors": "Weihao Xuan, Qingcheng Zeng, Heli Qi, Yunze Xiao, Junjue Wang, Naoto Yokoya",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07264v1",
    "source": "arXiv",
    "abstract": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
    "title_zh": "信心悖论：分析与缓解工具使用代理中的校准偏差",
    "abstract_zh": "基于大语言模型（LLM）的自主代理正在迅速发展，以应对多轮任务，但确保其可信性仍是一个关键挑战。可信性的基本支柱是校准能力，即代理表达信心的程度能够真实反映其实际表现水平。尽管校准在静态模型中已有充分研究，但在集成工具的代理工作流中，其动态特性仍缺乏深入探索。本文系统地研究了在使用工具的代理中进行言语化校准的现象，揭示了一种由工具类型驱动的根本性信心二分现象。具体而言，我们的初步研究表明，证据类工具（如网络搜索）由于检索信息中固有的噪声，会系统性地引发严重过度自信；而验证类工具（如代码解释器）则可通过确定性反馈来支撑推理，从而缓解校准偏差。为在各类工具间实现稳健的校准改进，我们提出了一种强化学习（RL）微调框架，该框架联合优化任务准确率与校准性能，并辅以全面的奖励设计基准测试。实验表明，经过训练的代理不仅实现了更优的校准效果，还在从局部训练环境到嘈杂网络场景、再到数学推理等不同领域展现出强大的泛化能力。研究结果强调了针对工具使用代理采用领域特定校准策略的必要性。更广泛地说，本工作为构建具备自我意识的智能体奠定了基础，使其能够在高风险、现实世界的应用中可靠地传达不确定性。"
  },
  {
    "date": "2026-01-12",
    "title": "SwarmFoam: An OpenFOAM Multi-Agent System Based on Multiple Types of Large Language Models",
    "authors": "Chunwei Yang, Yankai Wang, Jianxiang Tang, Haojie Qu, Ziqiang Zou, YuLiu, Chunrui Deng, Zhifang Qiu, Ming Ding",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07252v1",
    "source": "arXiv",
    "abstract": "Numerical simulation is one of the mainstream methods in scientific research, typically performed by professional engineers. With the advancement of multi-agent technology, using collaborating agents to replicate human behavior shows immense potential for intelligent Computational Fluid Dynamics (CFD) simulations. Some muti-agent systems based on Large Language Models have been proposed. However, they exhibit significant limitations when dealing with complex geometries. This paper introduces a new multi-agent simulation framework, SwarmFoam. SwarmFoam integrates functionalities such as Multi-modal perception, Intelligent error correction, and Retrieval-Augmented Generation, aiming to achieve more complex simulations through dual parsing of images and high-level instructions. Experimental results demonstrate that SwarmFoam has good adaptability to simulation inputs from different modalities. The overall pass rate for 25 test cases was 84%, with natural language and multi-modal input cases achieving pass rates of 80% and 86.7%, respectively. The work presented by SwarmFoam will further promote the development of intelligent agent methods for CFD.",
    "title_zh": "SwarmFoam：一种基于多种大型语言模型的OpenFOAM多智能体系统",
    "abstract_zh": "数值模拟是科学研究中的主流方法之一，通常由专业工程师完成。随着多智能体技术的发展，利用协作智能体模拟人类行为，在实现智能计算流体动力学（CFD）仿真方面展现出巨大潜力。目前已有一些基于大语言模型的多智能体系统被提出，但在处理复杂几何结构时仍存在显著局限性。本文提出了一种新的多智能体仿真框架——SwarmFoam。该框架集成了多模态感知、智能纠错以及检索增强生成等功能，通过图像与高层指令的双重解析，旨在实现更复杂的仿真任务。实验结果表明，SwarmFoam对来自不同模态的仿真输入具有良好的适应性：在25个测试案例中，整体通过率达到84%，其中自然语言输入和多模态输入的通过率分别达到80%和86.7%。SwarmFoam所展示的工作将有力推动CFD领域智能代理方法的进一步发展。"
  },
  {
    "date": "2026-01-12",
    "title": "Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation (Extended Version)",
    "authors": "Aja Khanal, Kaushik T. Ranade, Rishabh Agrawal, Kalyan S. Basu, Apurva Narayan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07152v1",
    "source": "arXiv",
    "abstract": "Generating high-quality structured data such as JSON records, remains a fundamental challenge for large language models (LLMs), particularly when semantic richness must coexist with strict schema adherence. While autoregressive LLMs offer strong structural consistency, they often struggle with semantic variation and output diversity. In contrast, diffusion language models (DLMs) introduce powerful mechanisms for semantic richness and bidirectional decoding, yet lack the inductive biases needed for reliable structure preservation. We present Agents of Diffusion (AoD), a novel framework that unifies the generative flexibility of DLMs with the reasoning capabilities of autoregressive models through language-mediated reinforcement learning. AoD frames structured text generation as a multi-agent alignment process, where a prompt optimization agent collaborates with a judge agent to iteratively guide a DLM using natural language feedback. This approach enables controllable, schema-consistent generation without modifying model parameters or relying on handcrafted constraints. AoD advances the state of controllable generation by demonstrating that diffusion models, when supervised by cooperative agents, can achieve both high semantic novelty and structural fidelity. Across multiple structured data benchmarks, AoD consistently outperforms diffusion and autoregressive baselines, establishing a new path forward for structure-aware, diversity-enhanced text synthesis.",
    "title_zh": "扩散的代理人：利用多智能体强化学习增强扩散语言模型以生成结构化数据（扩展版）",
    "abstract_zh": "生成高质量的结构化数据（如JSON记录）仍然是大型语言模型（LLMs）面临的一个基本挑战，尤其是在需要兼顾语义丰富性与严格模式一致性的情况下。尽管自回归语言模型在结构一致性方面表现优异，但往往在语义变化和输出多样性方面存在不足。相比之下，扩散语言模型（DLMs）引入了强大的语义丰富机制和双向解码能力，却缺乏维持可靠结构所必需的归纳偏置。我们提出了“扩散代理”（Agents of Diffusion, AoD）——一种新颖的框架，通过语言媒介强化学习，将DLM的生成灵活性与自回归模型的推理能力相结合。AoD将结构化文本生成建模为一个多智能体对齐过程，其中提示优化代理与评判代理协同工作，利用自然语言反馈迭代引导DLM生成。该方法无需修改模型参数，也无需依赖手工设计的约束，即可实现可控且符合模式的生成。AoD推动了可控生成技术的发展，证明了在合作智能体的监督下，扩散模型能够同时实现高度的语义新颖性与结构保真度。在多个结构化数据基准测试中，AoD始终优于扩散模型和自回归基线模型，为面向结构感知、增强多样性的文本合成开辟了新的路径。"
  },
  {
    "date": "2026-01-12",
    "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
    "authors": "Abdullah Al Mujahid, Mia Mohammad Imran",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07786v1",
    "source": "arXiv",
    "abstract": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
    "title_zh": "“待办事项：修复Gemini造成的混乱”：理解生成式AI引发的自我承认的技术债务",
    "abstract_zh": "随着大型语言模型（LLMs）如ChatGPT、Copilot、Claude和Gemini逐渐融入软件开发工作流程，开发者在代码注释中越来越多地留下AI参与的痕迹。其中，部分注释不仅明确承认生成式AI的使用，还坦承存在技术缺陷。通过对2022年11月至2025年7月期间公开的基于Python和JavaScript的GitHub仓库中6,540条提及LLM的代码注释进行分析，我们识别出81条同时自我承认存在技术债务（SATD）的注释。开发者最常描述的问题包括推迟测试、未完成适配以及对AI生成代码的理解有限，这表明AI辅助既影响了技术债务出现的时间，也影响了其产生的原因。为此，我们提出“GenAI-Induced Self-admitted Technical Debt”（GIST）这一概念性视角，用以描述一种反复出现的现象：开发者在明确引入AI生成代码的同时，又主动表达对其行为或正确性的不确定。"
  },
  {
    "date": "2026-01-12",
    "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
    "authors": "Lei Zhang, Mouxiang Chen, Ruisheng Cao, Jiawei Chen, Fan Zhou, Yiheng Xu, Jiaxi Yang, Liang Chen, Changwei Luo, Kai Zhang, Fan Yan, KaShun Shum, Jiajun Zhang, Zeyu Cui, Hu Feng, Junyang Lin, Binyuan Hui, Min Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07526v1",
    "source": "arXiv",
    "abstract": "The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.",
    "title_zh": "MegaFlow：面向智能体时代的大型分布式编排系统",
    "abstract_zh": "交互式与自主型AI系统的快速发展标志着我们正步入“代理时代”。在软件工程、计算机使用等复杂代理任务上训练和评估智能体，不仅需要高效的模型计算能力，还需要能够协调大规模智能体-环境交互的先进基础设施。然而，目前尚无开源基础设施能够有效支持此类复杂代理任务的大规模训练与评估。为应对这一挑战，我们提出了MegaFlow——一个大规模分布式编排系统，可实现对智能体-环境工作负载的高效调度、资源分配及细粒度任务管理。MegaFlow将智能体训练基础设施抽象为三个独立的服务（模型服务、智能体服务和环境服务），通过统一接口进行交互，从而支持在不同智能体-环境配置下独立扩展和灵活资源配置。在实际智能体训练部署中，MegaFlow成功编排了数以万计的并发智能体任务，同时保持了高系统稳定性并实现了高效的资源利用率。通过支持如此大规模的智能体训练，MegaFlow填补了新兴代理型AI生态中一项关键的基础设施空白。"
  },
  {
    "date": "2026-01-12",
    "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
    "authors": "Xin Cheng, Wangding Zeng, Damai Dai, Qinyu Chen, Bingxuan Wang, Zhenda Xie, Kezhao Huang, Xingkai Yu, Zhewen Hao, Yukun Li, Han Zhang, Huishuai Zhang, Dongyan Zhao, Wenfeng Liang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07372v1",
    "source": "arXiv",
    "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.",
    "title_zh": "通过可扩展查找实现的条件记忆：大型语言模型中稀疏性的一个新维度",
    "abstract_zh": "尽管混合专家（Mixture-of-Experts, MoE）通过条件计算实现了模型容量的扩展，但Transformer缺乏原生的知识检索机制，不得不以低效的计算方式模拟检索过程。为解决这一问题，我们引入了**条件记忆**作为互补的稀疏性维度，其具体实现基于Engram模块——该模块对经典的$N$-gram嵌入进行了现代化改造，实现了$O(1)$时间复杂度的快速查找。通过构建“稀疏性分配”（Sparsity Allocation）问题，我们发现了一条U型缩放规律，该规律在神经计算（MoE）与静态记忆（Engram）之间实现了最优权衡。基于此规律，我们将Engram扩展至270亿参数规模，在保持与MoE基线相同参数量和FLOPs的前提下，显著超越了纯MoE模型的表现。\n\n尤为值得注意的是，尽管预期记忆模块主要提升知识检索能力（如MMLU +3.4；CMMLU +4.0），但我们观察到其在通用推理任务中带来了更显著的增益（如BBH +5.0；ARC-Challenge +3.7），并在代码与数学领域表现出突出优势（HumanEval +3.0；MATH +2.4）。机制分析表明，Engram使主干网络的早期层摆脱了静态信息重建的负担，从而有效“加深”了网络结构，提升了复杂推理能力。此外，通过将局部依赖关系交由查表处理，Engram释放了注意力机制的容量，使其能够聚焦于全局上下文，大幅增强了长序列检索性能（例如Multi-Query NIAH从84.2提升至97.0）。最后，Engram还建立了面向基础设施的高效架构：其确定性地址映射支持从主机内存中进行运行时预取，开销几乎可忽略不计。\n\n我们展望，**条件记忆**将成为下一代稀疏模型中不可或缺的核心建模范式。"
  },
  {
    "date": "2026-01-12",
    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
    "authors": "Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07226v1",
    "source": "arXiv",
    "abstract": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.",
    "title_zh": "在噪声中迷失：推理模型如何因上下文干扰而失效",
    "abstract_zh": "近年来，推理模型和代理型AI系统的进展导致对多样化外部信息的依赖日益增加。然而，这种转变引入了本质上具有噪声的输入上下文，而当前经过净化的基准测试无法反映这一现实。为此，我们提出了NoisyBench——一个全面的基准测试，系统性地评估模型在11个数据集上的鲁棒性，涵盖RAG、推理、对齐和工具使用任务，并针对多种噪声类型进行测试，包括随机文档、无关的聊天记录以及困难负样本干扰项。我们的评估发现，当面对上下文干扰项时，最先进模型的性能最高可下降80%。尤为重要的是，我们发现代理型工作流往往会放大这些错误，因为模型过度信任噪声工具输出；此外，干扰项甚至能在无恶意意图的情况下引发新型的对齐偏差。我们还发现，提示工程、上下文设计、监督微调（SFT）以及仅基于结果奖励的强化学习（RL）均无法确保模型的鲁棒性；相比之下，我们提出的“理性感知奖励”（Rationale-Aware Reward, RARE）通过激励模型识别噪声中的有用信息，显著增强了其抗干扰能力。最后，我们揭示了一种反向缩放趋势：在噪声环境下，测试时计算量越大，模型表现反而越差。通过注意力可视化分析，我们发现模型会不均衡地过度关注干扰项 token，为构建下一代具备强推理能力且高度鲁棒的智能体提供了关键洞见。"
  },
  {
    "date": "2026-01-12",
    "title": "Active Context Compression: Autonomous Memory Management in LLM Agents",
    "authors": "Nikhil Verma",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07190v1",
    "source": "arXiv",
    "abstract": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.",
    "title_zh": "主动上下文压缩：大型语言模型代理中的自主内存管理",
    "abstract_zh": "大型语言模型（LLM）代理在处理长周期软件工程任务时面临“上下文膨胀”问题。随着交互历史的积累，计算成本急剧上升，延迟增加，且由于被无关的历史错误分散注意力，推理能力下降。现有解决方案通常依赖于代理无法控制的被动外部摘要机制。本文提出Focus——一种受黏菌（Physarum polycephalum）生物探索策略启发的以代理为中心的架构。Focus代理能够自主决定何时将关键学习成果整合到持久的“知识”块中，并主动撤回（剪枝）原始交互历史。我们采用优化后的框架，遵循行业最佳实践（持久化bash + 字符串替换编辑器），在SWE-bench Lite的N=5个高上下文依赖实例上，使用Claude Haiku 4.5对Focus进行了评估。在鼓励频繁压缩的激进提示策略下，Focus实现了22.7%的令牌减少（从1490万降至1150万），同时保持相同的准确率（两个代理均为3/5 = 60%）。Focus平均每项任务执行6.0次自主压缩，单个实例的令牌节省最高达57%。本研究证明，当赋予适当工具和提示时，具备能力的模型可自主调节其上下文，为构建成本敏感的智能体系统开辟了新路径，且无需牺牲任务性能。"
  },
  {
    "date": "2026-01-12",
    "title": "PRPO: Aligning Process Reward with Outcome Reward in Policy Optimization",
    "authors": "Ruiyi Ding, Yongxuan Lv, Xianhui Meng, Jiahe Song, Chao Wang, Chen Jiang, Yuan Cheng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07182v1",
    "source": "arXiv",
    "abstract": "Policy optimization for large language models often suffers from sparse reward signals in multi-step reasoning tasks. Critic-free methods like GRPO assign a single normalized outcome reward to all tokens, providing limited guidance for intermediate reasoning . While Process Reward Models (PRMs) offer dense feedback, they risk premature collapse when used alone, as early low-reward tokens can drive policies toward truncated outputs. We introduce Process Relative Policy Optimization (PRPO), which combines outcome reliability with process-level guidance in a critic-free framework. PRPO segments reasoning sequences based on semantic clues, normalizes PRM scores into token-level advantages, and aligns their distribution with outcome advantages through location-parameter shift. On MATH500, PRPO improves Qwen2.5-Math-1.5B accuracy from 61.2% to 64.4% over GRPO using only eight rollouts and no value network, demonstrating efficient fine-grained credit assignment within critic-free optimization.",
    "title_zh": "PRPO：在策略优化中对齐过程奖励与结果奖励",
    "abstract_zh": "大型语言模型的策略优化在多步推理任务中常面临奖励信号稀疏的问题。无评判器方法（如GRPO）为所有标记分配单一归一化结果奖励，对中间推理过程提供的指导有限。尽管过程奖励模型（PRMs）能提供密集反馈，但单独使用时存在过早坍缩的风险——早期低奖励标记可能引导策略生成截断输出。为此，我们提出**过程相对策略优化**（Process Relative Policy Optimization, PRPO），在无评判器框架下结合结果可靠性与过程层面的指导。PRPO基于语义线索对推理序列进行分段，将PRM得分归一化为逐标记优势，并通过位置-参数变换使过程优势分布与结果优势对齐。在MATH500数据集上，PRPO仅用8次采样、无需价值网络，便将Qwen2.5-Math-1.5B的准确率从GRPO的61.2%提升至64.4%，展示了无评判器优化中高效精细的信用分配能力。"
  },
  {
    "date": "2026-01-12",
    "title": "Structured Reasoning for Large Language Models",
    "authors": "Jinyi Han, Zixiang Di, Zishang Jiang, Ying Liao, Jiaqing Liang, Yongqi Wang, Yanghua Xiao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07180v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) achieve strong performance by generating long chains of thought, but longer traces always introduce redundant or ineffective reasoning steps. One typical behavior is that they often perform unnecessary verification and revisions even if they have reached the correct answers. This limitation stems from the unstructured nature of reasoning trajectories and the lack of targeted supervision for critical reasoning abilities. To address this, we propose Structured Reasoning (SCR), a framework that decouples reasoning trajectories into explicit, evaluable, and trainable components. We mainly implement SCR using a Generate-Verify-Revise paradigm. Specifically, we construct structured training data and apply Dynamic Termination Supervision to guide the model in deciding when to terminate reasoning. To avoid interference between learning signals for different reasoning abilities, we adopt a progressive two-stage reinforcement learning strategy: the first stage targets initial generation and self-verification, and the second stage focuses on revision. Extensive experiments on three backbone models show that SCR substantially improves reasoning efficiency and self-verification. Besides, compared with existing reasoning paradigms, it reduces output token length by up to 50%.",
    "title_zh": "大型语言模型的结构化推理",
    "abstract_zh": "大型语言模型（LLMs）通过生成长链条的思维过程取得了优异的表现，但更长的推理轨迹往往会引入冗余或无效的推理步骤。一种典型现象是，即使模型已经得出正确答案，仍会进行不必要的验证和修改。这一局限性源于推理路径的非结构化特性，以及对关键推理能力缺乏针对性的监督。为解决该问题，我们提出了结构化推理（Structured Reasoning, SCR）框架，将推理轨迹解耦为显式、可评估且可训练的组件。我们主要通过“生成-验证-修订”范式实现SCR。具体而言，我们构建了结构化的训练数据，并采用动态终止监督机制，引导模型判断何时终止推理过程。为避免不同推理能力之间学习信号的相互干扰，我们采用渐进式的两阶段强化学习策略：第一阶段聚焦于初始生成与自我验证，第二阶段则专注于修订。在三种主干模型上的大量实验表明，SCR显著提升了推理效率与自我验证能力；此外，相较于现有推理范式，其输出的 token 长度最多可减少 50%。"
  },
  {
    "date": "2026-01-12",
    "title": "Large Language Models for Physics Instrument Design",
    "authors": "Sara Zoccheddu, Shah Rukh Qasim, Patrick Owen, Nicola Serra",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07580v1",
    "source": "arXiv",
    "abstract": "We study the use of large language models (LLMs) for physics instrument design and compare their performance to reinforcement learning (RL). Using only prompting, LLMs are given task constraints and summaries of prior high-scoring designs and propose complete detector configurations, which we evaluate with the same simulators and reward functions used in RL-based optimization. Although RL yields stronger final designs, we find that modern LLMs consistently generate valid, resource-aware, and physically meaningful configurations that draw on broad pretrained knowledge of detector design principles and particle--matter interactions, despite having no task-specific training. Based on this result, as a first step toward hybrid design workflows, we explore pairing the LLMs with a dedicated trust region optimizer, serving as a precursor to future pipelines in which LLMs propose and structure design hypotheses while RL performs reward-driven optimization. Based on these experiments, we argue that LLMs are well suited as meta-planners: they can design and orchestrate RL-based optimization studies, define search strategies, and coordinate multiple interacting components within a unified workflow. In doing so, they point toward automated, closed-loop instrument design in which much of the human effort required to structure and supervise optimization can be reduced.",
    "title_zh": "用于物理仪器设计的大语言模型",
    "abstract_zh": "我们研究了大型语言模型（LLMs）在物理仪器设计中的应用，并将其性能与强化学习（RL）进行了对比。仅通过提示（prompting），LLMs 在给定任务约束以及先前高分设计方案摘要的情况下，即可提出完整的探测器配置方案，这些方案使用与RL优化中相同的模拟器和奖励函数进行评估。尽管RL最终生成的设计表现更优，但我们发现现代LLMs始终能够生成有效、资源意识强且具有物理意义的配置，其设计思路基于对探测器设计原理和粒子-物质相互作用的广泛预训练知识，而无需任何特定任务的训练。基于这一发现，作为迈向混合设计流程的第一步，我们探索将LLMs与专用的信任区域优化器相结合，作为未来工作流的前导：在此类流程中，LLMs负责提出并构建设计假设，而RL则执行以奖励为导向的优化。基于这些实验，我们认为LLMs非常适合作为“元规划者”——它们能够设计并协调基于RL的优化研究，定义搜索策略，并在统一的工作流中整合多个相互关联的组件。通过这种方式，LLMs为实现自动化、闭环式的仪器设计指明了方向，大幅减少了人类在结构化和监督优化过程中所需投入的努力。"
  },
  {
    "date": "2026-01-12",
    "title": "JudgeFlow: Agentic Workflow Optimization via Block Judge",
    "authors": "Zihan Ma, Zhikai Zhao, Chuanbo Hua, Federico Berto, Jinkyoo Park",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07477v1",
    "source": "arXiv",
    "abstract": "Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose {\\our{}}, an Evaluation-Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic. On top of this abstraction, we design a dedicated Judge module that inspects execution traces -- particularly failed runs -- and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows. We evaluate {\\our{}} on mathematical reasoning and code generation benchmarks, where {\\our{}} achieves superior performance and efficiency compared to existing methods. The source code is publicly available at https://github.com/ma-zihan/JudgeFlow.",
    "title_zh": "JudgeFlow：通过块判别实现智能工作流优化",
    "abstract_zh": "优化基于大语言模型（LLM）的智能体工作流在扩展AI能力方面面临诸多挑战。当前方法依赖于粗粒度的端到端评估信号，缺乏对改进方向的细粒度指示，常常导致修改效率低下或影响有限。为解决这些局限性，我们提出 {\\our{}}——一种“评估-评判-优化-更新”（Evaluation-Judge-Optimization-Update）的流水线框架。我们将可复用、可配置的逻辑模块引入智能体工作流中，以捕捉基本的逻辑形式。在此抽象基础上，我们设计了一个专用的评判模块，该模块分析执行轨迹（尤其是失败的运行），并为存在问题的逻辑块分配基于排名的责任评分。这些细粒度的诊断信号随后被一个基于LLM的优化器利用，使其能够聚焦于工作流中问题最严重的模块进行针对性改进。我们的方法显著提升了样本效率，通过模块级别的诊断增强了可解释性，并为自动化复杂度不断提升的智能体工作流提供了可扩展的基础。我们在数学推理和代码生成基准上对 {\\our{}} 进行了评估，结果表明其性能和效率均优于现有方法。源代码已公开，地址为 https://github.com/ma-zihan/JudgeFlow。"
  },
  {
    "date": "2026-01-12",
    "title": "Semantic Compression of LLM Instructions via Symbolic Metalanguages",
    "authors": "Ernst van Gassen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07354v1",
    "source": "arXiv",
    "abstract": "We introduce MetaGlyph, a symbolic language for compressing prompts by encoding instructions as mathematical symbols rather than prose. Unlike systems requiring explicit decoding rules, MetaGlyph uses symbols like $\\in$ (membership) and $\\Rightarrow$ (implication) that models already understand from their training data. We test whether these symbols work as ''instruction shortcuts'' that models can interpret without additional teaching. We evaluate eight models across two dimensions relevant to practitioners: scale (3B-1T parameters) and accessibility (open-source for local deployment vs. proprietary APIs). MetaGlyph achieves 62-81% token reduction across all task types. For API-based deployments, this translates directly to cost savings; for local deployments, it reduces latency and memory pressure. Results vary by model. Gemini 2.5 Flash achieves 75% semantic equivalence between symbolic and prose instructions on selection tasks, with 49.9% membership operator fidelity. Kimi K2 reaches 98.1% fidelity for implication ($\\Rightarrow$) and achieves perfect (100%) accuracy on selection tasks with symbolic prompts. GPT-5.2 Chat shows the highest membership fidelity observed (91.3%), though with variable parse success across task types. Claude Haiku 4.5 achieves 100% parse success with 26% membership fidelity. Among mid-sized models, Qwen 2.5 7B shows 62% equivalence on extraction tasks. Mid-sized open-source models (7B-12B) show near-zero operator fidelity, suggesting a U-shaped relationship where sufficient scale overcomes instruction-tuning biases.",
    "title_zh": "通过符号元语言对大模型指令进行语义压缩",
    "abstract_zh": "我们提出MetaGlyph，一种符号化语言，通过将指令编码为数学符号而非文字描述来压缩提示。与需要显式解码规则的系统不同，MetaGlyph使用$\\in$（隶属关系）和$\\Rightarrow$（蕴含）等符号，这些符号模型在训练数据中已自然习得。我们测试这些符号是否能作为“指令捷径”，使模型无需额外教学即可理解。我们在两个对实践者至关重要的维度上评估了八种模型：规模（30亿至1万亿参数）和可访问性（开源以支持本地部署 vs. 专有API）。MetaGlyph在所有任务类型中实现了62%-81%的令牌减少。对于基于API的部署，这直接转化为成本节约；对于本地部署，则降低了延迟和内存压力。结果因模型而异：Gemini 2.5 Flash在选择任务中实现75%的语义等价性，隶属运算符（$\\in$）的保真度达49.9%；Kimi K2对蕴含运算符（$\\Rightarrow$）的保真度高达98.1%，并使用符号提示在选择任务中达到完美的100%准确率；GPT-5.2 Chat表现出最高的隶属关系保真度（91.3%），但不同任务类型的解析成功率波动较大；Claude Haiku 4.5实现了100%的解析成功率，隶属关系保真度为26%。在中等规模模型中，Qwen 2.5 7B在抽取任务中达到62%的等价性。中等规模开源模型（7B-12B）的运算符保真度接近零，表明存在一种U型关系——足够的模型规模能够克服指令调优带来的偏差。"
  },
  {
    "date": "2026-01-12",
    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
    "authors": "Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, Yi Xu, Huacan Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07348v1",
    "source": "arXiv",
    "abstract": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks.To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels.Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.",
    "title_zh": "算法代码优化的受控自进化",
    "abstract_zh": "自演化方法通过迭代的“生成-验证-优化”循环来提升代码生成能力，但现有方法存在探索效率低下的问题，在有限预算下难以发现复杂度更优的解决方案。这种低效性主要源于：初始化偏差导致演化过程陷入次优解区域、随机操作缺乏反馈引导而失控，以及跨任务间经验利用不足。为解决这些瓶颈，我们提出了受控自演化（Controlled Self-Evolution, CSE），其包含三个核心组件：多样化规划初始化能够生成结构上差异显著的算法策略，以实现对解空间的广泛覆盖；遗传演化用反馈引导的机制替代随机操作，支持有针对性的变异和组合式交叉；层级化演化记忆则在跨任务与任务内两个层面捕捉成功与失败的经验。在EffiBench-X上的实验表明，CSE在多种LLM骨干模型上均持续优于所有基线方法。此外，CSE从早期生成阶段即展现出更高的效率，并在整个演化过程中保持持续改进。我们的代码已公开，地址为 https://github.com/QuantaAlpha/EvoControl。"
  },
  {
    "date": "2026-01-12",
    "title": "Is Agentic RAG worth it? An experimental comparison of RAG approaches",
    "authors": "Pietro Ferrazzi, Milica Cvjeticanin, Alessio Piraccini, Davide Giannuzzi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07711v1",
    "source": "arXiv",
    "abstract": "Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of \"Enhanced\" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as \"Agentic\" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.",
    "title_zh": "代理型RAG值不值得？对RAG方法的实验性对比",
    "abstract_zh": "检索增强生成（RAG）系统通常由生成器与检索组件共同构成，其中检索组件从知识库中提取文本上下文以回答用户查询。然而，这类基础实现存在诸多局限性，包括检索结果噪声大或不理想、对超出范围的查询错误使用检索、查询与文档匹配能力弱，以及生成器在性能或成本上的波动性。这些缺陷促使了“增强型”RAG的发展，即引入专门模块来解决流程中的特定问题。最近，大型语言模型（LLMs）日益增强的自我反思能力催生了一种新范式，我们称之为“代理型”RAG。在此范式中，LLM负责协调整个流程——决定执行哪些操作、何时执行以及是否需要迭代，从而减少对固定且人工设计的模块的依赖。尽管这两种范式都得到了快速应用，但它们在何种条件下更具优势仍不明确。在本研究中，我们通过大量实证评估，在多种场景和维度下对比了增强型与代理型RAG。我们的研究结果为两种范式的权衡提供了实用洞见，有助于在实际应用中根据成本与性能综合考量，选择最合适的RAG设计方案。"
  },
  {
    "date": "2026-01-12",
    "title": "Towards Automating Blockchain Consensus Verification with IsabeLLM",
    "authors": "Elliot Jones, William Knottenbelt",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07654v1",
    "source": "arXiv",
    "abstract": "Consensus protocols are crucial for a blockchain system as they are what allow agreement between the system's nodes in a potentially adversarial environment. For this reason, it is paramount to ensure their correct design and implementation to prevent such adversaries from carrying out malicious behaviour. Formal verification allows us to ensure the correctness of such protocols, but requires high levels of effort and expertise to carry out and thus is often omitted in the development process. In this paper, we present IsabeLLM, a tool that integrates the proof assistant Isabelle with a Large Language Model to assist and automate proofs. We demonstrate the effectiveness of IsabeLLM by using it to develop a novel model of Bitcoin's Proof of Work consensus protocol and verify its correctness. We use the DeepSeek R1 API for this demonstration and found that we were able to generate correct proofs for each of the non-trivial lemmas present in the verification.",
    "title_zh": "使用IsabeLLM实现区块链共识验证的自动化",
    "abstract_zh": "共识协议对于区块链系统至关重要，因为它们能够在潜在的敌对环境中实现系统节点之间的共识。因此，确保其设计和实现的正确性极为重要，以防止攻击者实施恶意行为。形式化验证能够保证此类协议的正确性，但其执行需要高水平的努力和专业知识，因此在开发过程中常常被忽略。本文提出了一种名为 IsabeLLM 的工具，该工具将证明助手 Isabelle 与大型语言模型相结合，以辅助并自动化证明过程。我们通过使用 IsabeLLM 开发了一个关于比特币工作量证明（Proof of Work）共识协议的新模型，并验证了其正确性。在演示中，我们采用了 DeepSeek R1 API，结果表明，我们成功为验证过程中存在的每个非平凡引理生成了正确的证明。"
  },
  {
    "date": "2026-01-12",
    "title": "GRPO with State Mutations: Improving LLM-Based Hardware Test Plan Generation",
    "authors": "Dimple Vijay Kochar, Nathaniel Pinckney, Guan-Ting Liu, Chia-Tung Ho, Chenhui Deng, Haoxing Ren, Brucek Khailany",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07593v1",
    "source": "arXiv",
    "abstract": "RTL design often relies heavily on ad-hoc testbench creation early in the design cycle. While large language models (LLMs) show promise for RTL code generation, their ability to reason about hardware specifications and generate targeted test plans remains largely unexplored. We present the first systematic study of LLM reasoning capabilities for RTL verification stimuli generation, establishing a two-stage framework that decomposes test plan generation from testbench execution. Our benchmark reveals that state-of-the-art models, including DeepSeek-R1 and Claude-4.0-Sonnet, achieve only 15.7-21.7% success rates on generating stimuli that pass golden RTL designs. To improve LLM generated stimuli, we develop a comprehensive training methodology combining supervised fine-tuning with a novel reinforcement learning approach, GRPO with State Mutation (GRPO-SMu), which enhances exploration by varying input mutations. Our approach leverages a tree-based branching mutation strategy to construct training data comprising equivalent and mutated trees, moving beyond linear mutation approaches to provide rich learning signals. Training on this curated dataset, our 7B parameter model achieves a 33.3% golden test pass rate and a 13.9% mutation detection rate, representing a 17.6% absolute improvement over baseline and outperforming much larger general-purpose models. These results demonstrate that specialized training methodologies can significantly enhance LLM reasoning capabilities for hardware verification tasks, establishing a foundation for automated sub-unit testing in semiconductor design workflows.",
    "title_zh": "带有状态变异的GRPO：提升基于大模型的硬件测试计划生成",
    "abstract_zh": "RTL设计在早期开发阶段通常严重依赖于临时构建的测试平台。尽管大型语言模型（LLMs）在RTL代码生成方面展现出巨大潜力，但其在理解硬件规范并生成针对性测试计划方面的推理能力仍基本未被探索。本文首次系统性地研究了LLM在RTL验证激励生成中的推理能力，提出了一种两阶段框架，将测试计划生成与测试平台执行解耦。我们的基准测试表明，当前最先进的模型（包括DeepSeek-R1和Claude-4.0-Sonnet）在生成能够通过“黄金版”RTL设计的激励信号方面，成功率仅为15.7%至21.7%。为提升LLM生成的激励质量，我们开发了一套综合训练方法，结合监督微调与一种新颖的强化学习策略——基于状态变异的GRPO（GRPO-SMu），该方法通过改变输入变异来增强探索能力。我们的方法采用基于树结构的分支变异策略，构建包含等价树与变异树的训练数据集，突破了传统线性变异方式的局限，提供了更丰富的学习信号。在该精心筛选的数据集上进行训练后，我们的7B参数模型实现了33.3%的黄金测试通过率和13.9%的变异检测率，相比基线模型绝对提升了17.6%，且优于许多更大规模的通用型模型。这些结果表明，针对特定任务的训练方法可显著提升LLM在硬件验证任务中的推理能力，为半导体设计流程中自动化子单元测试奠定了坚实基础。"
  },
  {
    "date": "2026-01-12",
    "title": "SecureCAI: Injection-Resilient LLM Assistants for Cybersecurity Operations",
    "authors": "Mohammed Himayath Ali, Mohammed Aqib Abdullah, Mohammed Mudassir Uddin, Shahnawaz Alam",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07835v1",
    "source": "arXiv",
    "abstract": "Large Language Models have emerged as transformative tools for Security Operations Centers, enabling automated log analysis, phishing triage, and malware explanation; however, deployment in adversarial cybersecurity environments exposes critical vulnerabilities to prompt injection attacks where malicious instructions embedded in security artifacts manipulate model behavior. This paper introduces SecureCAI, a novel defense framework extending Constitutional AI principles with security-aware guardrails, adaptive constitution evolution, and Direct Preference Optimization for unlearning unsafe response patterns, addressing the unique challenges of high-stakes security contexts where traditional safety mechanisms prove insufficient against sophisticated adversarial manipulation. Experimental evaluation demonstrates that SecureCAI reduces attack success rates by 94.7% compared to baseline models while maintaining 95.1% accuracy on benign security analysis tasks, with the framework incorporating continuous red-teaming feedback loops enabling dynamic adaptation to emerging attack strategies and achieving constitution adherence scores exceeding 0.92 under sustained adversarial pressure, thereby establishing a foundation for trustworthy integration of language model capabilities into operational cybersecurity workflows and addressing a critical gap in current approaches to AI safety within adversarial domains.",
    "title_zh": "SecureCAI：面向网络安全运维的抗注入大语言模型助手",
    "abstract_zh": "大型语言模型已成为安全运营中心（SOC）中变革性的工具，能够实现日志的自动化分析、钓鱼邮件的快速分类以及恶意软件行为的解释；然而，在对抗性网络安全环境中部署时，这些模型容易受到提示注入攻击的威胁——攻击者通过在安全数据中嵌入恶意指令，操控模型的行为。本文提出SecureCAI，一种创新的防御框架，该框架在宪法式人工智能（Constitutional AI）原则的基础上，引入了具备安全感知能力的防护机制、自适应的宪法演化机制，以及基于直接偏好优化（Direct Preference Optimization）的“遗忘”机制，以消除不安全的响应模式，从而应对高风险安全场景下传统安全机制难以抵御复杂对抗性攻击的独特挑战。实验评估表明，与基线模型相比，SecureCAI将攻击成功率降低了94.7%，同时在良性安全分析任务上保持了95.1%的准确率。该框架还集成了持续红队测试反馈循环，可动态适应新兴攻击策略，在持续对抗压力下仍能维持超过0.92的宪法遵循度，为将语言模型能力可信地融入实际网络安全工作流程奠定了基础，有效填补了当前对抗性领域中人工智能安全方法的关键空白。"
  },
  {
    "date": "2026-01-12",
    "title": "Tensor Algebra Processing Primitives (TAPP): Towards a Standard for Tensor Operations",
    "authors": "Jan Brandejs, Niklas Hörnblad, Edward F. Valeev, Alexander Heinecke, Jeff Hammond, Devin Matthews, Paolo Bientinesi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07827v1",
    "source": "arXiv",
    "abstract": "To address the absence of a universal standard interface for tensor operations, we introduce the Tensor Algebra Processing Primitives (TAPP), a C-based interface designed to decouple the application layer from hardware-specific implementations. We provide a mathematical formulation of tensor contractions and a reference implementation to ensure correctness and facilitate the validation of optimized kernels. Developed through community consensus involving academic and industrial stakeholders, TAPP aims to enable performance portability and resolving dependency challenges. The viability of the standard is demonstrated through successful integrations with the TBLIS and cuTENSOR libraries, as well as the DIRAC quantum chemistry package.",
    "title_zh": "张量代数处理原语（TAPP）：迈向张量操作的标准",
    "abstract_zh": "为解决张量操作缺乏通用标准接口的问题，我们提出了张量代数处理原语（Tensor Algebra Processing Primitives, TAPP），这是一种基于C语言的接口设计，旨在将应用层与硬件特定实现解耦。我们提供了张量收缩的数学表达形式以及一个参考实现，以确保正确性，并便于优化内核的验证。TAPP通过学术界和产业界多方参与的共识共同开发，旨在实现性能可移植性并解决依赖性难题。该标准的有效性已通过与TBLIS和cuTENSOR库以及DIRAC量子化学软件包的成功集成得到验证。"
  },
  {
    "date": "2026-01-12",
    "title": "AscendKernelGen: A Systematic Study of LLM-Based Kernel Generation for Neural Processing Units",
    "authors": "Xinzi Cao, Jianyang Zhai, Pengfei Li, Zhiheng Hu, Cen Yan, Bingxu Mu, Guanghuan Fang, Bin She, Jiayu Li, Yihan Su, Dongyang Tao, Xiansong Huang, Fan Xu, Feidiao Yang, Yao Lu, Chang-Dong Wang, Yutong Lu, Weicheng Xue, Bin Zhou, Yonghong Tian",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07160v1",
    "source": "arXiv",
    "abstract": "To meet the ever-increasing demand for computational efficiency, Neural Processing Units (NPUs) have become critical in modern AI infrastructure. However, unlocking their full potential requires developing high-performance compute kernels using vendor-specific Domain-Specific Languages (DSLs), a task that demands deep hardware expertise and is labor-intensive. While Large Language Models (LLMs) have shown promise in general code generation, they struggle with the strict constraints and scarcity of training data in the NPU domain. Our preliminary study reveals that state-of-the-art general-purpose LLMs fail to generate functional complex kernels for Ascend NPUs, yielding a near-zero success rate. To address these challenges, we propose AscendKernelGen, a generation-evaluation integrated framework for NPU kernel development. We introduce Ascend-CoT, a high-quality dataset incorporating chain-of-thought reasoning derived from real-world kernel implementations, and KernelGen-LM, a domain-adaptive model trained via supervised fine-tuning and reinforcement learning with execution feedback. Furthermore, we design NPUKernelBench, a comprehensive benchmark for assessing compilation, correctness, and performance across varying complexity levels. Experimental results demonstrate that our approach significantly bridges the gap between general LLMs and hardware-specific coding. Specifically, the compilation success rate on complex Level-2 kernels improves from 0% to 95.5% (Pass@10), while functional correctness achieves 64.3% compared to the baseline's complete failure. These results highlight the critical role of domain-specific reasoning and rigorous evaluation in automating accelerator-aware code generation.",
    "title_zh": "AscendKernelGen：面向神经处理单元的基于大语言模型的内核生成系统性研究",
    "abstract_zh": "为满足日益增长的计算效率需求，神经网络处理单元（NPUs）已成为现代人工智能基础设施中的关键组件。然而，要充分发挥其全部潜力，需要使用厂商特定的领域专用语言（DSL）开发高性能计算内核，这一过程不仅要求深入的硬件知识，而且耗时费力。尽管大型语言模型（LLMs）在通用代码生成方面展现出巨大潜力，但在NPUs领域，由于严格的约束条件和训练数据稀缺，其表现仍不尽如人意。我们前期研究表明，当前最先进的通用大模型在生成适用于Ascend NPU的复杂内核时几乎完全失败，成功率为接近零。\n\n针对上述挑战，我们提出AscendKernelGen——一个集生成与评估于一体的NPU内核开发框架。我们构建了Ascend-CoT，一个高质量的数据集，其中包含从真实内核实现中提炼出的思维链（Chain-of-Thought）推理过程；同时开发了KernelGen-LM，一种通过监督微调及基于执行反馈的强化学习进行领域适配的模型。此外，我们设计了NPUKernelBench，一个全面的基准测试体系，用于评估不同复杂度级别下的编译成功率、正确性与性能表现。\n\n实验结果表明，我们的方法显著缩小了通用大模型与硬件特定编程之间的差距。具体而言，对于复杂度较高的Level-2内核，编译成功率从0%提升至95.5%（Pass@10），功能正确率也达到64.3%，而基线方法则完全失败。这些成果凸显了领域特定推理与严格评估在自动化加速器感知代码生成中的关键作用。"
  },
  {
    "date": "2026-01-12",
    "title": "Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory",
    "authors": "Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.07470v1",
    "source": "arXiv",
    "abstract": "Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.",
    "title_zh": "学习如何记忆：一种用于结构化和可迁移智能体记忆的元认知管理方法",
    "abstract_zh": "大型语言模型（LLM）代理在解决长期决策任务时，越来越依赖于累积的记忆。然而，现有的大多数方法将记忆存储在固定的表示形式中，并以单一或隐式的抽象层次上重复使用，这限制了模型的泛化能力，且在分布变化时常常导致负面迁移。本文提出了一种元认知记忆抽象方法（MCMA），将记忆抽象视为一种可学习的认知技能，而非固定的预设设计。MCMA通过结合一个冻结的任务模型与一个可学习的记忆协管员（memory copilot），实现了任务执行与记忆管理的解耦。该记忆协管员采用直接偏好优化进行训练，能够自主决定记忆应如何组织、抽象和重用。此外，记忆被进一步组织成多层级的抽象结构，支持根据任务相似性进行选择性重用。当缺乏可转移的记忆时，MCMA则通过传递记忆协管员来转移抽象与管理记忆的能力。在ALFWorld、ScienceWorld和BabyAI等多个基准上的实验表明，MCMA在性能、分布外泛化能力以及跨任务迁移方面均显著优于多个基线方法。"
  },
  {
    "date": "2026-1-12",
    "title": "Invisible Risks, Visible Code: A Vision for Understanding Ethical Debt in AI-Based Coding",
    "authors": "Dina Salah",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00074",
    "source": "IEEE",
    "abstract": "Background: AI-based coding assistants such as GitHub Copilot, ChatGPT, and Amazon CodeWhisperer are transforming software development by automating tasks, generating code, and supporting prototyping. While these tools enhance productivity, they also pose ethical risks, including copyright violations, insecure code, and ambiguous accountability. Such risks may remain hidden during development but emerge later as legal, social, or technical liabilities. Existing constructs, such as technical debt, are insufficient to capture these challenges. Aims: This paper introduces and formalizes the concept of ethical debt—the accumulation of unresolved ethical risks in AI assisted software development. The objective is to conceptualize ethical debt as a distinct construct, identify its causes and consequences, and establish a research agenda for addressing it. Method: Inspired by the metaphor of technical debt, the paper develops a framework for ethical debt that defines its sources, manifestations, and outcomes. The proposed research program involves theorizing its dimensions, mapping accountability across developers, AI systems, and organizations, and designing empirical approaches for its detection and mitigation. Results: Illustrative cases and a structured research agenda demonstrate how ethical risks, typically invisible in early development, can be systematically surfaced and addressed. Conclusions: By framing hidden ethical risks as a form of debt, this work provides both a theoretical foundation and a practical pathway for responsible AI-enhanced software engineering. Ethical debt makes intangible risks visible and actionable, offering guidance for researchers, practitioners, and organizations to integrate ethical responsibility into modern software development.",
    "title_zh": "无形的风险，可见的代码：理解基于人工智能编程中伦理债务的愿景",
    "abstract_zh": "背景：基于人工智能的代码辅助工具（如 GitHub Copilot、ChatGPT 和 Amazon CodeWhisperer）正在通过自动化任务、生成代码和辅助原型设计，深刻改变软件开发方式。尽管这些工具提升了开发效率，但也带来了版权侵权、代码安全隐患以及责任归属模糊等伦理风险。这些风险在开发过程中可能隐匿不显，但后期可能演变为法律、社会或技术层面的负担。现有的概念框架（如技术债务）难以充分涵盖此类挑战。\n\n目标：本文提出并形式化“伦理债务”这一概念——即在人工智能辅助的软件开发中，未解决的伦理风险的累积。研究旨在将伦理债务作为一个独立的理论构念进行概念化，识别其成因与后果，并建立相应的研究议程以应对该问题。\n\n方法：受“技术债务”隐喻的启发，本文构建了一个伦理债务的分析框架，明确其来源、表现形式及影响结果。所提出的科研计划包括：理论化伦理债务的多维特征，厘清开发者、AI系统与组织之间的责任分配关系，并设计实证方法以实现对伦理债务的检测与缓解。\n\n结果：通过典型案例分析与结构化的研究议程，本文展示了如何系统性地揭示并应对那些在早期开发阶段通常不可见的伦理风险。\n\n结论：将隐蔽的伦理风险视为一种“债务”，不仅为负责任的人工智能增强型软件工程提供了理论基础，也开辟了切实可行的实践路径。伦理债务使原本无形的风险变得可见且可操作，为研究人员、从业者及组织在现代软件开发中融入伦理责任提供了重要指引。"
  },
  {
    "date": "2026-1-12",
    "title": "Mapping Code Smells and Refactorings Accurately: Insights from an Empirical Study",
    "authors": "Gautam Shetty, Tushar Sharma",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00044",
    "source": "IEEE",
    "abstract": "Background: Code smells indicate underlying quality issues that negatively impact software maintainability. Refactoring is a common way to improve code quality by restructuring it, often removing these code smells. While many recommendations exist on how to refactor code smells, we do not fully understand how developers how they are removed by developers in the real world. Aim: In this study, we aim to investigate the evolution of code smells and the impact of applied refactoring techniques. Method: Our study addresses this gap by investigating both implementation and design smells and the refactoring techniques developers use to remove them. We also explore how often code smells are removed using established refactoring techniques. We analyzed 212,664 commits from 87 open-source Java projects using both automated tools and manual review to understand the relationship between code smells and refactoring. Results: Our key findings include: a) Extract method refactoring is most effective at fixing multiple smell types, b) Most applied refactorings do not remove code smells, c) About 82% of removed code smells are “dangling” i.e., they are removed without a matching refactoring technique, and d) Design smells typically last longer in codebases than implementation smells. Conclusions: This research improves our understanding of the interplay between code smells and refactoring effectiveness. Our results can help researchers develop better tools and guide software engineers in making their refactoring processes more efficient.",
    "title_zh": "准确映射代码异味与重构方法：来自一项实证研究的洞察",
    "abstract_zh": "背景：代码异味（code smells）反映了软件质量的潜在问题，这些问题是影响代码可维护性的负面因素。重构是改善代码质量的常见方法，通过重新组织代码结构来消除这些代码异味。尽管已有大量关于如何重构代码异味的建议，但我们对开发者在实际开发中如何移除这些异味仍缺乏充分理解。\n\n目标：本研究旨在探究代码异味的演化过程及其所应用重构技术的影响。\n\n方法：为弥补这一知识空白，本研究同时考察了实现层面和设计层面的代码异味，以及开发者用于消除它们的重构技术。我们还分析了代码异味被使用既定重构技术移除的频率。通过结合自动化工具与人工审查，对87个开源Java项目的212,664次提交进行了分析，以揭示代码异味与重构之间的关系。\n\n结果：主要发现包括：a) 提取方法（Extract Method）重构在修复多种异味类型方面最为有效；b) 大多数已应用的重构并未真正消除代码异味；c) 约82%被移除的代码异味属于“孤立”型，即在没有对应重构技术的情况下被移除；d) 与实现层面的异味相比，设计层面的异味通常在代码库中持续时间更长。\n\n结论：本研究深化了我们对代码异味与重构有效性之间相互作用的理解。研究结果有助于研究人员开发更高效的工具，并指导软件工程师优化其重构实践，从而提升重构效率。"
  },
  {
    "date": "2026-1-12",
    "title": "How do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?",
    "authors": "Shamse Tasnim Cynthia, Nuri Almarimi, Banani Roy",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00072",
    "source": "IEEE",
    "abstract": "Background: Community smells reflect poor organizational practices that often lead to socio-technical issues and the accumulation of Self-Admitted Technical Debt (SATD). While prior studies have explored these problems in general software systems, their interplay in machine learning (ML)-based projects remains largely under-examined. Aims: In this study, we aim to investigate the prevalence of community smells and their relationship with SATD in open-source ML projects, analyzing data at the release level. Methods: We analyzed <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{1 5 5 ~ M L}$</tex>-based systems across multiple releases to examine the prevalence of ten community smell types. Then we detected SATD at the release level and applied statistical analysis to examine its correlation with community smells. Next, we considered the six identified types of SATD to determine which community smells are the most associated with each debt category. Finally, we analyzed how the community smells and SATD evolve over the releases, uncovering project size-dependent trends and shared trajectories. Results: Community smells are found to be widespread, exhibiting distinct distribution patterns across small, medium and large projects. Certain smells, such as Radio Silence and Organizational Silos, are strongly correlated with higher SATD occurrences, while authority- and communication-related smells often co-occur with persistent code and design debt. Temporal analysis revealed shared evolutionary trajectories of smells and SATD, influenced by project size. Conclusion: Our findings emphasize the importance of early detection and mitigation of socio-technical issues to maintain the long-term quality and sustainability of ML-based systems.",
    "title_zh": "社区异味如何影响机器学习项目中的自认技术债？",
    "abstract_zh": "背景：社区异味反映了不良的组织实践，这些实践常常导致社会技术问题，并积累自认技术债务（SATD）。尽管以往的研究已探讨了通用软件系统中的这些问题，但其在基于机器学习（ML）项目中的相互作用仍鲜有深入研究。  \n目的：本研究旨在调查开源机器学习项目中社区异味的普遍性及其与SATD的关系，分析数据以发布版本为单位。  \n方法：我们分析了155个基于ML的系统在多个发布版本中的情况，考察十种社区异味类型的普遍性；随后在发布级别检测SATD，并通过统计分析检验其与社区异味的相关性；接着，针对六类已识别的SATD，确定每类债务最相关的社区异味类型；最后，分析社区异味与SATD随发布时间演化的趋势，揭示受项目规模影响的模式及共同演变轨迹。  \n结果：社区异味普遍存在，且在小型、中型和大型项目中表现出不同的分布特征。某些异味（如“静默期”和“组织孤岛”）与更高的SATD发生率显著相关，而与权威和沟通相关的异味则常与持续存在的代码和设计债务共现。时间序列分析揭示了异味与SATD具有共享的演化路径，且该路径受项目规模的影响。  \n结论：我们的研究结果强调了早期发现并缓解社会技术问题的重要性，这对于保障基于机器学习系统的长期质量与可持续性至关重要。"
  },
  {
    "date": "2026-1-12",
    "title": "Integrating Big Data Analytics and Devsecops: Adaptive LLM-Based Workflow for Resilient Multi-Cloud Environments",
    "authors": "Ramesh Somayajula",
    "publish": "2025 20th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP)",
    "url": "https://doi.org/10.1109/isai-nlp66160.2025.11320786",
    "source": "IEEE",
    "abstract": "The convergence of Big Data analytics and DevSecOps practices is reshaping the way enterprises design, secure, and scale cloud-native systems. Multi-cloud deployments, while offering flexibility and redundancy, introduce new challenges in resilience, security, and compliance. Traditional DevSecOps pipelines are limited in their ability to adapt to dynamic threats and complex cross-cloud environments. To address this gap, we propose an adaptive workflow that integrates Large Language Models (LLMs) into DevSecOps pipelines, enabling real-time analysis of logs, anomaly detection, automated policy enforcement, and resilience-driven orchestration. Our framework leverages Big Data analytics to process high-velocity telemetry streams while LLM-based agents provide contextual recommendations, automated remediation, and compliance-aware adaptations across AWS, Azure, and GCP. Experimental evaluation demonstrates that the proposed system improves threat detection accuracy by 18%, reduces remediation latency by 27%, and enhances cross-cloud resilience under failure scenarios. This work establishes a foundation for resilient, AI-driven DevSecOps pipelines capable of sustaining security and compliance in complex multi-cloud ecosystems.",
    "title_zh": "大数据分析与DevSecOps的融合：基于自适应大语言模型的弹性多云环境工作流",
    "abstract_zh": "大数据分析与DevSecOps实践的融合正在重塑企业设计、保障和扩展云原生系统的方式。多云部署虽然提供了灵活性和冗余性，但也带来了韧性、安全性和合规性方面的新挑战。传统的DevSecOps流水线在应对动态威胁和复杂的跨云环境时能力有限。为弥补这一差距，我们提出一种自适应工作流，将大型语言模型（LLMs）集成到DevSecOps流水线中，实现日志的实时分析、异常检测、自动化策略执行以及基于韧性的编排。我们的框架利用大数据分析技术处理高速度的遥测数据流，同时基于LLM的智能代理提供上下文相关的建议、自动修复措施以及在AWS、Azure和GCP之间的合规感知适应能力。实验评估表明，所提出的系统将威胁检测准确率提升了18%，修复延迟降低了27%，并在故障场景下显著增强了跨云环境的韧性。本研究为构建具备高韧性、人工智能驱动的DevSecOps流水线奠定了基础，使其能够在复杂的多云生态系统中持续保障安全与合规。"
  },
  {
    "date": "2026-1-12",
    "title": "Large Language Models for Software Fault Localization: a Survey",
    "authors": "Xiaohui Chang, Dongcheng Li, W. Eric Wong",
    "publish": "2025 12th International Conference on Dependable Systems and Their Applications (DSA)",
    "url": "https://doi.org/10.1109/dsa66321.2025.00024",
    "source": "IEEE",
    "abstract": "This survey reviews 36 peer-reviewed studies (2021-2025) on Large Language Model (LLM)-based Fault Localization (FL) across encoder-only, encoder-decoder, and decoder-only paradigms. We propose a taxonomy of LLMFL methods, analyze integration strategies, and benchmark against Spectrum-Based Fault Localization (SBFL), MutationBased Fault Localization (MBFL), and learning-driven baselines on Defects4J, ConDefects, and Codeflaws. Our analysis indicates that encoder-only models deliver strong statementlevel ranking quality-achieving lower mean average rank and first-rank position, and higher top-k accuracy-while encoder-decoder architectures excel at line-level localization and repair integration. Decoder-only models demonstrate superior cross-language adaptability and interpretability. We further examine how datasets and metrics shape evaluation, highlighting risks of data leakage and metric inconsistency. Finally, we outline open challenges-long-context handling, generalization, explainability, and reproducibility-and sketch research directions toward robust, industrial-grade LLM-FL.",
    "title_zh": "大型语言模型在软件故障定位中的应用：一项综述",
    "abstract_zh": "本调查综述了2021至2025年间36篇经过同行评审的研究，涵盖基于大语言模型（LLM）的故障定位（FL）方法，涉及仅编码器、编码器-解码器以及仅解码器三种范式。我们提出了一个LLMFL方法的分类体系，分析了不同集成策略，并在Defects4J、ConDefects和Codeflaws数据集上对基于谱的故障定位（SBFL）、基于变异的故障定位（MBFL）以及学习驱动型基线方法进行了基准测试。分析结果表明，仅编码器模型在语句级定位方面表现优异，实现了更低的平均排名（mean average rank）和首项排名位置，同时具有更高的top-k准确率；而编码器-解码器架构在行级定位及修复建议集成方面更具优势；仅解码器模型则展现出更优的跨语言适应性与可解释性。我们进一步探讨了数据集与评估指标如何影响实验评价，指出了数据泄露和度量不一致等潜在风险。最后，我们总结了当前面临的关键挑战——长上下文处理、泛化能力、可解释性与可复现性，并勾勒出迈向稳健、工业级LLM故障定位技术的研究方向。"
  },
  {
    "date": "2026-1-12",
    "title": "Understanding Everything as Code: A Taxonomy and Conceptual Model",
    "authors": "Haoran Wei, Nazim Madhavji, John Steinbacher",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00064",
    "source": "IEEE",
    "abstract": "Background: Everything as Code (EaC) is an emerging paradigm aiming to codify all aspects of modern software systems. Despite its growing popularity, comprehensive industry standards and peer-reviewed research clarifying its scope and guiding its adoption remain scarce. Aims: This study systematically analyzes existing knowledge and perceptions of EaC, clarifies its scope and boundaries, and provides structured guidance for researchers and practitioners. Method: We conducted a largescale multivocal literature review (MLR), synthesizing academic and grey literature sources. Findings were analyzed quantitatively and thematically. Based on this analysis, we developed a taxonomy and conceptual model of EaC, validated through collaboration with industry experts. Results: The resulting taxonomy comprises <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{2 5}$</tex> distinct EaC practices organized into six layers based on industry awareness and functional roles. The conceptual model illustrates focus areas, overlaps, and interactions among these EaC practices within the software delivery lifecycle. Additionally, practical code examples demonstrating the implementation of these practices were developed in collaboration with industry experts. Conclusions: This work addresses the current scarcity of academic discourse on EaC by providing the first comprehensive taxonomy and conceptual model. These contributions enhance conceptual clarity, offer actionable guidance to practitioners, and lay the groundwork for future research in this emerging domain.",
    "title_zh": "将一切理解为代码：一种分类体系与概念模型",
    "abstract_zh": "背景：一切即代码（Everything as Code，简称EaC）是一种新兴范式，旨在将现代软件系统的各个方面全部编码化。尽管该理念日益流行，但目前仍缺乏全面的行业标准和经过同行评审的研究来明确其范围并指导其应用。  \n目标：本研究系统分析现有关于EaC的知识与认知，厘清其边界与内涵，并为研究人员和实践者提供结构化的指导。  \n方法：我们开展了一项大规模多源文献综述（Multivocal Literature Review, MLR），综合了学术文献与灰色文献资源。通过定量与定性相结合的方式对发现进行分析。基于此分析结果，我们构建了一个EaC的分类体系与概念模型，并通过与行业专家协作进行了验证。  \n成果：所提出的分类体系包含25种不同的EaC实践，按行业认知度与功能角色划分为六个层级。概念模型展示了这些EaC实践在软件交付生命周期中的关注领域、相互重叠及交互关系。此外，我们还与行业专家合作，开发了若干实用的代码示例，以展示这些实践的具体实现方式。  \n结论：本研究填补了当前EaC领域学术讨论匮乏的空白，首次提供了全面的分类体系与概念模型。这些成果提升了概念清晰度，为从业者提供了可操作的指导，并为该新兴领域的未来研究奠定了基础。"
  },
  {
    "date": "2026-1-12",
    "title": "Exploring the Jupyter Ecosystem: An Empirical Study of Bugs and Vulnerabilities",
    "authors": "Wenyuan Jiang, Diany Pressato, Harsh Darji, Thibaud Lutellier",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00052",
    "source": "IEEE",
    "abstract": "Background. Jupyter notebooks are one of the main tools used by data scientists. Notebooks include features (configuration scripts, markdown, images, etc.) that make them challenging to analyze compared to traditional software. As a result, existing software engineering models, tools, and studies do not capture the uniqueness of Notebook's behavior. Aims. This paper aims to provide a large-scale empirical study of bugs and vulnerabilities in the Notebook ecosystem. Method. We collected and analyzed a large dataset of Notebooks from two major platforms. Our methodology involved quantitative analyses of notebook characteristics (such as complexity metrics, contributor activity, and documentation) to identify factors correlated with bugs. Additionally, we conducted a qualitative study using grounded theory to categorize notebook bugs, resulting in a comprehensive bug taxonomy. Finally, we analyzed security-related commits and vulnerability reports to assess risks associated with Notebook deployment frameworks. Results. Our findings highlight that configuration issues are among the most common bugs in notebook documents, followed by incorrect API usage. Finally, we explore common vulnerabilities associated with popular deployment frameworks to better understand risks associated with Notebook development. Conclusions. This work highlights that notebooks are less wellsupported than traditional software, resulting in more complex code, misconfiguration, and poor maintenance.",
    "title_zh": "探索 Jupyter 生态系统：一项关于漏洞与安全缺陷的实证研究",
    "abstract_zh": "背景。Jupyter笔记本是数据科学家常用的主要工具之一。笔记本包含配置脚本、Markdown、图像等多种功能，相较于传统软件，其分析难度更高。因此，现有的软件工程模型、工具和研究无法充分反映笔记本的独特行为特征。\n\n目标。本文旨在对笔记本生态系统中的缺陷与安全漏洞进行大规模实证研究。\n\n方法。我们从两个主要平台收集并分析了大量笔记本数据集。研究方法包括对笔记本特征（如复杂度指标、贡献者活动、文档质量等）的定量分析，以识别与缺陷相关的因素。此外，我们采用扎根理论进行了定性研究，对笔记本缺陷进行分类，构建了一个全面的缺陷分类体系。最后，我们分析了与安全相关的提交记录和漏洞报告，评估了笔记本部署框架所面临的风险。\n\n结果。研究发现，配置问题是最常见的笔记本文档缺陷，其次是API使用错误。我们还探讨了常见部署框架中与笔记本相关的典型安全漏洞，以更深入理解笔记本开发中的潜在风险。\n\n结论。本研究表明，与传统软件相比，笔记本的支持程度较低，导致代码更加复杂、配置错误频发以及维护状况不佳。"
  },
  {
    "date": "2026-1-12",
    "title": "Research and Practice of Embedded AI-Driven Machine Vision Under the RISC-V Open Source Framework",
    "authors": "LinFang Liu, JingYu Gong",
    "publish": "2025 2nd International Symposium on IoT and Intelligent Robotics (IoTIR)",
    "url": "https://doi.org/10.1109/iotir66925.2025.00032",
    "source": "IEEE",
    "abstract": "This paper focuses on the research of key technologies for embedded artificial intelligence oriented towards machine vision, conducting a series of studies from key technology analysis, system architecture to application practice. The hardware is based on domestic RISC- V chips, with extended cameras and LCD screens as peripheral devices. Combined with the basic principles of convolutional neural networks and leveraging model lightweighting techniques, a complete embedded AI application development system was ultimately implemented. The system supports the full-process one-stop service of data collection and annotation, model training and testing, component generation, and inference deployment. Using digit recognition as an example, the practical value of the system was verified.",
    "title_zh": "RISC-V开源框架下嵌入式AI驱动机器视觉的研究与实践",
    "abstract_zh": "本文聚焦于面向机器视觉的嵌入式人工智能关键技术研究，从关键技术分析、系统架构到应用实践开展了一系列研究。硬件基于国产RISC-V芯片，外接摄像头和LCD显示屏作为外围设备。结合卷积神经网络的基本原理，运用模型轻量化技术，最终实现了一个完整的嵌入式AI应用开发系统。该系统支持数据采集与标注、模型训练与测试、组件生成以及推理部署的全流程一站式服务。以数字识别为例，验证了系统的实际应用价值。"
  },
  {
    "date": "2026-1-12",
    "title": "Rethinking Code Review Workflows with LLM Assistance: An Empirical Study",
    "authors": "Fannar Steinn Aðalsteinsson, Björn Borgar Magnússon, Mislav Milicevic, Adam Nirving Davidsson, Chih-Hong Cheng",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00013",
    "source": "IEEE",
    "abstract": "Background: Code reviews are a critical yet timeconsuming aspect of modern software development, increasingly challenged by growing system complexity and the demand for faster delivery. Aims: We examine how large language models (LLMs) can support code reviews by addressing common inefficiencies and contextual gaps. Method: At WirelessCar Sweden <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$A B$</tex>, we conducted an exploratory field study to identify current challenges, followed by a field experiment with two LLM-assisted review prototypes: one providing upfront, AIgenerated reviews and another enabling on-demand interaction. Both used a retrieval-augmented generation pipeline to assemble relevant contextual information. Results: The field study revealed frequent context switching, insufficient contextual information, and concerns around false positives. In practice, developers generally preferred the AI-led approach, especially for large or unfamiliar pull requests, though preferences varied with codebase familiarity and review risk. Conclusions: LLM-assisted reviews can reduce cognitive load and improve comprehension, with hybrid proactive/on-demand designs best balancing efficiency, trust, and reviewer control.",
    "title_zh": "利用大语言模型辅助重新思考代码审查工作流程：一项实证研究",
    "abstract_zh": "背景：代码审查是现代软件开发中至关重要但耗时的环节，随着系统复杂性的增加以及对更快交付的需求，这一过程正面临日益严峻的挑战。  \n目标：本文探讨大型语言模型（LLMs）如何通过解决常见的低效问题和上下文缺失，来支持代码审查。  \n方法：在瑞典无线汽车公司（WirelessCar Sweden）A B，我们开展了一项探索性实地研究，以识别当前面临的挑战，随后进行了一项实地实验，测试两种基于LLM的审查原型：一种提供预先生成的AI审查意见，另一种支持按需交互式审查。两种原型均采用检索增强生成（RAG）管道，整合相关上下文信息。  \n结果：实地研究揭示了频繁的上下文切换、上下文信息不足以及对误报的担忧。实际使用中，开发者普遍更倾向于由AI主导的审查方式，尤其是在处理大型或不熟悉的代码提交时；然而，具体偏好会因对代码库的熟悉程度和审查风险而异。  \n结论：基于LLM的代码审查能够降低认知负担并提升理解效率，混合型的主动式/按需式设计在效率、信任度与审查者控制权之间实现了最佳平衡。"
  },
  {
    "date": "2026-1-12",
    "title": "Demystifying Small Language Models (SLMs): The Future of Generative Artificial Intelligence",
    "authors": "G. Pradeep Reddy, Y. V. Pavan Kumar, K. Purna Prakash, D. John Pradeep",
    "publish": "2025 7th International Symposium on Advanced Electrical and Communication Technologies (ISAECT)",
    "url": "https://doi.org/10.1109/isaect68904.2025.11318707",
    "source": "IEEE",
    "abstract": "Small Language Models (SLMs) are emerging as essential in artificial intelligence (AI), particularly in generative AI and natural language processing (NLP), with optimum efficiency, accessibility, and performance. In generative AI, these models play a crucial role by enabling the creation of high-quality text, code, and content creation (viz., poetry, stories, etc.). While Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like language, they are often limited by high computational costs, significant memory necessities, and energy inefficiency. These limitations make LLMs impractical for deployment on edge devices or in low-resource environments, creating a demand for SLMs that provide similar functionalities with optimized efficiency and reduced resource consumption. Unlike LLMs, SLMs are designed to operate with lower computational requirements, fewer parameters, and minimal memory usage, making them ideal for deployment on resource-constrained devices, namely smartphones, IoT devices, and tiny embedded systems. Thus, this paper provides a beginner's guide to SLMs, exploring their core concepts, advantages, and applications.",
    "title_zh": "揭秘小型语言模型（SLMs）：生成式人工智能的未来",
    "abstract_zh": "小型语言模型（SLMs）正逐渐成为人工智能（AI）领域的重要组成部分，尤其在生成式AI和自然语言处理（NLP）中展现出卓越的效率、可及性和性能。在生成式AI中，这些模型发挥着关键作用，能够实现高质量文本、代码以及内容创作（如诗歌、故事等）的生成。尽管大型语言模型（LLMs）在生成类人语言方面表现出色，但其往往受限于高昂的计算成本、巨大的内存需求以及能源效率低下等问题。这些局限性使得LLMs难以在边缘设备或资源有限的环境中部署，从而催生了对小型语言模型（SLMs）的需求——它们能够在保持类似功能的同时，实现更高的效率并显著降低资源消耗。与LLMs不同，SLMs专为低计算需求、参数量少和内存占用小而设计，因此非常适合部署在资源受限的设备上，例如智能手机、物联网（IoT）设备以及微型嵌入式系统。本文旨在为初学者提供一份关于SLMs的入门指南，深入探讨其核心概念、优势及实际应用。"
  },
  {
    "date": "2026-1-12",
    "title": "Performance Estimation Using Both Source and Assembly Code for Data Layout Optimization on GPU",
    "authors": "Shintaro Yamamoto, Kazuhiko Ohno",
    "publish": "2025 Thirteenth International Symposium on Computing and Networking Workshops (CANDARW)",
    "url": "https://doi.org/10.1109/candarw68385.2025.00027",
    "source": "IEEE",
    "abstract": "In recent years, GPUs are widely used as highperformance computing platforms in various computing domains such as scientific computing and AI. Although GPUs have high computing power, architecture-aware optimizations are needed to exploit the performance. Optimizing data layout in the GPU memory is crucial among such optimizations because it directly affects the memory access efficiency which often dominates the GPU performance. However, current GPU programming requires low-level hand-optimizations thus automatic optimization is desired. In this paper, we propose a performance estimation method of GPU programs based on static code analysis. Estimating each execution time of the target program with a different data layout enables selecting the optimal data layout automatically. The proposed method analyzes not only the CUDA source code but also PTX code, which is compiled from the source code, to obtain the executed instructions. The method constructs a control flow graph and extracts memory access patterns from array index expressions. Then it estimates the execution time of the program using the instruction latency model of the GPU. As a result of an evaluation using 4 typical computation kernels such as matrix multiplication from PolyBench benchmark suite, the estimation of our method was enough to select the optimal data layout from AoS, SoA and ASTA.",
    "title_zh": "基于源代码和汇编代码的性能估算用于GPU数据布局优化",
    "abstract_zh": "近年来，GPU在科学计算和人工智能等各类计算领域被广泛用作高性能计算平台。尽管GPU具备强大的计算能力，但要充分发挥其性能仍需进行架构感知的优化。其中，优化GPU内存中的数据布局尤为关键，因为这直接影响内存访问效率，而内存访问效率往往是决定GPU性能的主要因素。然而，当前的GPU编程需要低层级的手动优化，因此亟需实现自动优化。本文提出一种基于静态代码分析的GPU程序性能估计算法。通过估算目标程序在不同数据布局下的执行时间，可实现最优数据布局的自动选择。所提方法不仅分析CUDA源代码，还分析由源代码编译生成的PTX代码，以获取实际执行的指令信息。该方法构建控制流图，并从数组索引表达式中提取内存访问模式，进而利用GPU的指令延迟模型估算程序的执行时间。通过对PolyBench基准测试套件中4个典型计算内核（如矩阵乘法）的评估结果表明，本方法的性能估计足以准确选出AoS、SoA和ASTA三种数据布局中的最优方案。"
  },
  {
    "date": "2026-1-12",
    "title": "Review of Large Language Model-Based Software Fault Localization Techniques",
    "authors": "Tian Pan, Pan Liu, Yihao Li",
    "publish": "2025 12th International Conference on Dependable Systems and Their Applications (DSA)",
    "url": "https://doi.org/10.1109/dsa66321.2025.00021",
    "source": "IEEE",
    "abstract": "Software fault localization (FL) is a critical step in debugging, aiming to identify the defective elements in source code with cost-effective effort. Recent advances in large language models (LLMs) have introduced new possibilities for automating and augmenting FL through code understanding, reasoning, and integration with dynamic analysis tools. This paper conducts a literature review of 21 studies on LLM-based FL. Specifically, reviewed studies are classified into five categories: (1) test-free/code-only localization, (2) agentic and tool-augmented frameworks, (3) crash/stack-trace-driven methods, (4) LLM-augmented information-retrieval-based FL, and (5) empirical analysis. For each category, we analyze methodological design choices, including the types of input leveraged (code, tests, logs, bug reports), localization granularity (file, method, statement, token), prompting strategies, fine-tuning vs. incontext learning, and ranking mechanisms. We also summarize datasets, evaluation metrics, and reported performance trends, drawing comparisons across LLMs. Finally, we outline open challenges and research directions for LLM-based FL. This survey provides a knowledge base for researchers and practitioners seeking to advance LLMdriven fault localization.",
    "title_zh": "基于大语言模型的软件故障定位技术综述",
    "abstract_zh": "软件故障定位（Fault Localization, FL）是调试过程中的关键步骤，旨在以低成本的代价识别源代码中的缺陷元素。近年来，大型语言模型（Large Language Models, LLMs）的发展为自动化和增强FL提供了新的可能性，通过代码理解、推理能力以及与动态分析工具的集成，显著提升了定位效率。本文对21项基于LLM的FL研究进行了系统性文献综述。具体而言，这些研究被划分为五类：（1）无测试/仅代码驱动的定位方法；（2）基于智能体（agentic）与工具增强的框架；（3）基于崩溃信息/堆栈跟踪驱动的方法；（4）基于LLM增强的信息检索型FL；（5）实证分析研究。\n\n针对每一类别，本文深入分析了其方法论设计选择，包括所利用的输入类型（如代码、测试用例、日志、错误报告）、定位粒度（文件级、方法级、语句级、词法单元级）、提示策略（prompting strategies）、微调（fine-tuning）与上下文学习（in-context learning）的比较，以及排序机制的设计。同时，本文总结了相关研究使用的数据集、评估指标及性能表现趋势，并对不同LLM在各类任务中的表现进行了横向对比。最后，文章指出了当前基于LLM的FL所面临的关键挑战，并提出了未来的研究方向。\n\n本综述为希望推进LLM驱动故障定位技术的研究人员和实践者提供了一个全面的知识基础。"
  },
  {
    "date": "2026-1-12",
    "title": "Research on Software Defect Detection and Repair Technology Based on Deep Learning",
    "authors": "Jiajia Li, Di An",
    "publish": "2025 20th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP)",
    "url": "https://doi.org/10.1109/isai-nlp66160.2025.11320738",
    "source": "IEEE",
    "abstract": "In response to the problems of module disconnection, insufficient positioning accuracy, and poor adaptability to safety critical areas in current software defect detection and repair, this paper proposes a deep learning based “pre-processing detection repair verification” closed-loop technology framework to improve the accuracy and engineering applicability of defect handling. In the preprocessing stage, D2U Flow slicing technology is used to extract line level code fragments associated with defects and eliminate redundant information; Design a CNN Transformer dual branch model during the detection phase, integrating local syntax features with global logical dependency modeling to synchronously achieve defect type classification and row level localization; Introducing a detection feedback mechanism during the repair phase, combined with sub word segmentation, replication, and coverage optimization, to generate semantically correct candidate patches; During the verification phase, effective patches are filtered through compilation checks and test case execution. The experiment was supported by the Defects4J benchmark dataset and the airborne software domain dataset. The results showed that the framework achieved a defect detection F1 score of 96.2 % and a repair success rate of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\text{1 0. 2 \\%}$</tex> on the Defects4J test set, while the F1 score detection and repair success rates on the airborne domain dataset were 93.8 % and 9.1 %, respectively; The performance improvement of the detection repair collaborative scheme compared to the non feedback scheme is <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$3.9-4.3$</tex> percentage points, which verifies the effectiveness and domain adaptability of the technology and provides a feasible solution for automated handling of software defects in safety critical areas.",
    "title_zh": "基于深度学习的软件缺陷检测与修复技术研究",
    "abstract_zh": "针对当前软件缺陷检测与修复中存在的模块断连、定位精度不足以及在安全关键领域适应性差等问题，本文提出了一种基于深度学习的“预处理—检测—修复—验证”闭环技术框架，以提升缺陷处理的准确性和工程适用性。在预处理阶段，采用D2U流切片技术提取与缺陷相关的行级代码片段，并剔除冗余信息；在检测阶段，设计了CNN-Transformer双分支模型，融合局部语法特征与全局逻辑依赖建模，实现缺陷类型分类与行级定位的同步完成；在修复阶段引入检测反馈机制，结合子词分段、复制及覆盖优化策略，生成语义正确的候选补丁；在验证阶段，通过编译检查和测试用例执行对有效补丁进行筛选。实验基于Defects4J基准数据集和航空软件领域数据集开展。结果表明，该框架在Defects4J测试集上实现了96.2%的缺陷检测F1值和10.2%的修复成功率；在航空领域数据集上，检测F1值为93.8%，修复成功率为9.1%；相较于无反馈机制的方案，检测与修复协同方案性能提升了3.9至4.3个百分点，验证了该技术的有效性与跨领域适应能力，为安全关键领域软件缺陷的自动化处理提供了可行解决方案。"
  },
  {
    "date": "2026-1-12",
    "title": "Software Reliability Testing Profile Generation for Industrial Robot Based on Large Language Model",
    "authors": "Liang Yan, Jingwei Shang",
    "publish": "2025 12th International Conference on Dependable Systems and Their Applications (DSA)",
    "url": "https://doi.org/10.1109/dsa66321.2025.00020",
    "source": "IEEE",
    "abstract": "As core equipment in smart manufacturing, the reliability of industrial robots directly impacts production line stability and product quality. This paper addresses the limitations of traditional software reliability test profiles in adequately capturing the motion/operational modes of industrial robots and in deeply characterizing the dynamic relationships between software and hardware layers. We propose a data- and AI large model-based framework for generating software reliability test profiles for industrial robots. This framework automatically extracts knowledge from industrial robot operational data and transforms it into executable, quantifiable software reliability test profiles, ultimately automating the generation of software reliability test cases. The method significantly enhances the ability to detect low-frequency, hidden temporal defects and logical errors, transforming the traditional testing paradigm that relies on expert experience. Application cases demonstrate the feasibility and effectiveness of the proposed method.",
    "title_zh": "基于大语言模型的工业机器人软件可靠性测试用例生成",
    "abstract_zh": "作为智能制造的核心设备，工业机器人的可靠性直接关系到生产线的稳定性与产品质量。本文针对传统软件可靠性测试用例在充分捕捉工业机器人运动/操作模式以及深入刻画软硬件层之间动态关系方面的局限性，提出了一种基于数据与人工智能大模型的工业机器人软件可靠性测试用例生成框架。该框架能够自动从工业机器人运行数据中提取知识，并将其转化为可执行、可量化的软件可靠性测试用例，最终实现软件可靠性测试用例的自动化生成。该方法显著提升了对低频、隐蔽时序缺陷及逻辑错误的检测能力，推动了传统依赖专家经验的测试模式向智能化、数据驱动的范式转变。应用案例验证了所提方法的可行性和有效性。"
  },
  {
    "date": "2026-1-12",
    "title": "An Analysis Methodology for Implicit Interfaces in Safety-Critical Embedded Software: Theory and Application",
    "authors": "Jiawei Ding, Xiaohong Bao",
    "publish": "2025 12th International Conference on Dependable Systems and Their Applications (DSA)",
    "url": "https://doi.org/10.1109/dsa66321.2025.00027",
    "source": "IEEE",
    "abstract": "As the complexity and scale of safety-critical embedded systems continue to grow, traditional interface specifications and verification methods are increasingly insufficient to cope with safety risks posed by implicit interfaces–interaction paths that are not explicitly declared yet exist in practice. This paper (i) introduces an operational definition of implicit interfaces and a multi-dimensional taxonomy, (ii) analyzes implicit interfaces from multiple perspectives and proposes safeguards spanning the entire software life cycle, and (iii) develops a static safety-analysis approach that combines LLVM intermediate representation (IR) with symbol-table reasoning for two representative mechanisms-shared-variable coupling and symbol binding-together with a prototype tool. Through an empirical study on three representative open-source embedded projects, the method efficiently and accurately uncovers implicit-interface risks that elude existing tools. Results show that our approach provides a scalable theoretical framework and practical tooling for assuring the safety and reliability of safety-critical embedded software in the presence of implicit interfaces.",
    "title_zh": "安全关键嵌入式软件中隐式接口的分析方法：理论与应用",
    "abstract_zh": "随着安全关键嵌入式系统复杂性和规模的持续增长，传统的接口规范和验证方法已越来越难以应对由隐式接口所带来的安全风险——这些接口是未被显式声明但在实际中却存在的交互路径。本文（i）提出了隐式接口的操作性定义及其多维度分类体系，（ii）从多个角度分析了隐式接口问题，并提出了贯穿整个软件生命周期的安全防护措施，（iii）开发了一种静态安全分析方法，该方法结合LLVM中间表示（IR）与符号表推理技术，针对两种典型机制——共享变量耦合与符号绑定——进行了分析，并实现了一个原型工具。通过对三个具有代表性的开源嵌入式项目进行实证研究，该方法能够高效且准确地发现现有工具难以识别的隐式接口风险。实验结果表明，本方法为在存在隐式接口的情况下，保障安全关键嵌入式软件的安全性与可靠性，提供了一个可扩展的理论框架和实用的工具支持。"
  },
  {
    "date": "2026-1-12",
    "title": "Preparing Future-Ready Software Engineers for Dependable, AI-Empowered Systems",
    "authors": "Mohammed Akour",
    "publish": "2025 12th International Conference on Dependable Systems and Their Applications (DSA)",
    "url": "https://doi.org/10.1109/dsa66321.2025.00011",
    "source": "IEEE",
    "abstract": "In a rapidly evolving engineering landscape where systems must not only function but also persist reliably, the imperative for dependable and trustworthy software has never been greater. This keynote explores how software engineering education must adapt to the growing demands for safe, resilient, and AI-augmented systems. In doing so, it outlines how academic programs can embed principles of dependability—such as fault-tolerance, resilience, maintainability, and verification—alongside modern practices driven by machine learning and deep learning. Through project-based learning, co-design with industry, and flexible credentialing, institutions can cultivate engineers who are proficient in code and skilled stewards of mission-critical, intelligent systems.",
    "title_zh": "培养面向未来的软件工程师，打造可靠、人工智能赋能的系统",
    "abstract_zh": "在工程领域快速演进的背景下，系统不仅需要正常运行，更需具备可靠的持续性，因此对可靠且值得信赖的软件需求比以往任何时候都更为迫切。本次主题演讲探讨了软件工程教育如何适应日益增长的安全性、韧性以及人工智能增强型系统的需求。在此过程中，演讲阐明了高校课程应如何将可靠性原则——如容错性、韧性、可维护性及验证——与由机器学习和深度学习驱动的现代实践相结合。通过项目制学习、与产业界的协同设计以及灵活的资质认证机制，教育机构能够培养出既精通代码编写，又善于管理关键任务智能系统的专业工程师。"
  },
  {
    "date": "2026-1-12",
    "title": "Performance Evaluation of CNN using RISC-V Vector Extension for Embedded System",
    "authors": "Manato Okawara, Kanemitsu Ootsu, Takuma Kitamoto, Takashi Yokota",
    "publish": "2025 Thirteenth International Symposium on Computing and Networking Workshops (CANDARW)",
    "url": "https://doi.org/10.1109/candarw68385.2025.00065",
    "source": "IEEE",
    "abstract": "In recent years, there has been a rapid increase in demand for deep learning inference in embedded systems such as autonomous vehicles and smart devices. In these embedded environments, the challenge is to achieve high-speed processing under constraints such as limited computing resources and power consumption. This paper investigates the performance of CNN inference processing using the RISC-V Vector Extension (RVV) on an embedded RISC-V processor. Considering the computational resource constraints of embedded systems, we implement CNN inference processing using fixed-point arithmetic. In the experiment, we analyze the effect of RVV on improving computational throughput and the effect of memory transfer performance in the overall processing by varying the parameters of vector register length, processor operating frequency, and memory configuration. As a result, we confirmed that performance improves by expanding the vector register length, while the effect of memory bandwidth is small. By comparing with scalar implementations in each processing layer, it was found that RVV vectorization is particularly effective for computationintensive operations such as addition and GEMM. It achieves up to approximately $23 \\times$ speedup, showing that the effect of parallelization varies significantly depending on the processing type.",
    "title_zh": "基于RISC-V向量扩展的CNN在嵌入式系统中的性能评估",
    "abstract_zh": "近年来，自动驾驶汽车和智能设备等嵌入式系统中对深度学习推理的需求迅速增长。在这些嵌入式环境中，挑战在于在计算资源和功耗受限的条件下实现高速处理。本文研究了在嵌入式RISC-V处理器上，利用RISC-V向量扩展（RVV）进行卷积神经网络（CNN）推理处理的性能表现。考虑到嵌入式系统的计算资源限制，我们采用定点数运算来实现CNN推理处理。实验中，通过改变向量寄存器长度、处理器工作频率和内存配置等参数，分析了RVV对提升计算吞吐量的影响，以及内存传输性能对整体处理效率的影响。结果表明，扩大向量寄存器长度可显著提升性能，而内存带宽的影响相对较小。与各处理层中的标量实现进行对比后发现，RVV向量化在计算密集型操作（如加法和GEMM）中尤为有效，最高可实现约23倍的加速，说明并行化效果因处理类型的不同而存在显著差异。"
  },
  {
    "date": "2026-1-12",
    "title": "Hardware Trojan Insertion and Detection in Chiplet-Based Architectures",
    "authors": "Zeyad Rashed, Peter Abdelmalk, Omar Mohamed, Mark Mikhaeil, Hassan Nassar, Jörg Henkel, Mohamed Abd El Ghany",
    "publish": "2025 37th International Conference on Microelectronics (ICM)",
    "url": "https://doi.org/10.1109/icm66518.2025.11322457",
    "source": "IEEE",
    "abstract": "The rise of chiplet-based system-on-chip (SoC) architectures enables unprecedented modularity and scalability across modern computing platforms. Chiplets have become essential in addressing the growing performance demands of both highperformance and embedded systems. However, this wide adoption introduces new attack surfaces, as heterogeneous chiplets sourced from untrusted vendors may host stealthy hardware Trojans. This paper presents the design and simulation of a two-chiplet AES encryption-decryption system communicating over the Bunch-of-Wires (BoW) protocol, into which multiple hardware Trojans from the Trust-Hub benchmark suite were integrated. The inserted Trojans were triggered by rare input conditions and evaluated through functional verification, structural analysis, and switching activity monitoring. A weighted Suspicion Score metric was introduced to unify the results from static inspection, DRC, timing, and switching activity analyses, enabling quantitative risk classification of Trojan presence. Experimental results demonstrated successful detection of several benchmark Trojans through measurable timing, logic, and power anomalies.",
    "title_zh": "芯粒架构中的硬件木马植入与检测",
    "abstract_zh": "基于芯粒（chiplet）的系统级芯片（SoC）架构的兴起，使得现代计算平台在模块化和可扩展性方面实现了前所未有的发展。芯粒已成为满足高性能与嵌入式系统日益增长性能需求的关键技术。然而，这种广泛应用也带来了新的攻击面，因为来自不可信供应商的异构芯粒可能潜藏隐蔽的硬件木马。本文设计并仿真了一个通过“一堆导线”（Bunch-of-Wires, BoW）协议通信的双芯粒AES加密解密系统，并在其中集成了来自Trust-Hub基准套件的多个硬件木马。这些插入的木马在罕见输入条件下被触发，通过功能验证、结构分析以及开关活动监控进行评估。本文提出了一种加权嫌疑评分（Suspicion Score）指标，综合静态检查、设计规则检查（DRC）、时序分析及开关活动分析的结果，实现对硬件木马存在风险的量化分类。实验结果表明，通过可测量的时序、逻辑和功耗异常，成功检测到了多个基准木马。"
  },
  {
    "date": "2026-1-12",
    "title": "Secure Software Engineering Through Sensible AutoMation (SESAM)",
    "authors": "Davide Fucci",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00027",
    "source": "IEEE",
    "abstract": "Background: Security is incorporated late in the Software Development Life Cycle (SDLC), whereas early activities supporting developers in understanding and implementing security measures are difficult to integrate. Aims: The project focuses on empowering developers with tools and practices to seamlessly integrate security and understand the role automation plays in it. Method: During the project, we perform several industrial empirical studies, qualitative and quantitative, under the Design Science Research paradigm. Results: Our studies supporting developers to secure their software supply chain show a positive stance despite low adoption of artifacts such as SBOM and VEX. Other efforts, embedding security in GUIbased testing, are showing promising results. Conclusion: The project covers a broad spectrum of development activities that can be enhanced from a security perspective. Initial results show that despite developers' interest, adoption is limited.",
    "title_zh": "通过合理自动化实现安全的软件工程（SESAM）",
    "abstract_zh": "背景：安全措施通常在软件开发生命周期（SDLC）的后期才被纳入，而支持开发人员理解并实施安全措施的早期活动却难以融入流程。  \n目标：本项目致力于为开发人员提供工具与实践方法，使其能够无缝地将安全融入开发过程，并理解自动化在其中所起的作用。  \n方法：在项目执行过程中，我们基于设计科学研究范式，开展了多项工业界的实证研究，涵盖定性与定量分析。  \n结果：我们的研究显示，尽管诸如SBOM（软件物料清单）和VEX（漏洞评估报告）等工件的采用率较低，但支持开发人员保障其软件供应链安全的努力仍获得积极反馈；此外，将安全嵌入基于图形界面的测试中也展现出良好的前景。  \n结论：本项目覆盖了广泛的开发活动，从安全角度均有提升空间。初步结果显示，尽管开发人员对安全有浓厚兴趣，但实际采纳程度仍然有限。"
  },
  {
    "date": "2026-1-12",
    "title": "Soft Computing Approaches for Software Maintainability Prediction: a Multi-Paradigm Review and Future Roadmap",
    "authors": "Preety Sharma, Ravi Kumar Sharma",
    "publish": "2025 International Conference on Digital Innovations for Sustainable Solutions (ICDISS)",
    "url": "https://doi.org/10.1109/icdiss68238.2025.11320773",
    "source": "IEEE",
    "abstract": "Because it allows for cost savings in terms of maintenance and improves the long term quality of systems, the predictability of the maintainability of software is now viewed as a vital element of software engineering. Because this work is a complete overview of the techniques that are most commonly used in the most common software development paradigms (such as Procedural, Object-Oriented, Component-Based Systems and Hybrid Systems) both historically and contemporarily, this study provides a complete overview of traditional and contemporary techniques of software maintainability prediction. Through the synthesis of current literature on methodology, metrics/prediction criteria, and results, studies were classified according to the type of technique used. Traditional techniques included empirically-based models, statistical regression, and metric-based techniques, while the primary attributes measured in these studies included modularity, readability, and traceability. Modern techniques utilize the soft computing paradigm including GWO-GA-NeuroFuzzy systems, and other hybrid models, such as FLANNGA, to increase both the accuracy and flexibility of software maintainability predictions. All of the research focused on key performance measures (MMRE, RMSE, MAE, SEM), and tools (Classic-Ada, CKJM) as well as the data sets (QUES, UIMS, OSS) used by each study. In addition to reviewing all of the research, the trend toward using AI-based predictive models, which have been validated through strict validation methods (i.e., <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$k$</tex>-fold or leave-one-out cross-validation), and have shown the ability to capture the dynamic aspects of maintainability (e.g., code complexity, design deficiencies, software obsolescence), was of particular interest. These findings have shown that although past methodologies were a good base for soft computing techniques to be applied; they improved forecast accuracy as well as the ability to scale up the forecasting process. Research in the future could focus on: applying soft computing methodologies to develop real-time forecasts of maintainability, automating the extraction of metrics, and developing intelligent solutions for specific domains.",
    "title_zh": "软计算方法在软件可维护性预测中的应用：多范式综述与未来路线图",
    "abstract_zh": "由于其在维护成本节约以及系统长期质量提升方面的优势，软件可维护性的可预测性如今被视为软件工程中至关重要的要素。本研究全面综述了在最常见软件开发范型（如过程式、面向对象、基于组件的系统及混合系统）中，无论是从历史还是当代视角来看，最常用的技术。因此，该研究为传统与现代软件可维护性预测技术提供了完整的概览。通过综合现有文献中的方法论、度量/预测标准以及研究结果，研究被按照所采用的技术类型进行分类。传统技术包括基于经验的模型、统计回归和基于度量的方法，这些研究主要衡量的属性包括模块化、可读性和可追溯性。而现代技术则采用软计算范式，如GWO-GA-NeuroFuzzy系统以及其他混合模型（如FLANNGA），以提高软件可维护性预测的准确性和灵活性。所有研究均聚焦于关键性能指标（MMRE、RMSE、MAE、SEM）、所使用的工具（Classic-Ada、CKJM）以及数据集（QUES、UIMS、OSS）。除了对已有研究的全面回顾外，特别值得关注的是，当前正呈现出使用基于人工智能的预测模型的趋势，这些模型通过严格的验证方法（如k折交叉验证或留一法交叉验证）得到验证，并展现出捕捉可维护性动态特征（如代码复杂性、设计缺陷、软件过时）的能力。研究结果表明，尽管以往的方法为软计算技术的应用奠定了良好基础，但它们不仅提升了预测准确性，也增强了预测过程的可扩展性。未来的研究可重点关注：将软计算方法应用于开发可维护性的实时预测、自动化度量提取，以及为特定领域开发智能化解决方案。"
  },
  {
    "date": "2026-1-12",
    "title": "Assessing RAG: A Comprehensive Review of Evaluation Frameworks",
    "authors": "Inaya Imtiyaz, Mahdi Parvaz, Shafaq Mandha, Fatima Farooq, Arash Kermani Kolankeh",
    "publish": "2025 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE)",
    "url": "https://doi.org/10.1109/iccike67021.2025.11318213",
    "source": "IEEE",
    "abstract": "Retrieval-Augmented Generation (RAG) pipelines, which combine retrieval and generative AI components, are becoming crucial for accurate context-aware text generation in a variety of applications. However, this remains challenging for RAG systems to evaluate due to their complexity and the need to evaluate retrieval and generation components. This survey is conducted to fill the gap in systematic evaluation methodologies by examining existing RAG evaluation frameworks and providing practical advice to improve performance. We compare and evaluate according to metrics such as precision, faithfulness, and adaptability. Our analysis shows that open-source tools focus on transparency and customization but demand some technical knowledge, while the proprietary options focus on integration ease at the cost of flexibility. Major findings show that the granularity of metrics is related to the computational cost and proximity to the domain-specific needs. This work highlights the importance of rigorous evaluation frameworks for RAG systems to achieve practical reliability in real-world applications, thus closing the gap between theoretical benchmarks and real-world deployment.",
    "title_zh": "评估RAG：评估框架的全面综述",
    "abstract_zh": "检索增强生成（RAG）管道将检索与生成式AI组件相结合，正日益成为各类应用中实现准确上下文感知文本生成的关键技术。然而，由于其复杂性以及需要对检索和生成两个组件进行评估，RAG系统的评估仍面临挑战。为填补系统化评估方法的空白，本综述深入分析了现有的RAG评估框架，并提供了提升性能的实用建议。我们依据精确度、忠实度和适应性等指标进行了比较与评估。分析表明，开源工具注重透明性和可定制性，但对用户的技术能力有一定要求；而专有方案则更强调集成便捷性，却牺牲了灵活性。主要发现指出，评估指标的粒度与计算成本及领域特定需求的接近程度密切相关。本研究强调了为RAG系统建立严谨评估框架的重要性，以实现在实际应用中的可靠表现，从而弥合理论基准与真实部署之间的差距。"
  },
  {
    "date": "2026-1-12",
    "title": "Contextual Code Retrieval for Commit Message Generation: A Preliminary Study",
    "authors": "Bo Xiong, Linghao Zhang, Chong Wang, Peng Liang",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00038",
    "source": "IEEE",
    "abstract": "Background: A commit message describes the main code changes in a commit and plays a crucial role in software maintenance. Existing commit message generation (CMG) approaches typically frame it as a direct mapping which inputs a code diff and produces a brief descriptive sentence as output. Aims: Since the raw code diff lacks the context related to the code itself, we intend to supplement the relevant code as input of CMG to generate high-quality and informative commit messages. Method: We propose a contextual code retrieval-based method called C3Gen to enhance CMG by retrieving commit-relevant code snippets from the repository and incorporating them into the model input to provide richer contextual information at the repository scope. In the experiments, we evaluated the effectiveness of C3Gen across various models using four objective and three subjective metrics. Meanwhile, we design and conduct a human evaluation to investigate how C3Gen-generated commit messages are perceived by human developers. Results & Conclusions: By incorporating contextual code into the input, C3Gen enables models to leverage additional information to generate more comprehensive and informative commit messages with greater practical value in real-world development scenarios. Further analysis underscores concerns about the reliability of similarity-based metrics and provides empirical insights for CMG.",
    "title_zh": "用于生成提交消息的上下文代码检索：一项初步研究",
    "abstract_zh": "背景：提交信息（commit message）描述了提交中的主要代码变更，在软件维护中起着至关重要的作用。现有的提交信息生成（CMG）方法通常将其视为一种直接映射任务，即输入代码差异（code diff），输出一句简短的描述性语句。  \n目标：由于原始代码差异缺乏与代码本身相关的上下文信息，我们旨在通过补充相关代码作为CMG的输入，以生成更高质量、更具信息量的提交信息。  \n方法：我们提出了一种基于上下文代码检索的方法——C3Gen，通过从代码仓库中检索与提交相关的代码片段，并将其融入模型输入，从而在仓库层面提供更丰富的上下文信息，以增强CMG的效果。在实验中，我们使用四种客观指标和三种主观指标，评估了C3Gen在多种模型上的有效性。同时，我们设计并开展了一项人工评估，以研究人类开发者对C3Gen生成的提交信息的感知情况。  \n结果与结论：通过在输入中引入上下文代码，C3Gen使模型能够利用额外信息，生成更加全面、富有信息量的提交信息，具有更高的实际应用价值，适用于真实开发场景。进一步分析揭示了基于相似性的度量指标在可靠性方面存在的问题，并为CMG研究提供了实证启示。"
  },
  {
    "date": "2026-1-12",
    "title": "ZAPx: Extending OWASP ZAP for Enhanced Web Vulnerability Detection and AI-Powered Remediation",
    "authors": "Kittipat Tangtanawirut, Archawit Changtor, Pakapon Rattanasrisuk, Somchart Fugkeaw",
    "publish": "2025 Thirteenth International Symposium on Computing and Networking Workshops (CANDARW)",
    "url": "https://doi.org/10.1109/candarw68385.2025.00045",
    "source": "IEEE",
    "abstract": "Web applications are increasingly targeted by automated attacks and client-side code vulnerabilities, posing significant risks to data confidentiality and system integrity. Automated threats-such as web scraping, brute force, and flooding requests-exploit weak defenses like missing rate limiting or CAPTCHA, while insecure client-side implementations often expose sensitive information such as API keys and access tokens. Existing tools, including the OWASP Zed Attack Proxy (ZAP), provide strong baseline detection but lack comprehensive capabilities to address these challenges and support actionable remediation. To fill this gap, we develop ZAPx, an extension of ZAP that integrates enhanced detection and AI-driven remediation. ZAPx introduces: (i) an active scan module to detect vulnerabilities exploitable by automated attacks, (ii) a passive scan module to identify sensitive data exposure in client-side code, and (iii) a generative AI engine that produces context-specific remediation plans. Experimental results demonstrate that ZAPx achieves a higher detection rate of web application vulnerabilities while reducing false positives compared to the baseline ZAP tool.",
    "title_zh": "ZAPx：扩展OWASP ZAP以增强Web漏洞检测与AI驱动的修复能力",
    "abstract_zh": "Web应用程序正日益成为自动化攻击和客户端代码漏洞的目标，对数据机密性和系统完整性构成重大威胁。自动化攻击（如网络爬虫、暴力破解和请求洪泛）利用诸如缺乏速率限制或CAPTCHA等薄弱防御机制，而不安全的客户端实现则常常暴露敏感信息，例如API密钥和访问令牌。现有的工具（如OWASP Zed Attack Proxy，简称ZAP）虽然提供了强大的基础检测能力，但在应对这些挑战方面仍缺乏全面的功能，且难以提供可操作的修复建议。为弥补这一空白，我们开发了ZAPx——ZAP的一个增强扩展，集成了更先进的漏洞检测与基于人工智能的修复功能。ZAPx引入了三项核心特性：(i) 主动扫描模块，用于发现可被自动化攻击利用的漏洞；(ii) 被动扫描模块，用于识别客户端代码中的敏感数据泄露问题；(iii) 生成式AI引擎，能够生成上下文相关的修复方案。实验结果表明，相较于基准版本的ZAP工具，ZAPx在检测Web应用漏洞方面具有更高的准确率，同时显著降低了误报率。"
  },
  {
    "date": "2026-1-12",
    "title": "Identifier Name Similarities: An Exploratory Study",
    "authors": "Carol Wong, Mai Abe, Silvia De Benedictis, Marissa Halim, Anthony Peruma",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00034",
    "source": "IEEE",
    "abstract": "Background: Identifier names, which comprise a significant portion of the codebase, are the cornerstone of effective program comprehension. However, research has shown that poorly chosen names can significantly increase cognitive load and hinder collaboration. Even names that appear readable in isolation may lead to misunderstandings in contexts when they closely resemble other names in either structure or functionality. Aims: This exploratory study aims to investigate the occurrence of similar identifier names in projects and develop a taxonomy categorizing name similarities. Method: Five open-source Java projects were analyzed using automated extraction and manual review. Results: Our findings reveal a taxonomy comprising seven categories of identifier name similarities. Conclusions: We envision our initial taxonomy providing researchers with a platform to analyze and evaluate the impact of identifier name similarity on code comprehension, maintainability, and collaboration among developers, while also allowing for further refinement and expansion of the taxonomy.",
    "title_zh": "标识符名称相似性：一项探索性研究",
    "abstract_zh": "背景：标识符名称构成了代码库的重要组成部分，是有效理解程序的基础。然而，研究表明，不恰当的命名会显著增加认知负担，并阻碍协作。即使某些名称在单独看来清晰易懂，但在上下文中，若其结构或功能与其他名称过于相似，仍可能导致误解。  \n目标：本探索性研究旨在调查项目中相似标识符名称的出现情况，并构建一个对名称相似性进行分类的分类体系。  \n方法：通过自动化提取与人工审查相结合的方式，分析了五个开源Java项目。  \n结果：研究发现，标识符名称的相似性可归纳为七个类别。  \n结论：我们期望这一初步分类体系能为研究人员提供一个平台，用于分析和评估标识符名称相似性对代码理解、可维护性以及开发者协作的影响，同时为进一步完善和扩展该分类体系奠定基础。"
  },
  {
    "date": "2026-1-12",
    "title": "A Novel Cross Project Defect Prediction Method Based on Data Filter",
    "authors": "Shiqi Tang, Minggui Song, Zhang Hu, Lingzhi Zhu",
    "publish": "2025 12th International Conference on Dependable Systems and Their Applications (DSA)",
    "url": "https://doi.org/10.1109/dsa66321.2025.00048",
    "source": "IEEE",
    "abstract": "Software defect prediction(SDP) is an important technique in software testing, which can predict the potential defects in software during the early stages of development and improving software quality. However, in practice, the lack of historical data often makes it difficult for traditional software defect prediction methods to achieve satisfactory results. As a result, cross-project software defect prediction (CPDP) techniques have gained increasing attention. Among these, data filter-based(namely DF) methods are a relatively mainstream defect prediction technique in CPDP. Nevertheless, current DF methods usually do not consider the issue of concept Shift between different projects. To overcome this problem, this paper proposes a novel cross-project defect based on data filter prediction method called TrDF. This method first selects multiple source project datasets that are similar to the target project. Then, we assigns different weights to these datasets based on the smoothness assumption. Finally, we bagging to train efect prediction classifier based on these these weights. This paper compares TrDF with other CPDP methods on 11 datasets, measuring their F1-score and G-mean values. The experimental results show that the performance achieved by TrDF is better than other CPDP methods.",
    "title_zh": "一种基于数据过滤的新型跨项目缺陷预测方法",
    "abstract_zh": "软件缺陷预测（SDP）是软件测试中的一项重要技术，能够在开发早期预测软件中潜在的缺陷，从而提升软件质量。然而，在实际应用中，历史数据的缺乏常常导致传统软件缺陷预测方法难以取得理想效果。因此，跨项目软件缺陷预测（CPDP）技术逐渐受到关注。在这些方法中，基于数据过滤（即DF）的方法是CPDP领域较为主流的技术之一。然而，现有的DF方法通常未考虑不同项目之间存在的概念漂移问题。为解决这一问题，本文提出了一种基于数据过滤的新型跨项目缺陷预测方法——TrDF。该方法首先选择多个与目标项目相似的源项目数据集，然后根据平滑性假设为这些数据集分配不同的权重，最后利用加权集成（bagging）训练缺陷预测分类器。本文在11个数据集上将TrDF与其他CPDP方法进行了对比，评估了其F1分数和G-mean值。实验结果表明，TrDF的性能优于其他CPDP方法。"
  },
  {
    "date": "2026-1-12",
    "title": "We Know What You're Looking For: Recommendation for Large-Scale Open Source Software",
    "authors": "Xing Cui, Jingzheng Wu, Xiang Ling, Tianyue Luo",
    "publish": "2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "url": "https://doi.org/10.1109/esem64174.2025.00046",
    "source": "IEEE",
    "abstract": "Background: In recent years, with the advancement of software engineering technologies and industry, Open Source Software (OSS) has become a mainstream model for software development and innovation. Increasingly, organizations and developers are adopting and customizing existing OSS to simplify and accelerate development processes. During OSS adoption, recommending suitable software based on user needs is crucial for enhancing development efficiency and addressing diverse requirements. However, the vast number and diversity of OSS make the recommendation task highly challenging. Despite progress in previous research, several issues remain, such as neglect of key software attributes, complexity in extracting multilingual features, and challenges of cold start and data sparsity. Aims: This paper presents AthenaRec, a large-scale OSS recommendation system comprising three core modules: Delphi, Argus, and Hestia. AthenaRec aims to recommend relevant and suitable software from a vast OSS based on user needs. Method: Specifically, Delphi first analyzes user queries to identify intention; Argus employs a heterogeneous ensemble recall approach to retrieve a large set of candidate software relevant to the identified intention; finally, Hestia adopts a two-stage deep ranking strategy. It performs coarse ranking by integrating multilingual modeling with contrastive learning, followed by fine ranking with a large language model, augmented by retrieval-augmented generation to incorporate external evidence. To evaluate the effectiveness of AthenaRec, we use a query dataset from real application scenarios. Results: Experimental results demonstrate that, on the test set of 7,500 queries, AthenaRec achieves superior recommendation performance, with Hits@20, MAP@20, NDCG@20, and MRR scores of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$98.27 \\%, 95.60 \\%, 95.05 {\\%}$</tex>, and 92.92%, respectively. On average, AthenaRec outperforms other top methods by 10.9% across all evaluation metrics. Conclusions: Additionally, we develop a Visual Studio Code (VSCode) plugin based on AthenaRec, which can be accessed via URL. We intend for this research to provide a reference for software developers, advancing the efficiency and accuracy of OSS recommendation.",
    "title_zh": "我们知道您在寻找什么：大规模开源软件推荐",
    "abstract_zh": "背景：近年来，随着软件工程技术和产业的不断发展，开源软件（Open Source Software, OSS）已成为软件开发与创新的主流模式。越来越多的组织和开发者采用并定制现有的开源软件，以简化和加速开发流程。在开源软件采纳过程中，根据用户需求推荐合适的软件，对于提升开发效率、满足多样化需求至关重要。然而，开源软件数量庞大且类型多样，使得推荐任务面临巨大挑战。尽管以往研究已取得一定进展，但仍存在若干问题，如忽视关键软件属性、多语言特征提取复杂、冷启动与数据稀疏性难题等。\n\n目标：本文提出 AthenaRec，一个大规模开源软件推荐系统，包含三个核心模块：Delphi、Argus 和 Hestia。AthenaRec 的目标是基于用户需求，从海量开源软件中推荐相关且合适的软件。\n\n方法：具体而言，Delphi 首先分析用户查询，识别其意图；Argus 采用异构集成召回策略，检索出大量与识别出的意图相关的候选软件；最后，Hestia 采用两阶段深度排序策略：首先通过融合多语言建模与对比学习进行粗粒度排序，随后利用大语言模型进行细粒度排序，并结合检索增强生成（Retrieval-Augmented Generation）引入外部证据，以提升推荐质量。\n\n为评估 AthenaRec 的有效性，我们使用来自真实应用场景的查询数据集进行测试。\n\n结果：实验结果表明，在包含 7,500 个查询的测试集上，AthenaRec 在 Hits@20、MAP@20、NDCG@20 和 MRR 等指标上分别达到 98.27%、95.60%、95.05% 和 92.92%，表现优异。平均而言，AthenaRec 在所有评估指标上均优于其他顶尖方法约 10.9%。\n\n结论：此外，我们基于 AthenaRec 开发了一个 Visual Studio Code（VSCode）插件，可通过 URL 访问。本研究旨在为软件开发者提供参考，推动开源软件推荐的效率与准确性进一步提升。"
  },
  {
    "date": "2026-1-12",
    "title": "Research on the Application of Machine-Readable Standards Comprehensive Evaluation Model and Testing Platform",
    "authors": "Wenwen Li, Yanruoyue Li, Tianyuan Cai",
    "publish": "2025 IEEE International Conference on Social Computing and Networking (SocialCom)",
    "url": "https://doi.org/10.1109/socialcom67919.2025.00010",
    "source": "IEEE",
    "abstract": "In the digital economy era, the transformation of all industries toward digitalization has become imperative. The digital transformation of standards is both a self-driven evolution and an intrinsic demand to adapt to the digital age. By establishing a comprehensive evaluation model for machine-readable standards and integrating it with application scenario-based testing platforms, a systematic assessment framework of “evaluation metrics-testing platform-machine-readable standard verification” has been developed. This helps to improve the stability and standardization of the quality of machine-readable standards, and promote the industrial application of machine-readable standards.",
    "title_zh": "机器可读标准综合评价模型与测试平台的应用研究",
    "abstract_zh": "在数字经济时代，各行业向数字化转型已成为必然趋势。标准的数字化转型既是自身发展的内在演进，也是适应数字时代的必然需求。通过构建全面的机器可读标准评估模型，并将其与基于应用场景的测试平台相融合，已形成“评价指标—测试平台—机器可读标准验证”的系统化评估框架。该框架有助于提升机器可读标准的质量稳定性与标准化水平，推动机器可读标准在产业中的广泛应用。"
  },
  {
    "date": "2026-1-12",
    "title": "Inside GateMate: Analysis and Benchmarking of a New FPGA Architecture",
    "authors": "Tarik Ibrahimović, Nedim Osmić",
    "publish": "2025 37th International Conference on Microelectronics (ICM)",
    "url": "https://doi.org/10.1109/icm66518.2025.11322434",
    "source": "IEEE",
    "abstract": "While modern FPGAs typically implement programmable logic using <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathrm{4}-\\mathrm{6}$</tex>-input LUTs, the Cologne Chip GateMate FPGA instead adopts a LUT-tree architecture, among several other distinctive features. We analyze its design trade-offs and support our findings with results from a targeted benchmark suite. LUT-trees prove less efficient for combinational logic than LUTs. Equivalent RTL designs (excluding DSPs) require <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">${1 0 - 3 0 \\%}$</tex> more programmable elements (CPEs) in GateMate than in comparable FPGAs. We observed a reduced logic-area efficiency, and the 1:1 flip-flop (FF) to logic ratio yields <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$30 \\% \\text{FF}$</tex> under-utilization across diverse RTL designs. Comparative evaluation with a peer FPGA demonstrates that GateMate is suited for deeply pipelined applications with modest DSP and control requirements. Lack of timing-driven place and route, distributed RAM and DSP blocks limits usability for arithmetic-heavy, monolithic designs. We conclude that its dual-port block RAM and FF-dense fabric are strengths in particular application domains. Improved LUT-tree-optimized logic synthesis and constraint-driven place and route are required to increase its competitiveness.",
    "title_zh": "门内GateMate：一种新型FPGA架构的分析与基准测试",
    "abstract_zh": "尽管现代FPGA通常采用4至6输入查找表（LUT）来实现可编程逻辑，但科隆芯片的GateMate FPGA却采用了LUT树架构，此外还具备其他若干独特特性。我们分析了其设计权衡，并通过专门的基准测试套件结果支持我们的发现。研究表明，与传统的LUT相比，LUT树在组合逻辑实现上效率较低。在不包含DSP模块的前提下，同等功能的RTL设计在GateMate中所需的可编程逻辑单元（CPE）数量比同类FPGA高出10%至30%。我们观察到逻辑面积利用率下降，且1:1的触发器（FF）与逻辑单元比例导致在多种RTL设计中触发器资源平均存在30%的未充分利用。与同代FPGA的对比评估表明，GateMate适用于对流水线深度要求较高、而对DSP和控制逻辑需求适中的应用场景。然而，由于缺乏时序驱动的布局布线、分布式RAM以及DSP模块，该架构在计算密集型、单片式设计中的适用性受到限制。综上所述，其双端口块RAM和高密度触发器结构在特定应用领域具有显著优势。为提升其竞争力，亟需改进针对LUT树优化的逻辑综合技术，以及引入约束驱动的布局布线方法。"
  },
  {
    "date": "2026-1-12",
    "title": "Contextual Embedding and Deep Neural Network Pipeline for Robust SQL Injection Detection and Categorization",
    "authors": "Ibrahim El Bitar, Razan El Mais, Mohamad Raad",
    "publish": "2025 IEEE International Conference on Emerging Trends in Engineering and Computing (ETECOM)",
    "url": "https://doi.org/10.1109/etecom66111.2025.11319095",
    "source": "IEEE",
    "abstract": "SQL injection (SQLi) attacks remain one of the most critical security threats to modern web applications, allowing attackers to bypass authentication and compromise sensitive data. This paper introduces SIDRAH, a dual-phase detection framework that combines BERT-based contextual embeddings with a deep neural network (DNN) pipeline for both binary SQLi detection and fine-grained attack categorization. SIDRAH operates in two stages: an offline phase for dataset preparation, model training, and optimization, and an online phase for realtime query analysis and classification. Evaluated on benchmark datasets and diverse payload collections, SIDRAH demonstrated robust performance and accurate classification across multiple SQLi attack categories, highlighting its potential for deployment as a scalable, low-latency middleware solution for securing web applications.",
    "title_zh": "基于上下文嵌入与深度神经网络的鲁棒SQL注入检测与分类管道",
    "abstract_zh": "SQL注入（SQLi）攻击仍然是现代Web应用面临的最严重安全威胁之一，攻击者可借此绕过身份验证并窃取敏感数据。本文提出了一种名为SIDRAH的双阶段检测框架，该框架结合基于BERT的上下文嵌入与深度神经网络（DNN）流水线，实现对SQL注入攻击的二分类检测及细粒度攻击类型识别。SIDRAH采用两阶段运行机制：离线阶段用于数据集准备、模型训练与优化；在线阶段则用于实时查询分析与分类。在基准数据集和多种攻击载荷集合上的评估结果表明，SIDRAH在多个SQL注入攻击类别中均表现出稳健的性能和高精度的分类能力，凸显其作为可扩展、低延迟中间件解决方案在保护Web应用方面的巨大潜力。"
  },
  {
    "date": "2026-1-12",
    "title": "LLM-based Anomaly Detection for Digital Substation Cybersecurity",
    "authors": "Priscilla Kyei Danso, Liam McGevna, Jason Chow, Jeffery Luo",
    "publish": "2025 IEEE 22nd International Conference on Smart Communities: Improving Quality of Life using AI, Robotics and IoT (HONET)",
    "url": "https://doi.org/10.1109/honet67928.2025.11318449",
    "source": "IEEE",
    "abstract": "The security of digital substations is pivotal in the operations of smart grid. These substations rely on intelligent electronic devices (IEDs) for real-time data exchange and control defined by the IEC 61850 standard of the Generic Object-Oriented Substation Event (GOOSE) protocol. Presently, the predominant approach for anomaly detection in standardized GOOSE communications relies on Machine Learning (ML) techniques. While these methods demonstrate impressive accuracy, ML techniques face the challenge of needing retraining whenever new attack types arise. This retraining process involves gathering extensive datasets, leaving the system vulnerable in the interim. The proposed work aims to utilize the contextual understanding of Large Language Models (LLMs), specifically ChatGPT for anomaly detection in the GOOSE protocol. We achieved this by developing a method to feed ChatGPT full GOOSE network packet traces and provide suggestions on identifying attack types, thereby incorporating Human-in-the-Loop (HITL) training. We also aimed to explore the potential of LLMs to offer comprehensive insights and explanations, known as Natural Language Explanations, regarding potential attacks. Based on our experimental results, the potential of LLMs to enhance the security of digital substations, showcasing promising results with improved accuracy and explainability in detecting anomalies in GOOSE communications.",
    "title_zh": "基于大语言模型的数字化变电站网络安全异常检测",
    "abstract_zh": "数字化变电站的安全性在智能电网的运行中至关重要。这些变电站依赖于智能电子设备（IEDs）进行实时数据交换与控制，其通信遵循通用对象面向变电站事件（GOOSE）协议的IEC 61850标准。目前，针对标准化GOOSE通信中的异常检测主要采用机器学习（ML）技术。尽管这些方法表现出较高的准确性，但当出现新型攻击时，ML技术面临需要重新训练的挑战。这一重新训练过程需收集大量数据集，导致系统在期间处于脆弱状态。本文提出利用大型语言模型（LLMs），特别是ChatGPT，在GOOSE协议异常检测中的上下文理解能力。我们通过开发一种方法，将完整的GOOSE网络数据包追踪信息输入ChatGPT，并由其提供攻击类型识别建议，从而实现“人在回路”（HITL）的训练模式。此外，我们还探索了LLM提供全面洞察与解释的潜力，即以自然语言形式呈现的解释（Natural Language Explanations），用于分析潜在攻击。基于实验结果，LLM在提升数字化变电站安全性方面展现出巨大潜力，不仅显著提高了对GOOSE通信中异常行为的检测准确率，还增强了检测过程的可解释性。"
  },
  {
    "date": "2026-1-12",
    "title": "Large Language Model-Based Hierarchical Framework for Emerging Trends in Extreme Ultraviolet Research",
    "authors": "Sung Tae Yoo, Seonho Kim, Jae-Min Lee",
    "publish": "2025 37th International Conference on Microelectronics (ICM)",
    "url": "https://doi.org/10.1109/icm66518.2025.11322531",
    "source": "IEEE",
    "abstract": "As artificial intelligence advances rapidly, demand for node scaling and improved device performance is increasing in semiconductors. This necessitates a systematic analysis of extreme ultraviolet (EUV) technology developments. This study introduces a hierarchical framework based on large language models (LLM) and applies it to 6,859 EUV-related papers published from 2015 to 2024. The framework combines clustering, summarization, evaluation, selection, and a decision stage guided by an LLM-based AI agent to build a hierarchical tree, and it then analyzes the hierarchy step by step to track emerging trends in the field. Emerging trends in EUV research include algorithmic metrology and overlay modeling, illumination engineering with pupil mapping, and advances in multi-beam mask writing along with the supplier ecosystem. This hierarchical framework is expected to not only determine the future direction of EUV technology but also broadly apply LLM-based methods to identify various emerging trends.",
    "title_zh": "基于大语言模型的极端紫外研究新兴趋势分层框架",
    "abstract_zh": "随着人工智能的迅速发展，半导体领域对节点扩展和设备性能提升的需求日益增长。这要求对极紫外（EUV）技术的发展进行系统性分析。本研究提出一种基于大语言模型（LLM）的分层框架，并将其应用于2015年至2024年间发表的6,859篇EUV相关论文。该框架结合聚类、摘要生成、评估、筛选以及由LLM驱动的AI代理引导的决策阶段，构建出一个分层树状结构，随后逐层分析该层次结构，以追踪该领域的新兴趋势。EUV研究中的新兴趋势包括算法计量学与套刻建模、基于瞳孔映射的照明工程，以及多束光罩写入技术的进步及其供应商生态系统的发展。这一分层框架不仅有望确定EUV技术的未来发展方向，还可广泛推广基于LLM的方法，用于识别各类新兴趋势。"
  },
  {
    "date": "2026-1-12",
    "title": "Testing-Based Formal Verification for Software Dependability",
    "authors": "Shaoying Liu",
    "publish": "2025 12th International Conference on Dependable Systems and Their Applications (DSA)",
    "url": "https://doi.org/10.1109/dsa66321.2025.00010",
    "source": "IEEE",
    "abstract": "Software dependability encompasses five critical properties: reliability, safety, integrity, availability, and maintainability. Ensuring these attributes throughout the software engineering process remains a significant and ongoing challenge. In this talk, I will begin by briefly outlining a general framework for developing dependable software. I will then focus on an advanced technique known as Testing-Based Formal Verification (TBFV), which integrates specification-based testing with formal methods to verify program correctness. TBFV is characterized by its ability to automatically verify the correctness of all program paths explored during testing, while also having the potential to uncover previously untested paths. This dual capability supports both validation and verification, enhancing the overall assurance of software quality. I will explain the core principles of TBFV, specific techniques it employs for fault prevention, validation, and verification, as well as the key challenges associated with its practical implementation. When fully realized, TBFV is expected to significantly reduce testing time and cost, and substantially improve software dependability.",
    "title_zh": "基于测试的软件可靠性形式化验证",
    "abstract_zh": "软件可靠性涵盖五个关键属性：可靠性、安全性、完整性、可用性和可维护性。在软件工程过程中始终确保这些特性，仍然是一个重大且持续面临的挑战。在本次演讲中，我将首先简要介绍一个开发可靠软件的通用框架。随后，我将重点探讨一种先进的技术——基于测试的形式化验证（TBFV），该技术将基于规格的测试与形式化方法相结合，以验证程序的正确性。TBFV 的特点在于，它能够自动验证测试过程中所探索的所有程序路径的正确性，同时还有潜力发现此前未被测试过的路径。这一双重能力支持了验证与确认，从而显著提升了软件质量的整体保障水平。我将阐述 TBFV 的核心原理，以及其在故障预防、验证和确认方面所采用的具体技术，并讨论其实际应用中的主要挑战。当该技术得以充分实现时，预计将大幅缩短测试时间与成本，并显著提升软件的可靠性。"
  },
  {
    "date": "2026-1-12",
    "title": "Neural Bandit Based Optimal LLM Selection for Pipeline of Tasks",
    "authors": "Baran Atalar",
    "publish": "ACM SIGMETRICS Performance Evaluation Review",
    "url": "https://doi.org/10.1145/3788882.3788889",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "基于神经赌徒的流水线任务最优大模型选择",
    "abstract_zh": "None"
  }
]