[
  {
    "date": "2025-12-18",
    "title": "Adaptation of Agentic AI",
    "authors": "Pengcheng Jiang, Jiacheng Lin, Zhiyi Shi, Zifeng Wang, Luxi He, Yichen Wu, Ming Zhong, Peiyang Song, Qizheng Zhang, Heng Wang, Xueqiang Xu, Hanwen Xu, Pengrui Han, Dylan Zhang, Jiashuo Sun, Chaoqi Yang, Kun Qian, Tian Wang, Changran Hu, Manling Li, Quanzheng Li, Hao Peng, Sheng Wang, Jingbo Shang, Chao Zhang, Jiaxuan You, Liyuan Liu, Pan Lu, Yu Zhang, Heng Ji, Yejin Choi, Dawn Song, Jimeng Sun, Jiawei Han",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16301v1",
    "source": "arXiv",
    "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
    "title_zh": "代理式人工智能的适应性调整",
    "abstract_zh": "前沿的智能体式AI系统建立在基础模型之上，这些模型能够被调整以规划、推理，并与外部工具交互，从而执行日益复杂且专业化的任务。随着这些系统能力与应用范围的不断扩展，适应性逐渐成为提升性能、可靠性与泛化能力的核心机制。本文将快速发展的研究领域整合为一个系统性框架，涵盖智能体适应与工具适应两个方面。我们进一步将智能体适应细分为由工具执行信号触发和由智能体输出信号触发两种形式，也将工具适应划分为无智能体依赖与有智能体监督两种形式。通过这一框架，我们展示了其在厘清智能体AI中适应策略设计空间、明确各类策略之间的权衡关系方面的价值，并为系统设计过程中选择或切换适应策略提供了实用指导。随后，我们对每一类别中的代表性方法进行了回顾，分析了它们的优势与局限性，并指出了关键的开放性挑战与未来机遇。总体而言，本文旨在为研究人员与实践者提供一个概念基础与实用路线图，助力构建更强大、高效且可靠的智能体式AI系统。"
  },
  {
    "date": "2025-12-18",
    "title": "Trustworthy and Controllable Professional Knowledge Utilization in Large Language Models with TEE-GPU Execution",
    "authors": "Yifeng Cai, Zhida An, Yuhan Meng, Houqian Liu, Pengli Wang, Yao Guo, Ding Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16238v1",
    "source": "arXiv",
    "abstract": "Future improvements in large language model (LLM) services increasingly hinge on access to high-value professional knowledge rather than more generic web data. However, the data providers of this knowledge face a skewed tradeoff between income and risk: they receive little share of downstream value yet retain copyright and privacy liability, making them reluctant to contribute their assets to LLM services. Existing techniques do not offer a trustworthy and controllable way to use professional knowledge, because they keep providers in the dark and combine knowledge parameters with the underlying LLM backbone. In this paper, we present PKUS, the Professional Knowledge Utilization System, which treats professional knowledge as a first-class, separable artifact. PKUS keeps the backbone model on GPUs and encodes each provider's contribution as a compact adapter that executes only inside an attested Trusted Execution Environment (TEE). A hardware-rooted lifecycle protocol, adapter pruning, multi-provider aggregation, and split-execution scheduling together make this design practical at serving time. On SST-2, MNLI, and SQuAD with GPT-2 Large and Llama-3.2-1B, PKUS preserves model utility, matching the accuracy and F1 of full fine-tuning and plain LoRA, while achieving the lowest per-request latency with 8.1-11.9x speedup over CPU-only TEE inference and naive CPU-GPU co-execution.",
    "title_zh": "基于TEE-GPU执行的大语言模型中可信且可控制的专业知识利用",
    "abstract_zh": "未来大型语言模型（LLM）服务的改进越来越依赖于高价值的专业知识，而非更通用的网络数据。然而，这些专业知识的数据提供者面临着收入与风险严重失衡的困境：他们仅能获得下游价值的一小部分，却仍需承担版权和隐私责任，因而不愿将自身资产贡献给LLM服务。现有技术无法为专业知识的使用提供可信且可控的途径，因为它们对数据提供者信息不透明，并将知识参数与底层LLM主干模型紧密耦合。本文提出PKUS——专业知识利用系统（Professional Knowledge Utilization System），将专业知识视为一种独立、可分离的一等公民实体。PKUS将主干模型保留在GPU上，同时将每位提供者的贡献编码为一个轻量级适配器，该适配器仅在经过验证的可信执行环境（TEE）内运行。通过基于硬件根的信任生命周期协议、适配器剪枝、多提供者聚合以及分段执行调度等机制，该设计在实际服务场景中具备高度可行性。在SST-2、MNLI和SQuAD任务上，使用GPT-2 Large和Llama-3.2-1B模型进行测试，PKUS在保持模型性能方面表现优异，其准确率和F1分数与全量微调及普通LoRA方法相当，同时实现了最低的每请求延迟，在8.1至11.9倍的速度提升方面显著优于纯CPU TEE推理和简单的CPU-GPU协同执行方案。"
  },
  {
    "date": "2025-12-18",
    "title": "ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs",
    "authors": "Hao Chen, Zhexin Hu, Jiajun Chai, Haocheng Yang, Hang He, Xiaohan Wang, Wei Lin, Luhang Wang, Guojun Yin, Zhuofeng zhao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16149v1",
    "source": "arXiv",
    "abstract": "Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .",
    "title_zh": "ToolForge：一种无需真实世界API的多跳搜索数据合成流水线",
    "abstract_zh": "训练大语言模型（LLM）以调用工具并利用检索到的信息，需要高质量且多样化的数据。然而，现有的合成数据生成流程通常依赖于数万次真实的API调用以提升泛化能力，这不仅成本高昂，还缺乏多跳推理和自我反思能力。为解决这些局限性，我们提出了ToolForge——一种自动化的合成框架，仅通过构建少量虚拟工具即可实现强大的真实世界工具调用性能，完全无需使用真实的API调用。ToolForge利用（问题，黄金上下文，答案）三元组，专门针对多跳搜索场景合成大规模的工具学习数据，并通过多跳推理与自我反思机制进一步丰富生成的数据内容。为确保数据质量，我们设计了多层验证框架，融合规则-based与模型-based评估方法。实验结果表明，仅使用80亿参数的模型，在我们合成的数据上进行训练后，其表现已超越GPT-4o在多个基准测试中的水平。我们的代码与数据集已公开，可访问 https://github.com/Buycar-arb/ToolForge 。"
  },
  {
    "date": "2025-12-18",
    "title": "INTELLECT-3: Technical Report",
    "authors": "Prime Intellect Team, Mika Senghaas, Fares Obeid, Sami Jaghouar, William Brown, Jack Min Ong, Daniel Auras, Matej Sirovatka, Jannik Straube, Andrew Baker, Sebastian Müller, Justus Mattern, Manveer Basra, Aiman Ismail, Dominik Scherm, Cooper Miller, Ameen Patel, Simon Kirsten, Mario Sieg, Christian Reetz, Kemal Erdem, Vincent Weisser, Johannes Hagemann",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16144v1",
    "source": "arXiv",
    "abstract": "We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",
    "title_zh": "智力-3：技术报告",
    "abstract_zh": "我们推出了 INTELLECT-3，这是一个拥有1060亿参数的专家混合模型（活跃参数为120亿），基于我们端到端强化学习基础设施栈，通过大规模强化学习训练而成。INTELLECT-3 在数学、代码、科学和推理等基准测试中，其性能在同类规模模型中达到顶尖水平，甚至超越了许多参数更大的前沿模型。我们开源了该模型及其完整的基础设施栈，包括强化学习框架、完整的训练配方，以及由我们的 Environments Hub 社区平台构建的大量环境，这些环境均基于验证器库（verifiers library）开发，可用于训练与评估。为支持本项目，我们还推出了 prime-rl——一个面向大规模异步强化学习的开源框架，可无缝扩展至单节点到数千张GPU，专为智能体强化学习设计，原生支持多轮交互和工具使用。借助这一完整技术栈，我们在 GLM-4.5-Air-Base 模型基础上同时执行监督微调（SFT）和强化学习训练，并将强化学习训练规模扩展至512块H200 GPU，实现了极高的训练效率。"
  },
  {
    "date": "2025-12-18",
    "title": "Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse",
    "authors": "Aaron Imani, Mohammad Moshirpour, Iftekhar Ahmed",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16790v1",
    "source": "arXiv",
    "abstract": "While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.",
    "title_zh": "《内心探秘：揭示评论内化如何引导大语言模型向好或向坏发展》",
    "abstract_zh": "尽管注释是源代码中非功能性元素，大型语言模型（LLM）在执行软件工程（SE）任务时却频繁依赖它们。然而，这种依赖在模型中的具体体现位置，以及其对性能的影响机制，目前仍缺乏清晰理解。本文首次从概念层面开展对LLM在SE应用中的可解释性研究，通过内部注释表征的视角，分析了代码补全、代码转换和代码优化三项任务。我们采用概念激活向量（CAV）方法，发现LLM不仅将注释作为独立的潜在概念内化于模型之中，还能区分不同子类型，如Javadoc注释、行内注释和多行注释。通过系统地在模型的嵌入空间中激活或抑制这些概念，我们观察到性能出现显著变化，且具有模型特异性和任务依赖性，性能波动范围从-90%到+67%。最后，我们设计了一项受控实验，使用相同的代码输入，引导LLM完成10种不同的SE任务，并测量其潜在表示中注释概念的激活程度。结果表明，代码摘要任务始终引发最强的注释概念激活，而代码补全任务则表现出最弱的敏感性。这些发现为构建新型SE工具与模型开辟了新方向：即不再仅依赖表面输入，而是能够推理并操作模型内部的概念表示。"
  },
  {
    "date": "2025-12-18",
    "title": "BrepLLM: Native Boundary Representation Understanding with Large Language Models",
    "authors": "Liyuan Deng, Hao Guo, Yunpeng Bai, Yongkang Dai, Huaxi Huang, Yilei Shi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16413v1",
    "source": "arXiv",
    "abstract": "Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.",
    "title_zh": "BrepLLM：基于大语言模型的原生边界表示理解",
    "abstract_zh": "当前基于token序列的大语言模型（LLMs）难以直接处理包含复杂几何与拓扑信息的3D边界表示（Brep）模型。为此，我们提出了BrepLLM，这是首个使大语言模型能够解析和推理原始Brep数据的框架，成功弥合了结构化3D几何与自然语言之间的模态鸿沟。BrepLLM采用两阶段训练流程：跨模态对齐预训练与多阶段LLM微调。\n\n在第一阶段，我们提出一种自适应UV采样策略，将Brep模型转换为包含几何与拓扑信息的图结构表示。随后，设计了一种分层的BrepEncoder，用于从几何要素（如面和边）及拓扑关系中提取特征，生成一个全局单一token以及一系列节点token。接着，通过对比学习方法，将该全局token与冻结的CLIP文本编码器（ViT-L/14）输出的文本嵌入进行对齐。\n\n在第二阶段，我们将预训练的BrepEncoder集成至大语言模型中，并采用三阶段渐进式训练策略对节点token序列进行对齐：(1) 训练一个基于MLP的语义映射模块，将Brep表示映射到二维空间，并引入2D-LLM先验知识；(2) 对LLM进行微调；(3) 设计混合查询专家（Mixture-of-Query Experts, MQE）机制，以增强对几何多样性的建模能力。\n\n此外，我们构建了Brep2Text数据集，包含269,444个Brep-文本问答对。实验结果表明，BrepLLM在3D物体分类与描述生成任务上均达到了当前最优（SOTA）性能。"
  },
  {
    "date": "2025-12-18",
    "title": "Using a Sledgehammer to Crack a Nut? Revisiting Automated Compiler Fault Isolation",
    "authors": "Yibiao Yang, Qingyang Li, Maolin Sun, Jiangchang Wu, Yuming Zhou",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16335v1",
    "source": "arXiv",
    "abstract": "Background: Compilers are fundamental to software development, translating high-level source code into executable software systems. Faults in compilers can have severe consequences and thus effective localization and resolution of compiler bugs are crucial. Problem: In practice, developers often examine version history to identify and investigate bug-inducing commit (BIC) for fixing bugs. However, while numerous sophisticated Spectrum-Based Fault Localization (SBFL) techniques have been proposed for compiler fault isolation, their effectiveness has not been evaluated against the BIC-based strategies widely adopted in practice. Objective: This study aims to bridge this gap by directly comparing a BIC-based strategy, Basic, with representative SBFL techniques in the context of compiler fault localization. The BIC-based strategy closely aligns with common developer practices, as it directly identifies the BIC and treats the files modified in that commit as faulty candidates. Method: The Basic identifies the most recent good release and earliest bad release, and then employs a binary search to pinpoint the bug-inducing commit. All files modified in the identified commit are flagged as potentially faulty. We rigorously compare Basic against SBFL-based techniques using a benchmark consisting of 60 GCC bugs and 60 LLVM bugs. Result: Our analysis reveals that Basic performs comparably to, and in many cases outperforms, state-of-the-art SBFL-based techniques, particularly on the critical Top-1 and Top-5 ranking metrics. Conclusion: This study provides new insights into the practical effectiveness of SBFL-based techniques in real-world compiler debugging scenarios. We recommend that future research adopt Basic as a baseline when developing and evaluating new compiler fault isolation methods.",
    "title_zh": "以大锤砸坚果？重新审视自动化编译器故障定位",
    "abstract_zh": "背景：编译器在软件开发中起着基础性作用，负责将高级源代码转换为可执行的软件系统。编译器中的缺陷可能带来严重后果，因此有效定位和修复编译器错误至关重要。  \n问题：在实际开发中，开发者通常通过检查版本历史来识别并调查引发缺陷的提交（BIC）以修复问题。然而，尽管已有大量先进的基于谱的故障定位（SBFL）技术被提出用于编译器故障隔离，其效果尚未与实践中广泛采用的基于BIC的策略进行对比评估。  \n目标：本研究旨在填补这一空白，直接比较一种基于BIC的策略（Basic）与代表性SBFL技术在编译器故障定位场景下的表现。该BIC策略与开发者的常规实践高度一致，因为它直接定位BIC，并将该提交所修改的文件视为潜在故障候选。  \n方法：Basic策略首先确定最近一次正常发布的版本和最早一次出现错误的版本，随后采用二分查找法精确定位引发缺陷的提交。所有在该提交中被修改的文件均被标记为潜在故障文件。我们使用包含60个GCC缺陷和60个LLVM缺陷的基准数据集，对Basic与基于SBFL的技术进行了严格对比分析。  \n结果：我们的分析表明，Basic在多数情况下表现不逊于甚至优于当前最先进的SBFL技术，尤其在关键的Top-1和Top-5排名指标上表现突出。  \n结论：本研究为SBFL技术在真实编译器调试场景中的实际有效性提供了新的洞见。我们建议未来的研究在开发和评估新型编译器故障隔离方法时，将Basic作为基准方法。"
  },
  {
    "date": "2025-12-18",
    "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
    "authors": "Yuxuan Qiao, Dongqin Liu, Hongchang Yang, Wei Zhou, Songlin Hu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16310v1",
    "source": "arXiv",
    "abstract": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
    "title_zh": "代理工具编排泄露问题加剧：数据集、基准测试与缓解措施",
    "abstract_zh": "在大型语言模型的驱动下，单智能体多工具架构因其简洁性和高效性，已成为自主智能体的流行范式。然而，该架构也引入了一种新型且严重的隐私风险，我们将其称为“工具编排隐私风险”（Tools Orchestration Privacy Risk, TOP-R）：即智能体为实现一个良性的用户目标，会自主地跨多个工具聚合信息片段，并利用其推理能力合成出意料之外的敏感信息。本文首次对这一风险进行了系统性研究。首先，我们建立了一个形式化框架，将该风险的根本原因归结为智能体目标函数的错位——过度优化“有用性”，而忽视了隐私保护意识。其次，我们构建了TOP-Bench基准测试集，包含成对的泄露场景与良性场景，以全面评估该风险。为量化安全与鲁棒性之间的权衡，我们提出了H-Score这一综合指标。评估结果表明，TOP-R是一种严重风险：八种代表性模型的平均风险泄露率（RLR）高达90.24%，而平均H-Score仅为0.167，没有任何模型超过0.3。最后，我们提出隐私增强原则（Privacy Enhancement Principle, PEP）方法，有效缓解了TOP-R，将风险泄露率降低至46.58%，并将H-Score显著提升至0.624。我们的工作不仅揭示了一类新的风险以及当前智能体架构固有的结构性局限，同时也提供了切实可行的缓解策略。"
  },
  {
    "date": "2025-12-18",
    "title": "Checking the HAL Interface Specification Continuously, Right from the Start",
    "authors": "Manuel Bentele, Onur Altinordu, Jan Körner, Andreas Podelski, Axel Sikora",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16897v1",
    "source": "arXiv",
    "abstract": "The correct use of a Hardware Abstraction Layer (HAL) interface in embedded applications is crucial to prevent malfunctions, crashes, or even hardware damage. Software model checking has been successfully applied to check interface specifications in application programs, but its employment in industrial practice is hindered by its unpredictability (whether it succeeds for a given application program or not). In this paper, we present a novel approach to address this problem by checking the HAL interface specification continuously and right from the start of the development. I.e., we develop an embedded application in several iterations without a formal connection between the steps. The steps start from a program skeleton which does nothing but calling HAL functions. Actual functionality is added consecutively. The HAL interface specification is checked in each step of the sequence. The idea of the approach is to exploit a specific feature of software model checking: Its attempt to compute exactly the abstraction that is needed for the check to succeed may carry over from one step to the next, even if there is no formal connection between the steps. The experience from a preliminary experimental evaluation of our approach in the development of embedded applications is very promising. Following our approach, the check succeeds in each step and in particular in the final application program.",
    "title_zh": "从一开始就持续检查HAL接口规范",
    "abstract_zh": "在嵌入式应用中正确使用硬件抽象层（HAL）接口对于防止系统故障、崩溃甚至硬件损坏至关重要。软件模型检查已被成功应用于验证应用程序中的接口规范，但在工业实践中其应用受到限制，主要原因是其结果具有不可预测性（即无法确定对某个特定应用程序是否能够成功完成检查）。本文提出了一种新方法，通过从开发初期就开始持续地、及时地检查HAL接口规范，以解决这一问题。具体而言，我们采用多轮迭代的方式开发嵌入式应用，各步骤之间并无正式的连接关系。开发过程始于一个仅调用HAL函数的程序骨架，不包含任何实际功能；随后逐步添加实际功能。在每一轮迭代中，均对HAL接口规范进行检查。该方法的核心思想在于利用软件模型检查的一个特殊特性：即使各步骤之间没有正式关联，其为使检查成功而尝试计算出的精确抽象，也可能在后续步骤中得以延续和复用。初步实验评估表明，该方法在嵌入式应用开发中的应用前景非常乐观——按照此方法，每一阶段的检查均能成功，包括最终的应用程序本身。"
  },
  {
    "date": "2025-12-18",
    "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
    "authors": "Hao Liang, Xiaochen Ma, Zhou Liu, Zhen Hao Wong, Zhengyang Zhao, Zimo Meng, Runming He, Chengyu Shen, Qifeng Cai, Zhaoyang Han, Meiyi Qiang, Yalin Feng, Tianyi Bai, Zewei Pan, Ziyi Guo, Yizhen Jiang, Jingwen Deng, Qijie You, Peichao Lai, Tianyu Guo, Chi Hsu Tsai, Hengyi Feng, Rui Hu, Wenkai Yu, Junbo Niu, Bohan Zeng, Ruichuan An, Lu Ma, Jihao Huang, Yaowei Zheng, Conghui He, Linpeng Tang, Bin Cui, Weinan E, Wentao Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16676v1",
    "source": "arXiv",
    "abstract": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
    "title_zh": "数据流：面向以数据为中心的人工智能时代，由大语言模型驱动的统一数据准备与工作流自动化框架",
    "abstract_zh": "大型语言模型（LLM）对高质量数据的快速增长需求，使得构建可扩展、可靠且语义丰富的数据准备流水线变得愈发迫切。然而，当前的数据处理实践仍主要依赖于临时编写的脚本和缺乏明确定义的工作流程，这些方法缺乏系统性抽象，难以保证可复现性，并且在“模型参与式”数据生成方面支持有限。为应对这些挑战，我们提出了DataFlow——一个统一且可扩展的、由大语言模型驱动的数据准备框架。DataFlow采用系统级抽象设计，支持模块化、可重用且可组合的数据转换操作，并提供类似PyTorch风格的流水线构建API，便于调试与优化数据流。该框架包含近200个可复用的操作符，以及覆盖文本、数学推理、代码生成、Text-to-SQL、智能代理RAG和大规模知识提取等领域的六类通用领域流水线。为进一步提升易用性，我们引入了DataFlow-Agent，它能够通过操作符合成、流水线规划与迭代验证，将自然语言描述自动转化为可执行的数据流水线。在六个代表性应用场景中，DataFlow均显著提升了下游LLM的性能表现：其数学、代码和文本流水线超越了精心筛选的人工数据集及专用合成基线，在Text-to-SQL任务上相较SynSQL实现最高达+3%的执行准确率提升；在代码基准测试中平均提升+7%；在MATH、GSM8K和AIME等评测集上取得1至3个百分点的增益。此外，由DataFlow生成的统一10,000样本数据集，已使基础模型性能超越使用100万条Infinity-Instruct数据训练的同类模型。这些结果表明，DataFlow为可靠、可复现且可扩展的LLM数据准备提供了高效实用的基础支撑，并为未来以数据为中心的人工智能发展奠定了系统级基石。"
  },
  {
    "date": "2025-12-18",
    "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
    "authors": "Xiao Li, Yue Li, Hao Wu, Yue Zhang, Yechao Zhang, Fengyuan Xu, Sheng Zhong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16538v1",
    "source": "arXiv",
    "abstract": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
    "title_zh": "基于大语言模型的漏洞检测中的代码混淆系统研究",
    "abstract_zh": "随着大型语言模型（LLMs）在代码漏洞检测中的日益应用，其在多种漏洞类型下的可靠性与鲁棒性已成为一个紧迫问题。在传统的对抗性场景中，代码混淆长期以来被用作一种通用策略，以绕过审计工具，同时保持漏洞的可利用性而不修改工具本身。尽管已有大量研究探索了各种混淆方法与工具，但它们在支持的技术、粒度以及编程语言方面存在差异，导致难以系统评估其对基于LLM的漏洞检测的影响。为填补这一空白，我们提出了一种结构化的混淆技术体系化方法，并在统一框架下对其进行评估。具体而言，我们将现有的混淆方法分为三大类（布局、数据流和控制流），涵盖11个子类别及19种具体技术。我们采用一致的LLM驱动方法，在四种编程语言（Solidity、C、C++ 和 Python）中实现了这些技术，并评估了它们对15个LLM（涵盖DeepSeek、OpenAI、Qwen和LLaMA四个模型系列）以及两个编码代理（GitHub Copilot 和 Codex）的影响。研究结果揭示了代码混淆对基于LLM的漏洞检测既可能带来积极影响，也可能造成负面影响，凸显了在何种条件下混淆会提升或降低检测性能。我们进一步结合漏洞特征、代码属性和模型特性对这些结果进行了深入分析。最后，我们指出了若干开放性问题，并提出了未来提升LLM在真实世界漏洞检测中鲁棒性的若干研究方向。"
  },
  {
    "date": "2025-12-18",
    "title": "XTC, A Research Platform for Optimizing AI Workload Operators",
    "authors": "Pompougnac Hugo, Guillon Christophe, Noiry Sylvain, Dutilleul Alban, Iooss Guillaume, Rastello Fabrice",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16512v1",
    "source": "arXiv",
    "abstract": "Achieving high efficiency on AI operators demands precise control over computation and data movement. However, existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across frameworks. No unified interface currently decouples scheduling specification from code generation and measurement. We introduce XTC, a platform that unifies scheduling and performance evaluation across compilers. With its common API and reproducible measurement framework, XTC enables portable experimentation and accelerates research on optimization strategies.",
    "title_zh": "XTC：用于优化AI工作负载算子的研究平台",
    "abstract_zh": "在AI算子上实现高效性能，需要对计算和数据移动进行精确控制。然而，现有的调度语言通常被锁定在特定的编译器生态中，导致不同框架之间难以进行公平比较、代码复用和评估。目前尚无统一接口能够将调度规范与代码生成及性能测量相分离。为此，我们提出了XTC平台，该平台实现了跨编译器的调度与性能评估的统一。凭借其通用API和可复现的测量框架，XTC支持可移植的实验，加速了优化策略的研究进程。"
  },
  {
    "date": "2025-12-18",
    "title": "Large Language Models as a (Bad) Security Norm in the Context of Regulation and Compliance",
    "authors": "Kaspar Rosager Ludvigsen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16419v1",
    "source": "arXiv",
    "abstract": "The use of Large Language Models (LLM) by providers of cybersecurity and digital infrastructures of all kinds is an ongoing development. It is suggested and on an experimental basis used to write the code for the systems, and potentially fed with sensitive data or what would otherwise be considered trade secrets. Outside of these obvious points, this paper asks how AI can negatively affect cybersecurity and law when used for the design and deployment of security infrastructure by its developers. Firstly, the paper discusses the use of LLMs in security, either directly or indirectly, and briefly tackles other types of AI. It then lists norms in cybersecurity, then a range of legal cybersecurity obligations from the European Union, to create a frame of reference. Secondly, the paper describes how LLMs may fail to fulfil both legal obligations and best practice in cybersecurity is given, and the paper ends with some economic and practical consequences for this development, with some notions of solutions as well. The paper finds that using LLMs comes with many risks, many of which are against good security practice, and the legal obligations in security regulation. This is because of the inherent weaknesses of LLMs, most of which are mitigated if replaced with symbolic AI. Both also have issues fulfilling basic traceability obligations and practice. Solutions are secondary systems surrounding LLM based AI, fulfilment of security norms beyond legal requirements and simply not using such technology in certain situations.",
    "title_zh": "大型语言模型作为监管与合规背景下的（不良）安全标准",
    "abstract_zh": "网络安全和各类数字基础设施提供商对大型语言模型（LLM）的应用正处于持续发展之中。目前，LLM已被建议并以实验性方式用于系统代码的编写，甚至可能被输入敏感数据或本应被视为商业秘密的信息。除了这些显而易见的问题之外，本文还探讨了当开发者在设计和部署安全基础设施时使用人工智能（AI），其可能对网络安全与法律带来的负面影响。首先，文章讨论了LLM在安全领域的直接或间接应用，并简要涉及其他类型的人工智能。接着，文章列举了网络安全领域的一系列规范，并梳理了来自欧盟的一系列法律合规义务，以此构建一个参考框架。其次，文章分析了LLM在履行法律义务及网络安全最佳实践方面可能存在的不足之处，并最终探讨了这一发展趋势所带来的经济与实际后果，同时提出了一些应对思路。研究发现，使用LLM存在诸多风险，其中许多风险违背了良好的安全实践以及安全监管中的法律要求。这主要源于LLM固有的缺陷，而这些缺陷若改用符号式人工智能（symbolic AI）则可得到很大程度缓解。此外，无论是LLM还是符号式AI，在满足基本可追溯性义务和实践方面均存在挑战。因此，解决方案包括：建立围绕基于LLM的人工智能的辅助系统、超越法律要求的安全规范执行，以及在某些情境下干脆不采用此类技术。"
  },
  {
    "date": "2025-12-18",
    "title": "GFLAN: Generative Functional Layouts",
    "authors": "Mohamed Abouagour, Eleftherios Garyfallidis",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16275v1",
    "source": "arXiv",
    "abstract": "Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.",
    "title_zh": "GFLAN：生成式功能布局",
    "abstract_zh": "自动化的平面图生成位于组合搜索、几何约束满足与功能设计需求的交汇处——这一复杂融合长期以来难以获得统一的计算处理。尽管近期的深度学习方法已显著提升了该领域的技术水平，但它们往往难以捕捉建筑设计中的深层逻辑：拓扑关系优先于几何实现、功能约束通过邻接网络传播，以及从局部连接决策中涌现出的通行流线模式。为应对这些根本性挑战，本文提出GFLAN（Generative Floorplan Layout with Architecture-aware Neural networks），一种通过显式分解为拓扑规划与几何实现两个阶段来重构平面图生成过程的生成框架。给定单一外部边界和前门位置，我们的方法摒弃了传统的像素到像素或墙体追踪式的直接生成方式，转而采用一种严谨的两阶段范式。第一阶段（Stage A）采用专用的卷积架构，配备双编码器——分别分离不变的空间上下文与动态演化的布局状态——通过在可行位置上的离散概率图，逐步将房间中心点分配至建筑围合区域内。第二阶段（Stage B）构建一个异构图结构，将房间节点与边界顶点相连接，并引入增强型Transformer的图神经网络（GNN），联合回归出各房间的边界。"
  },
  {
    "date": "2025-12-18",
    "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
    "authors": "Ora Nova Fandina, Eitan Farchi, Shmulik Froimovich, Raviv Gal, Wesam Ibraheem, Rami Katan, Alice Podolsky",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16272v1",
    "source": "arXiv",
    "abstract": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities. To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked. Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
    "title_zh": "超越盲点：缓解基于大语言模型评估陷阱的分析提示",
    "abstract_zh": "大型语言模型作为代码生成流水线中的评估者（LaaJ）正日益普及。尽管其在可扩展性方面具有吸引力，但LaaJ往往忽视领域特定问题，这引发了人们对其在关键评估任务中可靠性方面的担忧。为了更深入理解这些局限性在实际应用中的表现，我们以一个具体的工业应用场景——通过COBOL代码生成实现遗留系统现代化——为例，考察LaaJ的行为。研究发现，即使已部署于生产环境的LaaJ也可能会遗漏关键的领域错误，暴露出其评估能力中存在持续性的盲点。\n\n为更好地理解这些盲点，我们分析了生成的COBOL程序及其对应的LaaJ评价结果，并结合专家知识构建了一个初步的分类体系。基于该分类体系，我们开发了一款轻量级分析检查工具，能够识别出实践中观察到的30多个领域相关问题。我们将该工具的输出作为分析提示，动态注入到评估者的提示中，以促使LaaJ重新审视可能被忽略的方面。\n\n在包含100个程序的测试集上，对四个生产级别的LaaJ进行实验表明：仅依靠LaaJ时，平均只能检测到约45%的代码错误（在所有测试的评估者中均如此）；而单独使用分析检查工具则缺乏解释深度。当两者结合使用时，LaaJ+提示（Hints）配置下，最高可实现94%的错误覆盖率（针对表现最佳的评估者和提示注入方式），并生成质量更高、更准确的解释。这一结果表明，分析型与大语言模型相结合的混合方法，能够显著提升部署流水线中评估的可靠性。\n\n我们已公开发布该数据集及所有使用的提示文本。"
  },
  {
    "date": "2025-12-18",
    "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
    "authors": "Chenkai Xu, Yijie Jin, Jiajun Li, Yi Tu, Guoping Long, Dandan Tu, Tianqi Hou, Junchi Yan, Zhijie Deng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16229v1",
    "source": "arXiv",
    "abstract": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
    "title_zh": "LoPA：通过前瞻并行解码实现大语言模型推理的扩展",
    "abstract_zh": "扩散型大语言模型（dLLMs）在高速推理方面展现了巨大的潜力。然而，当前基于置信度的解码策略受限于有限的并行性，通常每轮前向传播仅能生成1–3个词元（TPF）。本文中，我们发现dLLM推理过程中的并行度高度依赖于词元填充顺序（TFO）。为此，我们提出了一种无需训练、即插即用的算法——前瞻并行解码LoPA（Lookahead PArallel Decoding LoPA），用于识别更优的TFO，从而显著加速推理过程。LoPA通过并行分支同时探索多种候选TFO，并根据各分支的置信度选择未来具有更高并行潜力的方案。我们将LoPA应用于当前最先进的D2F模型，在实验中观察到解码效率的显著提升。值得注意的是，LoPA将D2F-Dream在GSM8K数据集上的TPF提升至10.1，同时保持了优于Dream基线的性能表现。为进一步支持这种前所未有的高并行度，我们设计了一种专用的多设备推理系统，引入分支并行（Branch Parallelism, BP）机制，在多GPU部署下实现了单样本每秒1073.9个词元的吞吐量。相关代码已开源，地址为：https://github.com/zhijie-group/LoPA。"
  },
  {
    "date": "2025-12-18",
    "title": "PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving",
    "authors": "Jianming Liu, Ren Zhu, Jian Xu, Kun Ding, Xu-Yao Zhang, Gaofeng Meng, Cheng-Lin Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16214v1",
    "source": "arXiv",
    "abstract": "Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.",
    "title_zh": "PDE-Agent：一种工具链增强的多智能体框架，用于求解偏微分方程",
    "abstract_zh": "求解偏微分方程（PDE）是工程与科学研究的核心任务之一。传统的PDE求解方法繁琐复杂，依赖人工建模和领域专业知识。尽管物理信息神经网络（PINNs）提出了基于神经网络的端到端解决方案，而DeepXDE等框架进一步提升了自动化程度，但这些方法仍然需要专家知识介入，缺乏完全的自主性。在本研究中，我们首次将PDE求解问题转化为由大语言模型（LLM）驱动的智能体调用工具的过程，并提出PDE-Agent——首个基于工具链增强的多智能体协作框架。该框架融合了LLM的推理能力与外部工具的可控性，实现了从自然语言描述到自动PDE求解的全流程自动化。\n\nPDE-Agent通过两项关键创新，充分发挥多智能体与多工具协同的优势：（1）提出一种基于图记忆的Prog-Act框架，支持多智能体协作，通过双循环机制（局部修正与全局重构）实现高效的动态规划与错误纠正；（2）设计集成工具参数分离机制的Resource-Pool，实现多工具协作的集中化管理，有效解决了现有框架中存在的工具间依赖关系不明确、运行时资源管理混乱等问题。\n\n为验证并评估这一新型PDE求解范式，我们构建了PDE-Bench——一个面向代理驱动的工具协同求解的多类型PDE基准测试集，并提出了多层次的评估指标以衡量工具协调能力。实验结果表明，PDE-Agent在复杂、多步骤且跨步骤依赖的任务中展现出卓越的适用性与性能表现。这一基于工具链增强的多智能体PDE求解新范式，将有力推动自动化科学计算领域的未来发展。我们的源代码与数据集将公开发布，供学术界与工业界使用。"
  },
  {
    "date": "2025-12-18",
    "title": "Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems",
    "authors": "Muzeeb Mohammad",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16146v1",
    "source": "arXiv",
    "abstract": "Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.",
    "title_zh": "Apache Kafka事件流系统中设计模式与基准实践的分析",
    "abstract_zh": "Apache Kafka 已成为高吞吐事件流处理的基础平台，广泛应用于实时分析、金融交易处理、工业遥测以及大规模数据驱动系统。尽管其技术成熟度高且应用广泛，但关于可复用的架构设计模式和可重现的基准测试方法的研究，仍分散于学术界与工业界的各类出版物中，缺乏系统整合。本文对2015年至2025年间发表的42篇同行评审研究进行了结构化综述，识别出九种在Kafka系统中反复出现的设计模式，包括日志压缩、CQRS总线、精确一次处理管道、变更数据捕获、流表连接、Saga编排、分层存储、多租户主题以及事件溯源重放。分析深入探讨了这些模式的共现趋势、特定领域的部署实践，以及基于标准基准套件（如TPCx Kafka和Yahoo Streaming Benchmark）和自定义工作负载的实证基准测试方法。研究揭示了在配置披露、评估严谨性及可复现性方面存在显著不一致，严重制约了不同研究之间的比较以及实际系统的复制实施。本文通过建立统一的分类体系、模式基准矩阵以及实用的决策启发式规则，为架构师与研究人员设计可复现、高性能且容错性强的Kafka事件流系统提供了切实可行的指导。"
  },
  {
    "date": "2025-12-18",
    "title": "Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers",
    "authors": "Thanh Dat Hoang, Thanh Tam Nguyen, Thanh Trung Huynh, Hongzhi Yin, Quoc Viet Hung Nguyen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16083v1",
    "source": "arXiv",
    "abstract": "Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \\toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \\toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.",
    "title_zh": "通过基于函数依赖图重排序器的LLM高效模式过滤实现Text2SQL的扩展",
    "abstract_zh": "大多数现代Text2SQL系统会将完整的数据库模式（主要为列信息）与用户问题一同作为提示输入大型语言模型（LLMs）。尽管在小型数据库上表现良好，但该方法在真实世界中规模庞大的数据库上却难以奏效，甚至对商业级大模型也超出其上下文长度限制。近期的Spider 2.0基准测试便凸显了这一问题：其包含数百张表和数万列，现有系统在此类复杂模式下往往失效。当前的缓解方案要么依赖于成本高昂的多步提示流水线，要么通过独立地将列与用户问题进行相关性排序来筛选列，却忽略了列之间的内在结构关系。\n\n为实现现有系统的可扩展性，我们提出了\\toolname——一个开源且高效利用LLM的模式过滤框架。该框架通过以下三个步骤压缩Text2SQL提示：(i) 使用一种查询感知的LLM编码器，结合列值和元数据对列进行排序；(ii) 通过轻量级图Transformer对具有功能依赖关系的列进行二次重排序，捕捉列间的关联性；(iii) 采用斯坦纳树启发式算法，选取保持连通性的子模式。实验结果表明，在真实数据集上，\\toolname实现了近乎完美的召回率，并在精度上优于CodeS、SchemaExP、Qwen重排序器以及嵌入检索方法，同时保持亚秒级的中位延迟，可支持超过23,000列的大规模模式。我们的源代码已公开，地址为：https://github.com/thanhdath/grast-sql。"
  },
  {
    "date": "2025-12-18",
    "title": "LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)",
    "authors": "Xin Wang, Zhenhao Li, Zishuo Ding",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16070v1",
    "source": "arXiv",
    "abstract": "The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.",
    "title_zh": "LLM4Perf：大型语言模型在多目标性能建模中是有效的采样器（复制）",
    "abstract_zh": "现代软件系统的性能高度依赖于其复杂的配置选项。要在这片广阔的配置空间中构建准确的性能模型，需要有效的采样策略，然而现有方法在多目标优化方面往往表现不佳，且无法利用文档中的语义信息。大型语言模型（LLMs）近年来的成功，促使本文提出一个核心问题：大型语言模型能否作为多目标性能建模的有效采样器？为探究这一问题，我们开展了一项全面的实证研究，系统考察了基于LLM的采样能力及其特性。我们设计并实现了LLM4Perf——一种基于反馈的框架，并利用它对四个高度可配置的真实世界系统中的LLM引导采样过程进行了系统评估。研究结果表明，在大多数情况下，LLM引导的方法优于传统基线方法。定量分析显示，LLM4Perf在全部112个评估场景中，有77个（约占68.8%）取得了最佳性能，充分证明了其卓越的有效性。我们发现，这种有效性源于LLM在配置空间剪枝和基于反馈的策略优化两方面的双重能力。进一步验证表明，这种剪枝机制同样能提升基线方法的性能，使其在448个案例中的410个（约91.5%）中表现更优。此外，我们还揭示了LLM4Perf中各组件及超参数选择对整体效果的影响。总体而言，本文为LLM在性能工程中的有效性提供了有力证据，并深入揭示了其成功背后的作用机制。"
  },
  {
    "date": "2025-12-18",
    "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation",
    "authors": "William English, Chase Walker, Dominic Simon, Rickard Ewetz",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16770v1",
    "source": "arXiv",
    "abstract": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.",
    "title_zh": "GinSign：将自然语言嵌入系统签名以实现时序逻辑翻译",
    "abstract_zh": "自然语言（NL）到时序逻辑（TL）的翻译使工程师能够在不手动编写形式规范的情况下，指定、验证和强制执行系统行为——这对于构建可信的自主系统至关重要。尽管现有的NL到TL翻译框架已展现出令人鼓舞的初步成果，但这些系统要么明确假设能够访问准确的原子命题语义映射（atom grounding），要么在语义映射的准确性方面表现不佳。本文提出了一种名为GinSign（Grounding Natural Language Into System Signatures for Temporal Logic translation）的框架，旨在将自然语言精准地映射到系统签名中。该框架引入了一个接地模型，学习一种抽象任务：将自然语言片段映射到给定的系统签名上。具体而言，在给定一个抽象化的自然语言规范和系统签名 $\\mathcal{S}$ 的情况下，分类器需将每个抽象的原子命题分配至由签名定义的原子集合 $\\mathcal{P}$ 中的某个元素。我们采用分层式的方法分解这一接地任务：首先预测谓词标签，然后选择适当类型的常量参数。通过将原本自由生成的问题转化为结构化的分类问题，该方法使得使用更小规模的掩码语言模型成为可能，并消除了对昂贵的大语言模型（LLM）的依赖。在多个领域的实验表明，忽略接地步骤的现有框架往往生成语法正确但语义上与目标表达式不等价的抽象LTL公式；而我们的框架则支持下游模型检测，并实现了高达95.5%的语义等价率，相比当前最优方法（SOTA）提升了1.4倍。"
  },
  {
    "date": "2025-12-18",
    "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant",
    "authors": "Sören Auer, Allard Oelen, Mohamad Yaser Jaradeh, Mutahira Khalid, Farhana Keya, Sasi Kiran Gaddipati, Jennifer D'Souza, Lorenz Schlüter, Amirreza Alasti, Gollam Rabby, Azanzi Jiomekong, Oliver Karras",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16447v1",
    "source": "arXiv",
    "abstract": "The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.",
    "title_zh": "面向人工智能支持的研究：TIB AI助手的愿景",
    "abstract_zh": "生成式人工智能和大型语言模型的快速发展，有望彻底改变研究方式，为增强学术工作流程提供前所未有的机遇。然而，由于各学科领域需求差异大、AI素养普遍有限、工具与智能体协调复杂，以及生成式AI在科研中准确性的不确定性，将AI有效融入研究仍面临挑战。本文提出了TIB AIssistant的愿景——一个跨领域的、人机协同的研究平台，旨在支持各学科研究人员开展科学发现，通过AI助手协助完成研究全生命周期中的各类任务。该平台提供模块化组件，包括提示词与工具库、共享数据存储以及灵活的编排框架，共同助力研究构思、文献分析、方法设计、数据分析及学术写作等环节。本文详细阐述了该平台的概念框架、系统架构及早期原型的实现，展示了该方法的可行性及其潜在影响力。"
  },
  {
    "date": "2025-12-18",
    "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
    "authors": "William English, Dominic Simon, Sumit Kumar Jha, Rickard Ewetz",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16814v1",
    "source": "arXiv",
    "abstract": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
    "title_zh": "使用大语言模型将自然语言强制转换为时序逻辑的语法",
    "abstract_zh": "将自然语言（NL）翻译为形式化语言（如时序逻辑TL），是人类与机器人及自主系统进行沟通的关键环节。当前最先进的方法通常将该任务分解为两个阶段：原子命题（APs）的提升阶段和翻译阶段。然而，现有方法在准确的提升、共指现象的存在以及从有限数据中学习等方面仍面临挑战。本文提出一种名为语法强制翻译（Grammar Forced Translation, GraFT）的NL到TL翻译框架。该框架基于一个观察：以往的工作通过让语言模型从其完整词汇表中迭代预测标记来解决提升和翻译两个步骤。相比之下，GraFT通过在每一步中将有效输出标记集从完整词汇表缩减为极少数几个，从而降低了两个任务的复杂性。这种解空间的缩减是通过对每个问题的独特性质加以利用而实现的。此外，我们还提供了理论依据，说明为何解空间的缩减能够带来更高效的训练。我们在CW、GLTL和Navi三个基准测试上评估了GraFT的有效性。与现有的先进翻译方法相比，GraFT在端到端翻译准确率上平均提升了5.49%，在跨领域翻译准确率上平均提升了14.06%。"
  },
  {
    "date": "2025-12-18",
    "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
    "authors": "Claudia Vale Oliveira, Nelson Zagalo, Filipe Silva, Anabela Brandao, Syeda Faryal Hussain Khurrum, Joaquim Santos",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16750v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
    "title_zh": "可信性即失败：大语言模型与人类如何共同构建认知错误",
    "abstract_zh": "大型语言模型（LLMs）在日常推理中正日益作为认知伙伴被广泛使用，然而其错误仍主要通过预测性指标进行分析，而非考察其对人类判断的解释性影响。本研究探讨了在人机交互中，不同形式的认知失败如何产生、被掩盖并被容忍，其中“失败”被理解为由模型生成的合理性与人类解释性判断共同塑造的一种关系性断裂。我们通过三轮多模型评估，采用跨学科任务和逐步细化的评估框架，观察评估者如何在语言、认知和可信度维度上解读模型输出。研究发现，LLM的错误从预测性层面逐渐演变为诠释性层面：语言流畅性、结构连贯性以及表面上合理的引用，掩盖了深层的意义扭曲。评估者频繁混淆正确性、相关性、偏见、依据性与一致性等标准，表明人类判断将原本应清晰区分的分析维度简化为受形式与流畅性影响的直觉启发式策略。在各轮次中，我们观察到系统性的验证负担与认知漂移现象：随着任务复杂度增加，评估者越来越依赖表面线索，导致错误但结构良好的回答被误认为可信。这些结果表明，错误不仅是模型行为的属性，更是生成性合理性和人类解释捷径共同建构的结果。因此，理解人工智能的认知失败，需要将评估重新构想为一种关系性的解释过程，使系统失效与人类误判之间的界限变得模糊。本研究对LLM评估、数字素养培养以及可信赖的人机沟通设计具有重要启示。"
  },
  {
    "date": "2025-12-18",
    "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles",
    "authors": "Allard Oelen, Sören Auer",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16442v1",
    "source": "arXiv",
    "abstract": "The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.",
    "title_zh": "TIB AI助手：支持全研究生命周期的AI研究平台",
    "abstract_zh": "人工智能（AI）尤其是大型语言模型（LLMs）的迅速普及，正在对社会各个领域产生广泛影响，学术界也不例外。基于AI的研究支持系统具有潜力，能够协助研究人员完成整个研究生命周期中的各类任务。本文展示了TIB AIssistant——一个支持研究全生命周期的AI辅助研究平台。该AIssistant由一系列专门负责不同研究任务的智能助手组成，并提供工具以接入外部学术服务。生成的数据将存储在资产库中，可导出为RO-Crate包，以确保研究过程的透明性并提升研究的可重复性。我们通过一系列连续的操作演示了AIssistant的核心功能：多个助手相互协作，共同生成研究论文的初稿章节。最终，借助AIssistant，我们为构建一个由社区共同维护的AI支持型研究平台奠定了基础。"
  },
  {
    "date": "2025-12-18",
    "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
    "authors": "Qizhou Chen, Chengyu Wang, Taolin Zhang, Xiaofeng He",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16227v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
    "title_zh": "一种信息论框架下的鲁棒大型语言模型编辑方法",
    "abstract_zh": "大型语言模型（LLMs）已成为科学、技术和社会领域不可或缺的工具，推动了多个领域的变革性进展。然而，模型中存在错误或过时信息会削弱其准确性，并限制其安全部署。如何在不进行全量重新训练所带来的高昂成本和系统中断的前提下，高效地更新模型知识，仍是亟待解决的关键挑战。当前的模型编辑技术往往难以将修正效果推广到更广泛的领域，容易引发意外后果，从而制约了其实际应用价值。本文提出一种基于信息瓶颈理论的新颖框架，用于语言模型的编辑。该方法能够精确压缩并隔离实现通用知识修正所必需的核心信息，同时最大限度减少对无关模型行为的干扰。在此基础上，我们提出了信息瓶颈知识编辑器（IBKE），利用紧凑的潜在表示来指导基于梯度的更新，从而实现鲁棒且具有广泛适用性的模型编辑。我们在多种主流LLM架构及标准基准任务上验证了IBKE的有效性，结果表明其在准确率、编辑泛化性和特异性方面均达到当前最优水平。这些发现建立了一种理论严谨且实用的开放域知识编辑范式，显著提升了大型语言模型在真实应用场景中的实用性与可信度。"
  },
  {
    "date": "2025-12-18",
    "title": "Science Consultant Agent",
    "authors": "Karthikeyan K, Philip Wu, Xin Tang, Alexandre Alves",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16171v1",
    "source": "arXiv",
    "abstract": "The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.",
    "title_zh": "科学顾问代理",
    "abstract_zh": "科学顾问代理（Science Consultant Agent）是一款基于网络的人工智能（AI）工具，旨在帮助从业者选择并实施最有效的建模策略，以构建基于AI的解决方案。该工具通过四个核心组件运行：问卷调查、智能填充、研究引导推荐以及原型构建器。通过结合结构化问卷、基于文献的解决方案推荐以及原型生成，科学顾问代理能够加速从产品经理、软件开发人员到研究人员等各类用户的开发进程。完整的流程如图1所示。"
  },
  {
    "date": "2025-12-18",
    "title": "Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study",
    "authors": "Syed Salauddin Mohammad Tariq, Foyzul Hassan, Amiangshu Bosu, Probir Roy",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16066v1",
    "source": "arXiv",
    "abstract": "Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.",
    "title_zh": "无服务器系统中冷启动反模式及其重构：一项实证研究",
    "abstract_zh": "无服务器计算简化了部署与扩展，但冷启动延迟仍是主要的性能瓶颈。与以往将缓解措施视为黑箱优化的研究不同，我们把冷启动问题看作开发者可感知的设计挑战。通过对开源无服务器系统中81个经裁定的问题报告进行分析，我们归纳出初始化反模式、修复策略以及横跨设计、打包和运行时层的诊断难题的分类体系。基于这些洞察，我们提出了SCABENCH——一个可复现的基准测试工具，以及INITSCOPE——一个轻量级分析框架，能够将代码加载情况与实际执行行为关联起来。在SCABENCH上的实验表明，相较于现有工具，INITSCOPE将定位准确率提升了高达40%，诊断工作量减少了64%；开发人员实测也显示其任务准确率更高、诊断速度更快。这些成果推动了以证据为基础、注重性能的无服务器冷启动缓解实践的发展。可用性：本研究的相关成果已公开，可供未来研究与改进使用。"
  },
  {
    "date": "2025-12-18",
    "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
    "authors": "Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16917v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
    "title_zh": "生成对抗推理器：通过对抗强化学习增强大语言模型的推理能力",
    "abstract_zh": "具有显式推理能力的大语言模型（LLMs）在数学推理方面表现优异，但仍会犯过程性错误，例如计算错误、逻辑脆弱以及表面合理但无效的推理步骤。本文提出了一种生成对抗推理器（Generative Adversarial Reasoner），这是一种基于策略的联合训练框架，通过对抗式强化学习共同演化一个LLM推理器与一个基于LLM的判别器，以提升推理能力。该方法采用计算高效的审查机制，将每条推理链划分为长度相近、逻辑完整的片段，并由判别器对每个片段的合理性进行简洁、结构化的评估。学习过程中耦合了互补信号：LLM推理器因产生逻辑一致且最终得出正确答案的步骤而获得奖励；判别器则因准确识别错误或区分不同推理轨迹而获得奖励。这种机制生成密集、校准良好、基于策略的逐步奖励，补充了稀疏的精确匹配信号，从而改善信用分配，提高样本效率，并显著增强LLM的整体推理质量。在多个数学基准测试中，该方法均持续优于采用标准强化学习后训练的强基线模型。具体而言，在AIME24数据集上，我们将DeepSeek-R1-Distill-Qwen-7B的性能从54.0提升至61.3（+7.3），将DeepSeek-R1-Distill-Llama-8B从43.7提升至53.7（+10.0）。此外，模块化的判别器设计还支持灵活的奖励塑造，适用于教师蒸馏、偏好对齐以及基于数学证明的推理等多样化目标。"
  },
  {
    "date": "2025-12-18",
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "authors": "Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16912v1",
    "source": "arXiv",
    "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
    "title_zh": "探索与利用：通过裁剪、熵和虚假奖励重新思考强化学习中的RLVR",
    "abstract_zh": "本文探讨了在可验证奖励强化学习（RLVR）框架中探索与利用之间的权衡问题，该框架旨在提升大型语言模型（LLMs）的推理能力。近期研究表明，RLVR能够通过两种看似矛盾的机制激发LLM强大的数学推理能力：一是虚假奖励，通过奖励与真实答案无关的结果来抑制利用；二是熵最小化，通过促使模型输出更自信、更确定的结果来抑制探索。这一现象令人困惑：既抑制利用又抑制探索反而提升了推理表现，但其背后协调这些效应的基本原理仍不清晰。本文聚焦两个核心问题：（i）策略熵与性能之间的关系；（ii）虚假奖励是否能带来实际收益，可能源于裁剪偏差与模型污染之间的相互作用。研究结果表明，在虚假奖励下，裁剪偏差会降低策略熵，使模型输出更加自信和确定，而仅靠熵最小化本身不足以实现性能提升。此外，我们提出了一种奖励错配模型，解释了为何在模型被污染的情况下，虚假奖励仍能带来性能增益。本研究揭示了虚假奖励优势背后的机制，并为更有效的RLVR训练提供了理论指导原则。"
  },
  {
    "date": "2025-12-18",
    "title": "NRGPT: An Energy-based Alternative for GPT",
    "authors": "Nima Dehmamy, Benjamin Hoover, Bishwajit Saha, Leo Kozachkov, Jean-Jacques Slotine, Dmitry Krotov",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16762v1",
    "source": "arXiv",
    "abstract": "Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.",
    "title_zh": "NRGPT：一种基于能量的GPT替代方案",
    "abstract_zh": "生成式预训练变换器（GPT）架构是当前语言建模中最受欢迎的设计。能量基建模（EBM）则是一种不同的范式，将推理视为在能量景观上进行的动态过程。我们对GPT设置进行了最小程度的修改，将其与EBM框架统一起来。我们提出的模型——eNeRgy-GPT（NRGPT），其推理步骤被概念化为在能量景观上对词元的探索。我们证明并经实证验证，在某些条件下，这种探索会演变为梯度下降，尽管这并不一定意味着模型性能最优。我们展示了该模型在简单语言任务（如莎士比亚语料库）、代数类ListOPS任务以及更复杂的场景（如OpenWebText语言建模）中均表现出色。此外，我们还观察到，我们的模型可能对过拟合更具鲁棒性，仅在极长的训练过程中才会出现过拟合现象。"
  },
  {
    "date": "2025-12-18",
    "title": "An Empirical Study of the Realism of Mutants in Deep Learning",
    "authors": "Zaheed Ahmed, Philip Makedonski, Jens Grabowski",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16741v1",
    "source": "arXiv",
    "abstract": "Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL. This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches. Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.",
    "title_zh": "深度学习中突变体真实性的实证研究",
    "abstract_zh": "变异分析是一种在传统软件开发范式中评估测试质量的成熟技术，通过向程序中注入人工故障来实现。该技术在深度学习（DL）领域的应用已从经典测试扩展至故障定位、修复、数据生成以及模型鲁棒性评估等任务。其核心假设是：变异体的行为与真实故障相似。这一假设在传统软件系统中已被广泛验证，但在深度学习领域尚缺乏充分实证支持。本研究首次对深度学习中的预训练变异与后训练变异方法在真实性方面进行了实证比较。我们提出了一种统计框架，利用公开可用的漏洞数据集（CleanML、DeepFD、DeepLocalize 和 defect4ML）量化两种方法的耦合强度及其与真实故障的行为相似性。采用代表两类方法的先进工具生成变异体。结果表明，预训练变异体在耦合强度和与真实故障的行为相似性方面均显著优于后训练变异体，显示出更高的真实性。然而，预训练变异带来的巨大计算开销也凸显出迫切需要开发更高效的后训练变异算子，以达到甚至超越预训练变异体所展现的真实性水平。"
  },
  {
    "date": "2025-12-18",
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "authors": "Bingxiang He, Zekai Qu, Zeyuan Liu, Yinghao Chen, Yuxin Zuo, Cheng Qian, Kaiyan Zhang, Weize Chen, Chaojun Xiao, Ganqu Cui, Ning Ding, Zhiyuan Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16649v1",
    "source": "arXiv",
    "abstract": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
    "title_zh": "JustRL：用简单的强化学习方案扩展15亿参数的语言模型",
    "abstract_zh": "近年来，大型语言模型的强化学习研究逐渐趋向于增加复杂性：多阶段训练流程、动态超参数调度以及课程学习策略。这引发了一个根本性问题：\\textbf{这种复杂性是否必要？} 我们提出了 \\textbf{JustRL}，一种极简的方法，采用单阶段训练和固定超参数，在两个15亿参数的推理模型上取得了业界领先的表现（在九个数学基准测试中平均准确率分别为54.9\\%和64.3\\%），同时所用计算量仅为复杂方法的二分之一。相同的超参数无需调整即可在两个模型间通用，且训练过程在4000多步内表现出平稳、单调的提升，未出现通常需要干预的崩溃或停滞现象。关键的是，消融实验表明，添加“常规技巧”如显式的长度惩罚和鲁棒验证器反而可能因压缩探索空间而降低性能。这些结果提示，当前领域可能正在为本可通过稳定、可扩展的基础设置自然解决的问题引入不必要的复杂性。我们已公开发布模型与代码，旨在为社区建立一个简单且经过验证的基线标准。"
  },
  {
    "date": "2025-12-18",
    "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
    "authors": "Jinwu Chen, Qidie Wu, Bin Li, Lin Ma, Xin Si, Yang Hu, Shouyi Yin, Jun Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16465v1",
    "source": "arXiv",
    "abstract": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
    "title_zh": "cuPilot：一种用于CUDA内核演化的策略协同多智能体框架",
    "abstract_zh": "优化CUDA内核是一项极具挑战性且耗时的任务，这不仅需要软硬件协同设计的专业知识，还受限于高性能内核库的专有性质。尽管近期结合大型语言模型（LLMs）与进化算法的方法在自动内核优化方面展现出潜力，但现有方法由于代理设计不佳以及进化表示不匹配，往往在性能上表现欠佳。本文识别了这些关键不匹配问题，并提出cuPilot——一种策略协调的多智能体框架，通过引入“策略”作为内核演化的中间语义表示，有效提升了优化质量。主要贡献包括：策略协调的演化算法、基于roofline模型的提示生成方法，以及基于策略的种群初始化机制。实验结果表明，在100个内核的基准测试中，cuPilot生成的内核相比PyTorch实现了平均3.09倍的加速。在GEMM任务中，cuPilot展现了复杂的优化能力，显著提升了关键硬件单元的利用率。相关生成的内核已开源，地址为：https://github.com/champloo2878/cuPilot-Kernels.git。"
  },
  {
    "date": "2025-12-18",
    "title": "Learning to Wait: Synchronizing Agents with the Physical World",
    "authors": "Yifei She, Ping Zhang, He Liu, Yanmin Jia, Yang Jing, Zijun Liu, Peng Sun, Xiangbin Li, Xiaohe Hu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16262v1",
    "source": "arXiv",
    "abstract": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
    "title_zh": "学会等待：让智能体与物理世界同步",
    "abstract_zh": "现实世界中的智能体任务，与同步的马尔可夫决策过程（MDPs）不同，通常涉及非阻塞操作以及可变的延迟，从而在动作发起与完成之间产生一个根本性的【时间间隙】。现有的环境端解决方案，如阻塞包装器或频繁轮询，要么限制了系统的可扩展性，要么因冗余观测信息而稀释了智能体的上下文窗口。在本工作中，我们提出一种【智能体端方法】，使大型语言模型（LLMs）能够主动将其【认知时间线】与物理世界对齐。通过将“代码即行动”范式拓展至时间维度，智能体利用语义先验和上下文学习（ICL），精准预测等待时长（\\texttt{time.sleep(t)}），从而在无需反复检查的情况下，有效实现与异步环境的同步。在模拟 Kubernetes 集群中的实验表明，智能体能够精确校准其内部时钟，显著降低查询开销与执行延迟。这验证了时间感知是一种可习得的能力，对于开放环境中智能体的自主演化至关重要。"
  },
  {
    "date": "2025-12-18",
    "title": "Sigma-Moe-Tiny Technical Report",
    "authors": "Qingguo Hu, Zhenghao Lin, Ziyue Yang, Yucheng Ding, Xiao Liu, Yuting Jiang, Ruizhe Wang, Tianyu Chen, Zhongxin Guo, Yifan Xiong, Rui Gao, Lei Qu, Jinsong Su, Peng Cheng, Yeyun Gong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16248v1",
    "source": "arXiv",
    "abstract": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures. Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny Code: https://github.com/microsoft/ltp-megatron-lm",
    "title_zh": "Sigma-Moe-Tiny 技术报告",
    "abstract_zh": "混合专家（Mixture-of-Experts, MoE）因其高效且强大的可扩展性，已成为基础模型领域的一个有前景的范式。在本工作中，我们提出了 Sigma-MoE-Tiny，这是一个在现有开源模型中实现了最高稀疏度的 MoE 语言模型。Sigma-MoE-Tiny 采用细粒度的专家划分，每层最多包含 96 个专家，但每个 token 仅激活其中一个专家，总参数量为 200 亿，而激活的参数仅为 5 亿。这种极端稀疏性带来的主要挑战在于专家负载均衡问题。我们发现，在该设置下，广泛使用的负载均衡损失在低层中往往变得无效。为解决这一问题，我们提出了一种渐进式稀疏化训练策略，旨在平衡专家利用率与训练稳定性。Sigma-MoE-Tiny 在一个多样化且高质量的语料库上进行预训练，并通过后训练进一步释放其潜力。整个训练过程表现出极高的稳定性，未出现任何不可恢复的损失突增现象。全面评估表明，尽管仅激活了 5 亿参数，Sigma-MoE-Tiny 的性能仍达到同规模或更大规模模型中的顶尖水平。此外，我们还深入探讨了高度稀疏 MoE 模型中的负载均衡问题，为未来 MoE 架构的稀疏性发展提供了重要洞见。\n\n项目页面：https://qghuxmu.github.io/Sigma-MoE-Tiny  \n代码仓库：https://github.com/microsoft/ltp-megatron-lm"
  },
  {
    "date": "2025-12-18",
    "title": "Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis",
    "authors": "Zhi Helu, Huang Jingjing, Xu Wang, Xu Yangbin, Zhang Wanyue, Jiang Baoyang, Deng Shirui, Zhu Liang, Li Fangfang, Zhao Tiejun, Lin Yankai, Yao Yuan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16237v1",
    "source": "arXiv",
    "abstract": "Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.",
    "title_zh": "通过程序化数据合成提升多模态大语言模型中的空间推理能力",
    "abstract_zh": "具身智能是人工智能领域的一项重大挑战，其核心瓶颈在于当前模型在空间理解与推理能力上的局限性。现有通过增强视觉-语言模型（VLMs）来应对该问题的努力陷入两难境地：基于模板的数据集虽可扩展，但结构僵化；而人工标注则语言多样性丰富，却难以规模化，且关键在于计算精度不足。我们提出SPRITE——一种新颖的框架，通过利用模拟器和大模型，程序化地生成可扩展、多样化且高质量的空间推理数据，从而突破这一困境。SPRITE的核心创新在于将真实答案的生成重构为代码生成任务：我们使用大语言模型（LLMs）将复杂的空间问题编译为可执行程序，并基于从模拟器中提取的高精度场景元信息进行验证。这确保了我们的真实答案既具备计算精确性又可验证，同时LLM强大的生成能力带来了丰富的语言多样性。基于这一流程，我们构建了一个涵盖3个模拟器、超过11,000个场景以及30万+图像/视频指令微调对的数据集。实验表明，基于SPRITE数据训练的VLM在多个空间推理基准上均取得显著性能提升，优于同等规模的其他开源数据集。此外，可扩展性分析验证了我们的假设：克服传统模板方法固有的低多样性缺陷，是构建鲁棒且可泛化的空间智能的关键。我们将公开SPRITE框架的源代码及完整的30万+数据集，以推动未来在空间智能领域的研究发展。"
  },
  {
    "date": "2025-12-18",
    "title": "Meta-RL Induces Exploration in Language Agents",
    "authors": "Yulun Jiang, Liangze Jiang, Damien Teney, Michael Moor, Maria Brbic",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.16848v1",
    "source": "arXiv",
    "abstract": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
    "title_zh": "元强化学习促进语言智能体的探索",
    "abstract_zh": "强化学习（RL）已使得大型语言模型（LLM）代理能够与环境交互，并完成多轮次、长时程的任务。然而，经过RL训练的代理在需要主动探索的任务中往往表现不佳，且难以从试错经验中高效地进行适应。本文提出LaMer，一种通用的元强化学习（Meta-RL）框架，使LLM代理能够在测试阶段主动探索并从环境反馈中学习。LaMer包含两个关键组件：（i）跨回合训练框架，以促进探索并优化长期奖励；（ii）基于反思的上下文内策略自适应，使代理能够在不进行梯度更新的情况下，根据任务反馈信号调整自身策略。在多种环境中的实验表明，LaMer相较于传统RL基线显著提升了性能，在Sokoban、MineSweeper和Webshop任务上分别实现了11%、14%和19%的性能提升。此外，与RL训练的代理相比，LaMer在面对更具挑战性或此前未见过的任务时也展现出更强的泛化能力。总体而言，我们的结果表明，元强化学习为引导语言代理产生探索行为提供了一种系统性的方法，通过学习到的探索策略，使代理能够更稳健地适应新环境。"
  },
  {
    "date": "2025-12-18",
    "title": "Galet: a Geant4-Based Monte Carlo Simulation Platform for Modeling Space Radiation Effects on Advanced Semiconductor Devices",
    "authors": "I.-C. Cho, T. Aso, S.-W. Hsiao, Y.-C. Lin, T.-C. Chao, C.-M. Tan",
    "publish": "2025 IEEE Nuclear Science Symposium (NSS), Medical Imaging Conference (MIC) and Room Temperature Semiconductor Detector Conference (RTSD)",
    "url": "https://doi.org/10.1109/nss/mic/rtsd57106.2025.11287125",
    "source": "IEEE",
    "abstract": "As semiconductor technologies scale into the deep sub-micron regime, the susceptibility of electronic devices to space radiation has emerged as a critical design challenge. Accurate modeling of particle interactions with nanoscale microelectronic structures is essential for developing radiation-hardened systems. In this study, we introduce Galet, a Geant4-based Monte Carlo simulation platform specifically designed to investigate radiation effects in advanced semiconductor devices operating in harsh environments such as outer space. The Galet supports GDML-based geometry input, allowing the direct import of realistic device models from electronic design automation (EDA) tools. It also integrates radiation environment data from platforms such as SPENVIS and OMERE, enabling mission-specific source spectra for space-relevant particle simulations. As a demonstration, we implemented and simulated a 14 nm FinFET structure to evaluate spatially resolved energy deposition at nanometer scales. Galet offers a flexible and extensible framework for studying total ionizing dose (TID), single-event effects (SEE), and radiation-induced degradation in emerging semiconductor technologies.",
    "title_zh": "Galet：基于Geant4的蒙特卡洛模拟平台，用于建模空间辐射对先进半导体器件的影响",
    "abstract_zh": "随着半导体技术进入深亚微米领域，电子器件对空间辐射的敏感性已成为关键的设计挑战。准确模拟粒子与纳米尺度微电子结构的相互作用，对于开发抗辐射加固系统至关重要。在本研究中，我们提出了Galet——一个基于Geant4的蒙特卡罗仿真平台，专门用于研究先进半导体器件在恶劣环境（如外太空）中的辐射效应。Galet支持GDML格式的几何输入，可直接从电子设计自动化（EDA）工具导入真实的器件模型。同时，它集成了SPENVIS和OMERE等平台的辐射环境数据，能够为特定任务提供空间相关粒子的源谱，实现高精度的粒子模拟。作为示范应用，我们实现了14 nm FinFET结构的建模与仿真，评估了纳米尺度下的空间分辨能量沉积。Galet为研究新兴半导体技术中的总电离剂量（TID）、单粒子效应（SEE）以及辐射引起的退化现象，提供了一个灵活且可扩展的研究框架。"
  },
  {
    "date": "2025-12-18",
    "title": "Optimization of FPGA Core for MEMS-Based Terahertz Detection at Room Temperature",
    "authors": "M. Cautero, B. Bertoni, L. Alborghetti, D. Giuressi, G. Cautero, S. Zanotto, A. Pitanti",
    "publish": "2025 IEEE Nuclear Science Symposium (NSS), Medical Imaging Conference (MIC) and Room Temperature Semiconductor Detector Conference (RTSD)",
    "url": "https://doi.org/10.1109/nss/mic/rtsd57106.2025.11287539",
    "source": "IEEE",
    "abstract": "Room temperature detectors based on microelectromechanical systems (MEMS) offer significant advantages in size, cost, and integration compared to cryogenic alternatives, but require highly efficient readout electronics to achieve competitive sensitivity and dynamic range. This work presents the design and optimization of a field-programmable gate array (FPGA) core tailored for the readout and signal processing of MEMS sensors designed for terahertz (THz) detection at room temperature. We developed a modular and reconfigurable FPGA architecture that interfaces directly with an array of MEMS resonators characterized by a frequency addressing scheme. The core includes signal generation using a numerically controlled oscillator (NCO) for actuation and demodulation, digital lock-in amplification, and a proportional-integral (PI) feedback control for automatic resonance tracking through a phase-locked loop (PLL) architecture. Furthermore, the core is responsible for the completely automated search of the resonances, exploiting the finite number of bits of integral component within the PI. The number of multipliers is optimized through a time-multiplexed scheme that can be configured according to the number of sensors in use. The system was implemented on an Intel Cyclone V GX and validated using a linear array of eight MEMS bolometric sensors. Depending on the parameters used, both high responsivity (<tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$&gt;100$</tex> Hz modulation bandwidth) and sensitivity sufficient to detect black-body radiation from the human body under ambient conditions have been demonstrated. This FPGA-based approach provides a scalable and versatile solution for portable array-based THz imaging systems.",
    "title_zh": "基于MEMS的太赫兹探测在室温下的FPGA核心优化",
    "abstract_zh": "基于微机电系统（MEMS）的室温探测器相较于低温替代方案，在尺寸、成本和集成度方面具有显著优势，但需要高效率的读出电子电路才能实现具有竞争力的灵敏度和动态范围。本文提出了一种针对室温下用于太赫兹（THz）检测的MEMS传感器读出与信号处理的现场可编程门阵列（FPGA）核心的设计与优化。我们开发了一种模块化且可重构的FPGA架构，可直接与采用频率寻址方案的MEMS谐振器阵列进行接口。该核心集成了使用数控振荡器（NCO）生成激励与解调信号、数字锁相放大以及基于锁相环（PLL）结构的积分-比例（PI）反馈控制，以实现自动共振跟踪。此外，该核心还负责完全自动化的共振搜索功能，利用PI控制器中积分部分的有限位数特性。通过时间复用方案对乘法器数量进行优化，可根据实际使用的传感器数量进行配置。该系统在Intel Cyclone V GX平台上实现，并通过一个由八个MEMS热释电传感器组成的线性阵列进行了验证。根据所采用的参数，系统展示了高响应度（>100 Hz调制带宽）以及足以在环境条件下探测人体黑体辐射的灵敏度。这种基于FPGA的方法为便携式阵列式太赫兹成像系统提供了一种可扩展且灵活的解决方案。"
  },
  {
    "date": "2025-12-18",
    "title": "Application-Specific Optimization of Hybrid Post-Quantum Cryptographic Frameworks in Resource-Constrained Environments",
    "authors": "Timur Khudaybergenov, Temur Khudayberganov, Bahrombek Sabirov, Temur Turdiyev",
    "publish": "2025 IEEE XVII International Scientific and Technical Conference on Actual Problems of Electronic Instrument Engineering (APEIE)",
    "url": "https://doi.org/10.1109/apeie66761.2025.11289437",
    "source": "IEEE",
    "abstract": "the rapid advancement of quantum computing introduces substantial risks to classical cryptographic algorithms that rely on number-theoretic problems, such as the Rivest–Shamir–Adleman algorithm and elliptic curve cryptography. These cryptographic primitives are susceptible to polynomial-time quantum algorithms like those proposed by Shor and Grover, thereby endangering the security of contemporary digital infrastructures. In light of these emerging threats, the global cryptographic community is prioritizing the development and deployment of quantum-resistant cryptographic techniques. These include diverse algorithmic families such as those based on lattices, hash functions, error-correcting codes, and multivariate polynomials. Each approach exhibits distinct trade-offs in terms of computational performance, key length, signature size, and long-term resilience. This paper introduces a hybrid cryptographic framework optimized for specific application domains, particularly those with limited computational and energy resources. The proposed architecture strategically combines lattice-based encryption, hash-based digital signatures, and code-based key encapsulation mechanisms to form a unified model for secure data transmission. The architecture is implemented and evaluated across heterogeneous computing environments, including low-power embedded systems and Field-Programmable Gate Arrays. Experimental metrics such as execution latency, computational overhead, memory consumption, and energy efficiency are assessed and benchmarked against traditional single-algorithm deployments. The findings demonstrate that the hybrid approach maintains strong quantum resistance while significantly reducing performance penalties, making it suitable for mobile platforms, Internet of Things environments, and real-time communication systems. Future work should emphasize further fine-tuning for specific industrial domains, standardization of hybrid cryptographic protocols, and integration with artificial intelligence-based adaptive security layers to ensure sustainability in the approaching quantum era.",
    "title_zh": "资源受限环境下的专用应用优化混合后量子密码框架",
    "abstract_zh": "量子计算的快速发展对依赖数论难题的经典密码算法（如Rivest–Shamir–Adleman算法和椭圆曲线密码学）构成了重大威胁。这些密码原语易受Shor和Grover等提出的多项式时间量子算法攻击，从而危及现代数字基础设施的安全性。面对这些新兴威胁，全球密码学界正优先推进抗量子密码技术的研发与部署。这类技术涵盖多种算法体系，包括基于格、哈希函数、纠错码以及多变量多项式的方案。每种方法在计算性能、密钥长度、签名大小和长期抗性方面均表现出不同的权衡特性。\n\n本文提出一种面向特定应用领域的混合密码框架，尤其适用于计算能力和能源资源受限的场景。所提出的架构通过战略性地融合基于格的加密、基于哈希的数字签名以及基于编码的密钥封装机制，构建了一个统一的 secure 数据传输模型。该架构在异构计算环境中进行了实现与评估，涵盖低功耗嵌入式系统和现场可编程门阵列（FPGA）。实验指标如执行延迟、计算开销、内存消耗和能效表现均被测量，并与传统的单一算法部署方案进行对比。结果表明，该混合方案在保持强抗量子能力的同时，显著降低了性能损耗，因而特别适用于移动平台、物联网环境以及实时通信系统。\n\n未来工作应着重于针对特定工业领域的进一步优化，推动混合密码协议的标准化，并探索与基于人工智能的自适应安全层集成，以确保在即将到来的量子时代中系统的可持续性与安全性。"
  },
  {
    "date": "2025-12-18",
    "title": "Critical Path Optimization and Hardware Implementation for CKKS Key Switching Accelerator",
    "authors": "Hai Huang, Hanchun Wu, Bin Yu",
    "publish": "2025 5th International Conference on Electronic Information Engineering and Computer Science (EIECS)",
    "url": "https://doi.org/10.1109/eiecs67708.2025.11283533",
    "source": "IEEE",
    "abstract": "The CKKS homomorphic encryption scheme provides a robust theoretical foundation for privacy-preserving computations, however, the computational overhead associated with its key switching operations significantly impedes practical deployment. To address this challenge, this paper introduces a hardware-accelerated architecture through algorithmic process restructuring and resource reuse strategies. By optimizing the execution flow of the ModUp and ModDown phases, the critical path latency is effectively minimized. Additionally, developed a dynamic resource multiplexing strategy for the NTT/INTT computational units, enabling flexible scheduling of hardware resources across various computational stages, thereby substantially enhancing hardware utilization and reducing resource consumption. The proposed design is implemented on the Xilinx UltraScale+ FPGA platform, with experimental results demonstrate that the proposed architecture achieves superior resource efficiency compared to existing solutions. Specifically, the area-latency product is improved by 40.68% relative to HEAX, and overall performance is enhanced by a factor of $\\mathbf{1. 5 8 x}$ to $\\mathbf{3. 5 7 x}$ times compared to coxHE. This architecture offers an efficient and feasible hardware acceleration pathway for CKKS key switching in resource-constrained environments.",
    "title_zh": "CKKS密钥切换加速器的路径优化与硬件实现",
    "abstract_zh": "CKKS同态加密方案为隐私保护计算提供了坚实的理论基础，然而其密钥切换操作带来的计算开销严重制约了实际应用。针对这一挑战，本文通过算法流程重构与资源复用策略，提出了一种硬件加速架构。通过对ModUp和ModDown阶段的执行流程进行优化，有效缩短了关键路径延迟。此外，设计了一种NTT/INTT计算单元的动态资源复用策略，实现了不同计算阶段间硬件资源的灵活调度，显著提升了硬件利用率并降低了资源消耗。所提出的架构在Xilinx UltraScale+ FPGA平台上实现，实验结果表明，与现有方案相比，该设计在资源效率方面表现优异：相较于HEAX，面积-延迟乘积改善了40.68%；相较于coxHE，整体性能提升达1.58倍至3.57倍。该架构为资源受限环境下的CKKS密钥切换提供了高效且可行的硬件加速途径。"
  },
  {
    "date": "2025-12-18",
    "title": "Democratising Parallel Compute: Triton 3D Joseph Projector Implementation",
    "authors": "I. R.D. Singh, G. Schramm, K. Thielemans",
    "publish": "2025 IEEE Nuclear Science Symposium (NSS), Medical Imaging Conference (MIC) and Room Temperature Semiconductor Detector Conference (RTSD)",
    "url": "https://doi.org/10.1109/nss/mic/rtsd57106.2025.11287902",
    "source": "IEEE",
    "abstract": "Medical imaging often involves computationally intensive algorithms that process highdimensional, weakly dependent data; these data characteristics allow for parallel computation on dedicated hardware. The most common hardware used are Nvidia Graphic Processing Units (GPUs), and the language for writing highly optimised kernels is Compute Unified Device Architecture (CUDA) that is mature but proprietary. This restricts deployment primarily to Nvidia hardware. This Hackathon project aimed to investigate the viability of Triton as an alternative, focusing on implementing the computationally expensive Joseph projector in Triton. This was written within a day and in less than 500 lines of code; this demonstrates the productivity that is enabled by Triton's Python-based meta-programming interface, block-based programming model, and Just-In-Time (JIT) compilation capabilities. Further to this, the advantages of programming with Triton are: (1) mixed precision, (2) support for Nvidia and AMD graphics cards, (3) Triton's internal code optimisation, and (4) autotuning of block sizes to maximally utilise the GPU. The Triton-based Joseph projector revealed comparable performance as compared to ParallelProj, an open-source Joseph Projector written in CUDA. The code is available at https://github.com/Imraj-Singh/Triton_Joseph_Projector.",
    "title_zh": "民主化并行计算：Triton 3D约瑟夫投影器实现",
    "abstract_zh": "医学影像通常涉及计算密集型算法，需要处理高维且弱相关性数据；这些数据特性使得在专用硬件上进行并行计算成为可能。目前最常用的硬件是Nvidia图形处理器（GPU），而用于编写高度优化内核的编程语言是成熟但专有的CUDA。这限制了其部署主要局限于Nvidia硬件。本黑客松项目旨在探究Triton作为替代方案的可行性，重点在于使用Triton实现计算开销较大的约瑟夫投影器（Joseph projector）。该项目仅用一天时间完成，代码量不足500行，充分展示了Triton基于Python的元编程接口、基于块的编程模型以及即时编译（JIT）能力所带来的高效生产力。此外，使用Triton编程还具有以下优势：（1）支持混合精度计算，（2）兼容Nvidia和AMD显卡，（3）Triton内部的代码优化机制，以及（4）自动调优块大小以最大限度利用GPU性能。基于Triton实现的约瑟夫投影器在性能上与开源CUDA实现ParallelProj相当。相关代码已发布于 https://github.com/Imraj-Singh/Triton_Joseph_Projector。"
  },
  {
    "date": "2025-12-18",
    "title": "Identifying Time-series Functions of regulated Measuring Instruments by means of advanced Deep Learning and Machine Learning",
    "authors": "Levin Ho, Marko Esche, Martin Nischwitz, Manuel Maue, Sabine Glesner",
    "publish": "IEEE Transactions on Instrumentation and Measurement",
    "url": "https://doi.org/10.1109/tim.2025.3645944",
    "source": "IEEE",
    "abstract": "Smart measuring instruments encompass software systems that are subject to legal regulations and require conformity assessments to ensure compliance during use, such as in Legal Metrology. Continuous quality control of such systems, especially after introducing functional updates or software patches, remains an open yet critical challenge. In this article, we develop an extension in form of a text-event-parser to an established deep learning framework to interpret text-based system logbooks describing the metrological function of regulated measuring instruments, and compare the performance with a state-of-the-art approach. Results of implementing our proposed approach onto real-world devices reveal its ability to continuously monitor and identify regulated measuring instruments’ time-dependent behavior. The comparison study shows that our approach has an overall advantage, considering its performance and practical aspects such as required training costs. Additionally, we investigate combinations of the developed approach with other machine learning frameworks, including natural language processing and transfer learning, to evaluate and improve the overall scalability and robustness. Building on the results of the investigations, our approach facilitates the fully automatic quality control of modern software-driven measuring instruments.",
    "title_zh": "通过先进的深度学习与机器学习识别受控测量仪器的时间序列函数",
    "abstract_zh": "智能测量仪器包含受法律法规监管的软件系统，这些系统在使用过程中需经过符合性评估以确保合规性，例如在法定计量领域。对于此类系统，在引入功能更新或软件补丁后，持续进行质量控制仍然是一个尚未解决但至关重要的挑战。本文在已有的深度学习框架基础上，开发了一种文本事件解析器（text-event-parser）扩展模块，用于解读描述受监管测量仪器计量功能的基于文本的系统日志记录，并与当前最先进的方法进行了性能对比。将所提出的方法应用于实际设备的实验结果表明，该方法能够持续监测并识别受监管测量仪器随时间变化的行为特征。对比研究显示，本方法在性能及实际应用方面（如所需训练成本）均具有整体优势。此外，我们还探讨了所开发方法与其他机器学习框架（包括自然语言处理和迁移学习）相结合的可能性，以评估并提升系统的整体可扩展性与鲁棒性。基于上述研究结果，本方法为现代软件驱动型测量仪器的全自动质量控制提供了有效支持。"
  },
  {
    "date": "2025-12-18",
    "title": "Real-Time Embedded Signal-Processing for Time-of-Flight Computed Tomography ASIC",
    "authors": "D. Roshani, G. Bélanger, W. Tremblay, R. Espagnet, Y. Bérubé-Lauzière, A. C. Therrien, M.-A. Tétrault, R. Fontaine",
    "publish": "2025 IEEE Nuclear Science Symposium (NSS), Medical Imaging Conference (MIC) and Room Temperature Semiconductor Detector Conference (RTSD)",
    "url": "https://doi.org/10.1109/nss/mic/rtsd57106.2025.11287960",
    "source": "IEEE",
    "abstract": "The advancement of Time-of-Flight Computed Tomography (ToF-CT) demands high-resolution timing and efficient data handling to improve image quality and reduce radiation dose. This work presents the implementation and verification of a mixed-signal ASIC for TOF-CT. The ASIC integrates 25 channels, each responsible for capturing and processing fast-timing signals from photo-detectors. Each channel includes a time-walk correction block, and a circuit to 3D histogram time-of-flight and energy of events. A serializer sends the processed data from all <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$25 \\sim$</tex> channels to an a FPGA through LVDS link.",
    "title_zh": "飞行时间计算机断层扫描专用集成电路的实时嵌入式信号处理",
    "abstract_zh": "飞行时间计算机断层扫描（ToF-CT）技术的进步需要高分辨率的时间测量和高效的数据处理，以提升图像质量并降低辐射剂量。本文介绍了用于ToF-CT的混合信号专用集成电路（ASIC）的实现与验证。该ASIC集成了25个通道，每个通道负责采集并处理来自光电探测器的快速时间信号。每个通道均包含一个时间走动校正模块，以及用于构建事件飞行时间与能量三维直方图的电路。一个串行器通过LVDS链路将所有25个通道的处理数据发送至FPGA。"
  },
  {
    "date": "2025-12-18",
    "title": "A Resilient, Multi-Layered RAG Framework for High-Fidelity Retrieval in Complex Technical Manuals",
    "authors": "Tianjin Huang, Xingdong Zhu, Kai Liu, Peng Zhao",
    "publish": "2025 5th International Conference on Electronic Information Engineering and Computer Science (EIECS)",
    "url": "https://doi.org/10.1109/eiecs67708.2025.11283308",
    "source": "IEEE",
    "abstract": "Retrieval-Augmented Generation (RAG) systems are widely adopted to enhance Large Language Models (LLMs) with external knowledge, particularly in domains requiring up-todate and precise information. However, conventional RAG approaches often fail in safety-critical industrial domains like aviation, where documents possess rigid hierarchical structures (e.g., Technical Instruction Manuals(TIMs)). These exhibit contextual disintegration, resulting in semantic ambiguity, overspecialization, and critical hallucination risks. This paper introduces DeepPerception-RAG, a novel framework designed to address these limitations by integrating document hierarchy awareness directly into the retrieval process. The core innovation lies in a Parallel Depth-Aware Retrieval engine, where sparse (BM25) and dense (vector) retrieval pathways independently apply a sophisticated, logarithmic depth-aware scoring function to re-rank candidates. This function intelligently balances semantic relevance with hierarchical preference. The optimized results from both paths are merged, and a final, standardized LLM-based arbitration mechanism with robust fault tolerance resolves topranked conflicts. A confidence-based fallback guarantees system resilience. Rigorous evaluation on the newly constructed AeroRef150 benchmark - 150 queries across five question types demonstrates that DeepPerception-RAG achieves 91.3% Gold Path Coverage (GPC) and reduces hallucination rates by over $\\mathbf{8 0 \\%}$ compared to baselines. This work contributes a state-of-the-art, resilient RAG framework, a hierarchical benchmark methodology, and a novel dataset for high-fidelity retrieval in complex industrial documentation.",
    "title_zh": "一种稳健的多层RAG框架，用于复杂技术手册中的高保真检索",
    "abstract_zh": "检索增强生成（RAG）系统被广泛采用，以通过引入外部知识来提升大型语言模型（LLMs）的性能，尤其在需要实时且精确信息的领域。然而，传统的RAG方法在航空等安全关键型工业领域中表现不佳，因为这些领域的文档具有严格的层级结构（如技术操作手册TIMs）。这类文档常出现上下文割裂问题，导致语义模糊、过度专业化以及严重的幻觉风险。本文提出一种名为DeepPerception-RAG的新框架，旨在通过将文档层级结构感知直接融入检索过程，解决上述局限性。其核心创新在于一个并行的深度感知检索引擎，该引擎中稀疏（BM25）与密集（向量）检索路径各自独立地应用一种复杂的对数深度感知评分函数，对候选结果进行重新排序。该函数智能平衡语义相关性与层级优先级。两条路径优化后的结果被融合，并由一个基于LLM的最终仲裁机制进行处理，该机制具备强大的容错能力，可有效解决排名冲突。同时，基于置信度的回退机制确保了系统的鲁棒性。在新构建的AeroRef150基准测试集上进行了严格评估——涵盖五种题型共150个查询，结果显示，DeepPerception-RAG实现了91.3%的黄金路径覆盖率（GPC），并将幻觉率相比基线降低了超过80%。本研究贡献了一个先进的、具备高鲁棒性的RAG框架，一套面向层级结构的基准评测方法，以及一个用于复杂工业文档高保真检索的新型数据集。"
  },
  {
    "date": "2025-12-18",
    "title": "LLM-Powered Automated Detection and Optimization of Inference Bottlenecks in Distributed Systems: A Static Analysis Approach",
    "authors": "Xiaoying Li",
    "publish": "2025 6th International Conference on Computer Vision and Data Mining (ICCVDM)",
    "url": "https://doi.org/10.1109/iccvdm66874.2025.11290260",
    "source": "IEEE",
    "abstract": "This paper presents a novel approach to automating the detection and optimization of inference bottlenecks in distributed deep learning systems through the application of Large Language Models (LLMs). While traditional performance optimization in distributed environments requires extensive manual analysis across multiple components, this paper propose a static analysis framework that leverages LLMs' code comprehension capabilities to identify performance-critical patterns without execution.This system processes distributed model architectures and implementation code to generate an abstract syntax tree representation, which is then analyzed by a fine-tuned LLM trained on a curated dataset of 5,000+ annotated performance bottlenecks from production distributed environments. Experimental evaluation across 78 diverse distributed neural network deployments demonstrates that the approach identifies 93.7% of inference bottlenecks that would otherwise require dynamic profiling, while suggesting optimizations that yield an average 2.8x inference speedup on resource-constrained devices. A comparative analysis reveals this static analysis method reduces optimization time by 76% compared to conventional profiling-based approaches in distributed settings. Additionally, the system generates natural language explanations of detected issues and recommends contextually appropriate optimization strategies for distributed inference pipelines, facilitating knowledge transfer to practitioners. This research addresses critical challenges in deep learning deployment across distributed environments and enables more efficient utilization of computational resources in production systems.",
    "title_zh": "基于大语言模型的分布式系统推理瓶颈自动化检测与优化：一种静态分析方法",
    "abstract_zh": "本文提出了一种新颖的方法，通过应用大语言模型（LLMs）自动化检测与优化分布式深度学习系统中的推理瓶颈。传统上，分布式环境下的性能优化需要对多个组件进行大量手动分析，而本文提出的静态分析框架则利用大语言模型的代码理解能力，在无需实际执行的情况下识别出关键性能模式。该系统处理分布式模型架构和实现代码，生成抽象语法树表示，并由一个在超过5000个经过标注的生产环境中性能瓶颈数据集上微调过的LLM进行分析。在78个不同类型的分布式神经网络部署上的实验评估表明，该方法能够识别出93.7%原本需要动态剖析才能发现的推理瓶颈，同时提出的优化建议在资源受限设备上平均实现了2.8倍的推理速度提升。对比分析显示，相较于传统的基于剖析的优化方法，该静态分析方法在分布式场景下将优化时间减少了76%。此外，该系统还能生成自然语言形式的问题解释，并为分布式推理流水线推荐上下文相关的优化策略，有助于知识向实践者有效传递。本研究解决了深度学习在分布式环境中部署所面临的若干关键挑战，推动了生产系统中计算资源的更高效利用。"
  },
  {
    "date": "2025-12-18",
    "title": "Verification Methods for BPF-based DPU",
    "authors": "Danila Lisovsky, Valery Slizkoy, Egor Kuznetsov",
    "publish": "2025 IEEE XVII International Scientific and Technical Conference on Actual Problems of Electronic Instrument Engineering (APEIE)",
    "url": "https://doi.org/10.1109/apeie66761.2025.11289447",
    "source": "IEEE",
    "abstract": "The gap between central processing units’ performance and network transmission speed is ever increasing, leading to performance bottlenecks in modern computing systems. To address this issue, hardware accelerators are being developed to offload tasks such as network traffic filtering, packet classification, load balancing, and data processing from the CPU. However, the design of such accelerators must be accompanied by thorough verification to ensure correctness and reliability. Unfortunately, there are very few established solutions for verifying data processing units, as this area of research is still emerging. This paper presents functional verification methods applicable to the design of hardware network traffic accelerators, with a focus on a Berkeley Packet Filter virtual machine-based architecture. The proposed approach adapts simulation-based functional and behavioral verification techniques commonly used in general-purpose processor development. We describe the construction of a test bench environment, the generation of meaningful test stimuli, and the use of coverage-driven verification to assess design completeness. Results demonstrate effective detection of design errors and high functional coverage. Additionally, suggestions for improving code coverage, introducing custom verification metrics, and accelerating the verification process through automation are discussed. The presented methodology contributes to more robust and efficient development of hardware accelerators in high-speed networking applications.",
    "title_zh": "基于BPF的DPU验证方法",
    "abstract_zh": "中央处理器性能与网络传输速度之间的差距正在持续扩大，导致现代计算系统中出现性能瓶颈。为解决这一问题，硬件加速器正被开发用于将网络流量过滤、包分类、负载均衡和数据处理等任务从CPU中卸载。然而，这类加速器的设计必须伴随全面的验证，以确保其正确性和可靠性。遗憾的是，目前针对数据处理单元的验证解决方案非常有限，因为该研究领域仍处于发展初期。本文提出了一种适用于硬件网络流量加速器设计的功能性验证方法，重点聚焦基于伯克利包过滤器（Berkeley Packet Filter）虚拟机架构的实现。所提出的方案借鉴了通用处理器开发中常用的基于仿真的功能性和行为性验证技术。我们详细描述了测试环境的构建、有意义测试激励的生成，以及利用覆盖率驱动的验证来评估设计完整性。实验结果表明，该方法能够有效检测设计错误，并实现了较高的功能覆盖率。此外，本文还讨论了提升代码覆盖率的建议、引入自定义验证指标的方法，以及通过自动化手段加速验证过程的策略。所提出的验证方法有助于在高速网络应用中实现更稳健、高效的硬件加速器开发。"
  },
  {
    "date": "2025-12-18",
    "title": "Simulation Analysis of an Intelligent Operations and Monitoring Data Indicator Warning and Diagnosis System Based on a Large Language Model-Enhanced Transformer",
    "authors": "Pan Hui, Pang Peng, Deng Churan, Zhao Shuang, Ma Guanxiong, Luo Xuan",
    "publish": "2025 5th International Conference on Electronic Information Engineering and Computer Science (EIECS)",
    "url": "https://doi.org/10.1109/eiecs67708.2025.11283611",
    "source": "IEEE",
    "abstract": "With the widespread application of cloud computing and modern microservice architectures, the data from intelligent operations and maintenance (AIOps) for information technology systems exhibit complex characteristics such as high dimensionality, massive volume, and dynamic correlations. Traditional warning methods based on static thresholds or simple statistical models are insufficient for the early detection of subtle anomalies and the precise diagnosis of complex faults. To address this challenge, this paper proposes an intelligent operation and monitoring data warning and diagnosis framework based on a Large Language Model enhanced Transformer (LLM-Enhanced Anomaly Transformer, LEAT). This framework adopts a twostage structure: In the first stage, the cutting-edge Anomaly Transformer model is utilized to perform efficient unsupervised anomaly detection by leveraging its innovative “Association Discrepancy” metric to capture early deviations in system operation patterns. In the second stage, for the detected anomalous data segments, carefully designed Prompt Engineering techniques are used to encapsulate their contextual information and numerical dynamics, and a Large Language Model (LLM) is invoked for Zero-Shot inference to achieve interpretable Root Cause Analysis. To validate the effectiveness of the proposed framework, comprehensive simulation experiments were conducted on the public large-scale Server Machine Dataset (SMD). The results show that the LEAT framework achieves an F1-score of 0.930 in anomaly detection, which is on par with the current state-of-the-art. In the more critical task of root cause diagnosis, its Top-1 diagnosis accuracy (Precision@1) reaches 0.891, significantly outperforming baseline methods. This research confirms the great potential of the LEAT framework in enhancing the automation and intelligence of intelligent operations and monitoring.",
    "title_zh": "基于大语言模型增强的Transformer的智能运监数据指标预警与诊断系统仿真分析",
    "abstract_zh": "随着云计算和现代微服务架构的广泛应用，信息技术系统智能运维（AIOps）所产生的数据呈现出高维度、海量规模以及动态关联性等复杂特征。传统的基于静态阈值或简单统计模型的告警方法，难以实现对细微异常的早期发现以及对复杂故障的精准诊断。为应对这一挑战，本文提出了一种基于大语言模型增强的Transformer架构（LLM-Enhanced Anomaly Transformer, LEAT）的智能运维数据告警与诊断框架。该框架采用两阶段结构：第一阶段，利用前沿的Anomaly Transformer模型，通过其创新的“关联差异”（Association Discrepancy）度量，高效地进行无监督异常检测，从而捕捉系统运行模式中的早期偏差；第二阶段，针对检测出的异常数据片段，采用精心设计的提示工程（Prompt Engineering）技术，封装其上下文信息与数值动态特征，并调用大语言模型（LLM）进行零样本推理，实现可解释的根因分析。\n\n为验证所提框架的有效性，本文在公开的大规模服务器机器数据集（Server Machine Dataset, SMD）上开展了全面的仿真实验。结果表明，LEAT框架在异常检测任务中取得了0.930的F1分数，达到当前先进水平。在更为关键的根因诊断任务中，其Top-1诊断准确率（Precision@1）高达0.891，显著优于基线方法。本研究充分证实了LEAT框架在提升智能运维自动化与智能化水平方面的巨大潜力。"
  },
  {
    "date": "2025-12-18",
    "title": "BMGANet: Implementation of a Multi-Granularity Attention Network for Vulnerability Detection",
    "authors": "Xiangshan Qu, Erzhou Zhu",
    "publish": "2025 5th International Conference on Artificial Intelligence, Automation and High Performance Computing (AIAHPC)",
    "url": "https://doi.org/10.1109/aiahpc66801.2025.11290031",
    "source": "IEEE",
    "abstract": "Deep learning has become a prominent approach for automated code vulnerability detection. However, state-of-the-art models often struggle with complex vulnerabilities due to their inability to simultaneously capture both fine-grained token-level semantics and function-level structural features. To address this limitation, we propose BMGANet, a novel deep learning model that integrates multi-granularity features for effective vulnerability detection. Our framework begins by using Joern to construct Program Dependence Graphs (PDGs) and Abstract Syntax Trees (ASTs), followed by cross-function slicing and code normalization. A BERT-based network then extracts token-level and function-level features, which are further refined by an LSTM and a multi-head attention mechanism to model complex code contexts. These enhanced features are subsequently fused for final prediction. Furthermore, BMGANet is pre-trained on two auxiliary tasks—masked token prediction and inter-code-line logical correlation prediction—to improve its semantic understanding of code syntax and logic. Evaluated on widely-used benchmarks, BMGANet demonstrates superior performance and significantly outperforms existing methods.",
    "title_zh": "BMGANet：一种用于漏洞检测的多粒度注意力网络实现",
    "abstract_zh": "深度学习已成为自动化代码漏洞检测的主流方法。然而，现有先进模型在处理复杂漏洞时往往表现不佳，主要原因在于其难以同时捕捉细粒度的令牌级语义信息和函数级的结构特征。为解决这一局限性，我们提出了一种名为BMGANet的新式深度学习模型，该模型通过融合多粒度特征实现高效的漏洞检测。我们的框架首先利用Joern构建程序依赖图（PDGs）和抽象语法树（ASTs），随后进行跨函数切片与代码规范化处理。接着，基于BERT的网络提取令牌级与函数级特征，并通过LSTM和多头注意力机制进一步优化，以建模复杂的代码上下文。这些增强后的特征随后被融合用于最终的预测。此外，BMGANet在两个辅助任务上进行了预训练：掩码令牌预测和跨代码行逻辑相关性预测，从而提升了对代码语法与逻辑语义的理解能力。在广泛使用的基准数据集上的评估表明，BMGANet表现出卓越的性能，显著优于现有方法。"
  },
  {
    "date": "2025-12-18",
    "title": "Domain-Enhanced SimCSE with Lexicon Injection for Semantic Representation of Technical Texts",
    "authors": "Zhida Song, Xiaotong Guan, Naijun Zhao, Jingbo Zhao",
    "publish": "2025 2nd International Conference on Machine Learning, Pattern Recognition and Automation Engineering (MLPRAE)",
    "url": "https://doi.org/10.1109/mlprae67267.2025.11290737",
    "source": "IEEE",
    "abstract": "This study proposes an enhanced semantic representation model for Airbus Service Bulletin (SB) texts by integrating an improved SimCSE framework with domain-specific lexicon injection. To address the challenge of accurately capturing highly technical and standardized terminology within SBs, we first construct a specialized aerospace maintenance dictionary. This lexicon is then utilized to generate domain-aware positive samples during the contrastive learning process of SimCSE, forcing the model to learn invariant representations for technical terms and their synonyms. The resulting model generates high-quality sentence embeddings that are both semantically rich and domain-sensitive. Evaluated on downstream tasks such as SB retrieval and classification, our approach demonstrates superior performance compared to baseline models, providing a robust foundation for automated processing and knowledge discovery in aerospace maintenance documentation.",
    "title_zh": "基于词典注入的领域增强型SimCSE用于技术文本的语义表示",
    "abstract_zh": "本研究提出了一种增强的语义表示模型，用于空客服务公告（SB）文本的处理，该模型通过将改进的SimCSE框架与领域特定词汇注入相结合，有效提升了技术文本的表征能力。为解决SB中高度专业且标准化术语难以准确捕捉的问题，我们首先构建了一个专门的航空航天维修术语词典。该词典在SimCSE的对比学习过程中被用于生成具有领域感知能力的正样本，促使模型学习技术术语及其同义词的不变表示。由此得到的模型能够生成语义丰富且具备领域敏感性的高质量句子嵌入。在服务公告检索与分类等下游任务上的评估结果表明，该方法显著优于基线模型，为航空航天维修文档的自动化处理与知识发现提供了坚实基础。"
  },
  {
    "date": "2025-12-18",
    "title": "Large Language Models: Principles, Applications, and Cutting-edge Advances",
    "authors": "Xuefei Bian, Yingchun Zhang",
    "publish": "2025 2nd International Conference on Machine Learning, Pattern Recognition and Automation Engineering (MLPRAE)",
    "url": "https://doi.org/10.1109/mlprae67267.2025.11291015",
    "source": "IEEE",
    "abstract": "In recent years, large language models (LLMs) have become a cornerstone of artificial intelligence, advancing natural language processing, multimodal interaction, and intelligent agent development. This paper reviews the theoretical foundations, training methods, and representative applications of LLMs. It outlines the basics of machine learning and neural networks, analyzes the Transformer architecture and its variants, and surveys applications in scientific research, programming, and healthcare. The integration of LLMs with reinforcement learning and knowledge graphs is also discussed. Current challenges—such as hallucination, scalability, interpretability, and alignment—are summarized, alongside prospects for future development. The paper makes three main contributions: it proposes a \"task– context–risk\" taxonomy of application scenarios, synthesizes methodological frameworks for alignment and integration, and offers a forward-looking perspective on embodied LLMs, highlighting their trajectory toward autonomous decision-making agents.",
    "title_zh": "大语言模型：原理、应用与前沿进展",
    "abstract_zh": "近年来，大型语言模型（LLMs）已成为人工智能的基石，推动了自然语言处理、多模态交互以及智能代理的发展。本文综述了LLMs的理论基础、训练方法及代表性应用。文章介绍了机器学习与神经网络的基本原理，分析了Transformer架构及其变体，系统回顾了LLMs在科学研究、编程以及医疗健康等领域的应用。同时，本文还探讨了LLMs与强化学习、知识图谱的融合。针对当前面临的挑战——如幻觉问题、可扩展性、可解释性与对齐性——进行了总结，并展望了未来发展方向。本文主要贡献有三：提出了一种“任务–上下文–风险”应用情境分类体系；整合并归纳了对齐与集成的方法论框架；并对具身化LLMs的未来发展提出了前瞻性视角，强调其向自主决策智能体演进的趋势。"
  },
  {
    "date": "2025-12-18",
    "title": "Logical Environment for Intelligent Robot Control",
    "authors": "Evgenii Cherkashin, Nadezhda Nagul, Artem Davydov, Yingjie Wang, Haojun Teng",
    "publish": "2025 IEEE XVII International Scientific and Technical Conference on Actual Problems of Electronic Instrument Engineering (APEIE)",
    "url": "https://doi.org/10.1109/apeie66761.2025.11289343",
    "source": "IEEE",
    "abstract": "This paper proposes a concept of an integrated logical environment, specifically engineered for the formalization, solving, and practical implementation of complex control tasks for autonomous robots, with a particular focus on autonomous underwater vehicles, as well as coordinated groups of robots. The control architecture considered is supposed to be hierarchically structured, featuring a discrete event system as its core, which serves to seamlessly interconnect all decision-making and execution levels within the hierarchy. To process asynchronous events and ensure critical behavioral constraints within the framework of supervisory control theory, we employ a non-Horn calculus of positively-constructed formulas powered by an intuitionistic inference engine. A pivotal element of the environment is a novel Domain-Specific Language (DSL), developed to allow for the intuitive and natural expression of the control system's logic in a manner that is directly aligned with problem-solving objectives. This DSL facilitates the description of system components and their interactions, which are then automatically translated for processing by the logical engine. The approach is illustrated through a case study of the classic Box Pushing problem.",
    "title_zh": "智能机器人控制的逻辑环境",
    "abstract_zh": "本文提出了一种集成逻辑环境的概念，该环境专门设计用于自主机器人的复杂控制任务的形式化、求解与实际实现，尤其关注自主水下航行器以及机器人协同群体。所考虑的控制架构应具有层次化结构，其核心为离散事件系统，用以无缝连接层级内部的所有决策与执行层面。为了处理异步事件，并在监督控制理论框架下确保关键的行为约束，我们采用了一种基于直觉主义推理引擎的非霍恩型正构造公式演算。该环境的一个关键组成部分是一种新型领域特定语言（DSL），它能够以直观且自然的方式表达控制系统逻辑，使其直接契合问题求解目标。该DSL支持对系统组件及其交互关系的描述，这些描述随后将被自动转换并交由逻辑引擎进行处理。本文通过经典的“推箱”问题案例研究展示了该方法的应用。"
  }
]