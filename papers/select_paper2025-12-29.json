[
  {
    "date": "2025-12-29",
    "title": "Logic for Computer Engineering",
    "authors": "Vladimir Hahanov, Marina Abashidze, Abdullayev Vugar Hacimahmud, Tetiana Borysenko, Hanna Khakhanova, Nataliya Maksymova, Irina Hahanova",
    "publish": "2025 IEEE East-West Design &amp;amp; Test Symposium (EWDTS)",
    "url": "https://doi.org/10.1109/ewdts67441.2025.11303690",
    "source": "IEEE",
    "abstract": "The system issues of using computing logic to solve practical engineering problems are considered. It is demonstrated that the logic in a computer encompasses the existence and development of all emerging architectures, models, and algorithms. Relations are given that show the commonality of logical data structures for various types of computing, including quantum and Artificial Intelligence. A map of science and practice is provided, illustrating the relationship between key concepts such as science, engineering, education, and scientific results, all of which are essential for the creative activities of scientists, students, and engineers. A map of model minimization, its pros and cons for verification procedures, is given modelling for simulation. It is shown that to perform verification successfully, it is necessary to meet the requirements of testable standards to reduce the time for modeling and simulation procedures. To do this, it is essential to redesign (remodel) the digital product to maximize the enlargement of the logical elements of the new circuit model. A vector-matrix technology is presented for building a map to test functions and structures without relying on algorithms or processor instructions. Mechanisms for rendering any structure, function, truth table, and matrix using graph construction for any logical vector are given. Structurally logical constants are determined, enabling the solution of the problem of modeling a testing map for functionality or structure in a more technologically advanced manner.",
    "title_zh": "计算机工程中的逻辑",
    "abstract_zh": "在利用计算逻辑解决实际工程问题时，系统性问题被深入探讨。研究表明，计算机中的逻辑涵盖了所有新兴架构、模型与算法的存在与发展。文中给出了各类计算（包括量子计算和人工智能）之间逻辑数据结构的共性关系。同时，提供了一张科学与实践的图谱，展示了科学、工程、教育及科研成果等关键概念之间的关联，这些要素对于科学家、学生和工程师的创造性活动至关重要。此外，还提出了模型最小化的图谱，并分析了其在验证过程中的优缺点，以支持仿真建模。研究指出，为成功完成验证，必须满足可测试标准的要求，从而缩短建模与仿真所需时间。为此，必须对数字产品进行重新设计（重构），以最大限度地扩展新电路模型中的逻辑元素。本文提出了一种向量-矩阵技术，用于构建测试函数与结构的映射，而无需依赖算法或处理器指令。文中还给出了通过图结构构造实现任意结构、功能、真值表和矩阵的机制，确定了结构化逻辑常数，从而能够以更先进的技术手段解决功能或结构测试映射的建模难题。"
  },
  {
    "date": "2025-12-29",
    "title": "PaSSER: A Platform for Evaluating LLMs in RAG",
    "authors": "Miroslava Dimitrova, Ivan Popchev, Irina Radeva",
    "publish": "2025 International Conference on Big Data, Knowledge and Control Systems Engineering (BdKCSE)",
    "url": "https://doi.org/10.1109/bdkcse67969.2025.11300500",
    "source": "IEEE",
    "abstract": "This paper describes PaSSER, an open-source platform designed for the evaluation of RAG. Current evaluation tools operate with fixed configurations. This restricts researchers' ability to study the impact of retrieval settings on model performance. PaSSER addresses this issue by enabling the systematic variation of retrieval parameters while ensuring reproducible results. The platform integrates three key components. A web-based interface supports both normal and score retrieval modes with configurable similarity thresholds. A computational backend aggregates eleven evaluation metrics into a weighted composite performance score. Blockchain-based logging ensures that parameter settings and results are immutable and can be verified independently. A controlled evaluation using the Mistral 7B model demonstrates the platform's capability. Two similarity thresholds (0.85 vs. 0.55) were compared on a domain-specific dataset. The evaluation reveals measurable differences between lexical precision and semantic breadth approaches. The scope of this paper is limited to demonstrating the functionality of the platform. Blockchain integration provides tamper-proof experimental records. This addresses the challenge of reproducibility in RAG research, where parameter variations can significantly impact results, yet are often inadequately documented. Future development will focus on automated parameter exploration, expanded metric libraries, additional model integrations, and multi-domain dataset management to support larger-scale comparative studies.",
    "title_zh": "PaSSER：一个用于评估大语言模型在检索增强生成（RAG）中表现的平台",
    "abstract_zh": "本文介绍了PaSSER，一个专为RAG（检索增强生成）模型评估而设计的开源平台。当前的评估工具大多采用固定配置，这限制了研究人员探究检索设置对模型性能影响的能力。PaSSER通过允许系统性地调整检索参数，同时确保结果可复现，解决了这一问题。该平台集成了三个核心组件：一个基于Web的界面，支持常规检索与评分检索两种模式，并可配置相似度阈值；一个计算后端，将十一个评估指标整合为加权综合性能得分；以及基于区块链的日志记录机制，确保参数设置和实验结果不可篡改，且可独立验证。\n\n通过使用Mistral 7B模型进行受控评估，展示了该平台的能力。在特定领域数据集上，对比了两个不同的相似度阈值（0.85 vs. 0.55），评估结果揭示了词法精确性方法与语义广度方法之间存在可测量的差异。本文的研究范围仅限于展示平台的功能。区块链的集成提供了防篡改的实验记录，有效应对了RAG研究中可复现性难题——参数变化可能显著影响结果，但往往缺乏充分记录。\n\n未来的发展方向将聚焦于自动化参数探索、扩展评估指标库、增加更多模型集成，以及支持多领域数据集管理，以推动更大规模的对比研究。"
  },
  {
    "date": "2025-12-29",
    "title": "An AI-Enhanced, Case-Based Approach to Blended C Programming Instruction",
    "authors": "Dan Zhang, Jianpeng Zhang, Dong Zhao, Xiujuan Ma, Jiawei Wang",
    "publish": "2025 6th International Conference on Modern Education and Information Management (ICMEIM)",
    "url": "https://doi.org/10.1109/icmeim66684.2025.11307020",
    "source": "IEEE",
    "abstract": "With the rapid advancement of artificial intelligence (AI), traditional programming instruction must adapt to the evolving demands of interdisciplinary talent cultivation. This paper proposes an AI-empowered, casedriven teaching model for the “C Programming Language” course, integrating AI-assisted tools, real-world case studies, and ideological and political education to foster both technical competence and social responsibility. Practical implementation demonstrates that the model enhances student engagement, programming and data processing skills, and awareness of ethical and civic values. Comparative analysis further shows that students from different genders, regions, and programming backgrounds benefit in an inclusive manner, helping to narrow learning disparities. Future directions include expanding the case library, extending the model to other programming languages such as Python and Java, and addressing challenges related to data privacy and responsible AI use, thereby offering a sustainable pathway for curriculum reform in computer science education",
    "title_zh": "一种基于案例的、融合人工智能的混合式C语言编程教学方法",
    "abstract_zh": "随着人工智能（AI）的快速发展，传统编程教学必须适应跨学科人才培养的不断变化需求。本文提出了一种以人工智能赋能、案例驱动的《C语言程序设计》课程教学模式，融合AI辅助工具、真实案例研究以及思想政治教育，旨在培养学生的专业技术能力与社会责任意识。实践应用表明，该模式有效提升了学生的学习参与度，增强了编程与数据处理能力，并提高了对伦理规范与公民价值的认知。对比分析进一步显示，不同性别、地区及编程背景的学生均能从中受益，实现了包容性发展，有助于缩小学习差距。未来方向包括扩充案例库资源，将该模式推广至Python、Java等其他编程语言课程，并应对数据隐私保护与负责任使用AI所面临的挑战，从而为计算机科学教育的课程改革提供可持续的发展路径。"
  },
  {
    "date": "2025-12-29",
    "title": "Bridging the Semantic Gap in Metadata Management using Large Language Models",
    "authors": "Valery Pyatetsky, Alexander Suleykin, Valentina Sorokina",
    "publish": "2025 7th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)",
    "url": "https://doi.org/10.1109/summa68668.2025.11302314",
    "source": "IEEE",
    "abstract": "Effective metadata management is fundamental to data governance, ensuring that data assets are discoverable, understandable, and usable across the enterprise. However, traditional metadata systems often remain purely technical, describing structures without conveying business meaning. This disconnect — known as the semantic gap — limits the interpretability and value of metadata for business users. To address this challenge, this paper proposes a Cognitive Data Catalog (CDC) framework that leverages Large Language Models (LLMs) to generate human-readable, business-oriented descriptions for tables and attributes using their names, data types, and sample values. A quantitative measure, the Semantic Gap Index (SGI), is introduced to assess the improvement in semantic richness achieved through LLM-based enrichment.Experimental evaluation with several LLMs demonstrates that the proposed approach substantially enhances metadata completeness and interpretability, reduces manual documentation effort, and accelerates data discovery. The results confirm the potential of LLMs to advance metadata management by transforming conventional catalogs into intelligent, semantically aware knowledge systems.",
    "title_zh": "利用大型语言模型弥合元数据管理中的语义鸿沟",
    "abstract_zh": "有效的元数据管理是数据治理的基础，确保企业范围内的数据资产可被发现、可理解且可使用。然而，传统的元数据系统往往仅停留在技术层面，仅描述数据结构而未能传达业务含义。这种“语义鸿沟”导致元数据对业务用户而言难以理解，降低了其价值。为应对这一挑战，本文提出一种认知数据目录（Cognitive Data Catalog, CDC）框架，利用大语言模型（LLM）根据表和属性的名称、数据类型及样本值，生成易于理解、面向业务的自然语言描述。为此，本文引入一个定量指标——语义鸿沟指数（Semantic Gap Index, SGI），用于评估基于LLM的元数据丰富化所带来的语义增强效果。通过多种LLM的实验评估表明，该方法显著提升了元数据的完整性与可读性，减少了人工文档编写的工作量，并加快了数据发现速度。结果证实，大语言模型在推动元数据管理方面具有巨大潜力，能够将传统数据目录转变为智能、具备语义感知能力的知识系统。"
  },
  {
    "date": "2025-12-29",
    "title": "Weighted Community Division for Automated Software Architecture Refactoring",
    "authors": "Sirong Zhao, Jialing Yang, Jiao Xie, Kaiwei Fan, Jianmei Lei, Guoqi Xie",
    "publish": "IEEE Transactions on Software Engineering",
    "url": "https://doi.org/10.1109/tse.2025.3648996",
    "source": "IEEE",
    "abstract": "Adopting Model-Based Development (MBD) in automotive software becomes necessary because it provides a rigorous development process. As the increase in the number of software components (SWCs) and their interactions, modifications to one part of the software might impact other parts due to the high coupling of SWCs. Developing an automated software refactoring technique to implement high cohesion and low coupling of software is necessary. Software refactoring usually contains code refactoring and architecture refactoring. Code refactoring only modifies the code inside the SWCs without changing its original functionality and external behavior, whereas current architecture refactoring does not consider the interaction weight (strength) between SWCs. <p xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">In this study, we propose a weighted community division algorithm called Weighted-Girvan-Newman (W-GN) for software architecture refactoring. W-GN algorithm refactors the architecture by dividing SWCs into modules based on the interaction weights of SWCs. We further develop an automated architecture refactoring tool called AutoToolMD. This tool assists engineers in improving development efficiency and ensures the architecture aligns with the requirements of high cohesion and low coupling. Evaluations show that W-GN indicates more suitable modularity values than the traditional Girvan-Newman (GN) algorithm because the former can better reflect the real architecture with interaction weights. We apply AutoToolMD to automatically refactor architecture in automotive industry practice. Case study with the Zone_B PwrSplyMngt and Zone L_Lock modules in the Honda zone control software shows that AutoToolMD effectively refactors architecture with a significant improvement in testing efficiency and readability.</p>",
    "title_zh": "加权社区划分在自动化软件架构重构中的应用",
    "abstract_zh": "在汽车软件开发中采用基于模型的开发（MBD）变得至关重要，因为它提供了一种严谨的开发流程。随着软件组件（SWCs）数量及其相互作用的增加，软件某一部分的修改可能会因SWCs之间高度耦合而影响其他部分。因此，开发一种自动化的软件重构技术以实现软件的高内聚性和低耦合性十分必要。软件重构通常包括代码重构和架构重构。代码重构仅修改SWCs内部的代码，而不改变其原有的功能和外部行为；而现有的架构重构方法并未考虑SWCs之间的交互权重（强度）。<p xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">本研究提出了一种名为加权Girvan-Newman（W-GN）的加权社区划分算法，用于软件架构重构。该算法通过基于SWCs之间的交互权重将软件组件划分为模块，从而实现架构重构。我们进一步开发了一款自动化架构重构工具——AutoToolMD。该工具可帮助工程师提升开发效率，并确保软件架构满足高内聚、低耦合的要求。评估结果表明，与传统的Girvan-Newman（GN）算法相比，W-GN算法能给出更合适的模块化数值，因为它更能真实反映具有交互权重的软件架构特征。我们将AutoToolMD应用于汽车工业实践中的自动架构重构。以本田区域控制软件中的Zone_B_PwrSplyMngt和Zone_L_Lock模块为例的案例研究显示，AutoToolMD能够有效重构架构，在测试效率和代码可读性方面均取得了显著提升。</p>"
  },
  {
    "date": "2025-12-29",
    "title": "Circuits for Secure Root-of-Trust: TRNG, PUF, Attack-Resistant Cryptographic Accelerators [Feature]",
    "authors": "Sanu K. Mathew, Raghavan Kumar, Vivek De",
    "publish": "IEEE Circuits and Systems Magazine",
    "url": "https://doi.org/10.1109/mcas.2025.3595508",
    "source": "IEEE",
    "abstract": "Silicon-embedded root-of-trust circuits such as True Random Number Generators (TRNG) and Physically Unclonable Functions (PUF) are foundational primitives that generate dynamic and static entropy essential for data security and privacy applications. At the same time, secure platforms are vulnerable to physical side-channel and fault-injection attacks that attempt to steal embedded secret keys/IDs. This article describes all-digital area-efficient implementations of TRNGs and PUFs that generate cryptographic quality random bitstreams with tolerance to process/voltage/temperature variations. These include: (a) Ring-oscillator-based TRNG implementations that employ multiple injected-edges and beat frequency detectors to harvest and convert thermal noise-induced jitter into random bitstreams with throughput and energy-efficiency of 23Mbps and 15Mbps/mW respectively; (b) SRAM PUF with data-remanence bit-masking technique to identify 100% stable bitcells; (c) Unified PUF-TRNG with a common-entropy source and self-calibrating Von Neumann extractor circuits generating PUF/TRNG random bit throughput of 0.56/1.48Gbps with 0.9996/0.99997 static/dynamic entropy respectively. A reconfigurable AES accelerator with minimum-traces-to-disclosure (MTD)<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$&gt;1\\text{B}$ </tex-math></inline-formula> traces in on-demand SCA-resistant mode, while also providing a <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$2.2\\times $ </tex-math></inline-formula> boost in encryption performance during a dual-core mode of operation is also described. Finally, a self-checking AES engine that uses arithmetic and parity checkers for real-time detection of laser and voltage-based attacks with up to 99.99% fault coverage is described.",
    "title_zh": "安全信任根电路：真随机数生成器（TRNG）、物理不可克隆函数（PUF）及抗攻击密码加速器 [专题]",
    "abstract_zh": "嵌入式硅基信任根电路，如真随机数生成器（TRNG）和物理不可克隆函数（PUF），是数据安全与隐私应用中生成动态与静态熵的核心基础构件。然而，安全平台仍易受到物理侧信道攻击和故障注入攻击的威胁，这些攻击旨在窃取嵌入的密钥或身份标识。本文介绍了全数字、面积高效的TRNG与PUF实现方案，能够在工艺、电压、温度变化条件下保持稳定，生成具备密码学质量的随机比特流。具体包括：(a) 基于环形振荡器的TRNG实现，采用多路注入边沿与拍频检测器，将热噪声引起的抖动有效采集并转换为随机比特流，分别实现23Mbps的吞吐率和15Mbps/mW的能量效率；(b) 采用数据残余位掩码技术的SRAM PUF，可识别100%稳定的比特单元；(c) 统一型PUF-TRNG架构，共享熵源并集成自校准的冯·诺依曼提取器电路，实现PUF/TRNG的随机比特输出速率分别为0.56/1.48Gbps，静态/动态熵值分别达到0.9996/0.99997。此外，本文还描述了一种可重构的AES加速器，在按需启用的抗侧信道攻击模式下，最小泄露痕迹（MTD）超过10亿次，同时在双核运行模式下加密性能提升2.2倍。最后，提出一种自检式AES引擎，通过算术与奇偶校验器实现实时检测激光与电压攻击，故障覆盖率达99.99%。"
  },
  {
    "date": "2025-12-29",
    "title": "A Machine Learning Method to Determine the Optimal Number of Stages for a Ring Oscillator",
    "authors": "Vazgen Melikyan, Tatevik Chilingaryan, Gevorg Harutyunyan, Davit Marukhyan, Arthur Sahakyan, Manvel Grigoryan",
    "publish": "2025 IEEE East-West Design &amp;amp; Test Symposium (EWDTS)",
    "url": "https://doi.org/10.1109/ewdts67441.2025.11303710",
    "source": "IEEE",
    "abstract": "An effective method of automated selection of the number of stages of one of the most important and widespread blocks used in digital integrated circuits, the ring oscillator, based on machine learning (ML), is presented. In addition to the ability to automatically determine the number of stages, it also provides the best tradeoff between the main parameters: power, area and performance due to the ability to take the peculiarities caused by the technological process into account. The use of the ML method made it possible to reduce the duration of the RO stage calculation by approximately 4-5 times, at the expense of a 3,57% loss of accuracy.",
    "title_zh": "一种基于机器学习的方法来确定环形振荡器的最佳级数",
    "abstract_zh": "本文提出了一种基于机器学习（ML）的自动化方法，用于确定数字集成电路中最为重要且应用最广泛的模块之一——环形振荡器（Ring Oscillator, RO）的级数。该方法不仅能够自动确定级数，还能通过考虑工艺过程带来的特殊因素，在功耗、面积和性能之间实现最佳权衡。采用机器学习方法后，环形振荡器各级计算时间缩短了约4至5倍，仅损失3.57%的精度。"
  },
  {
    "date": "2025-12-29",
    "title": "Mitigating Syntax and Logic Errors in LLM Based Code Generation via XML-Structured Prompts",
    "authors": "Saket Sambaraju, Jeffrey Boman, Howell Wu, Ziliang Zong",
    "publish": "2025 IEEE International Performance, Computing, and Communications Conference (IPCCC)",
    "url": "https://doi.org/10.1109/ipccc66453.2025.11304661",
    "source": "IEEE",
    "abstract": "Large language models (LLMs) have become powerful tools for automated code generation. Yet, they remain prone to both syntax and logic errors that limit their effectiveness in real-world software development. This paper comprehensively evaluates LLM-generated code utilizing the Codeforces Benchmark, and proposes XML-Structured Prompts (XSP) to improve code quality. Specifically, we present three creative strategies: XSP for improving problem comprehension, Syntax-Aware XSP (SA-XSP) for mitigating syntax errors, and Testcase-Driven Logic Error Debugging (TDLED) for resolving logic errors through test case feedback. Our experiments on 30 LLMs, ranging from <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$0.5 ~\\mathrm{B}, 70 ~\\mathrm{B}$</tex>, to 671 B parameters and including 5 reasoning models, demonstrate that the proposed methods improve accuracy by up to 27.5 %, especially in smaller models, with syntax error reductions of up to 60 % and logic error corrections by up to 30%.",
    "title_zh": "通过XML结构化提示缓解基于大语言模型的代码生成中的语法和逻辑错误",
    "abstract_zh": "大型语言模型（LLMs）已成为自动化代码生成的强大工具。然而，它们仍容易出现语法和逻辑错误，限制了其在真实软件开发中的实际应用效果。本文基于 Codeforces 基准对 LLM 生成的代码进行了全面评估，并提出了 XML 结构化提示（XSP）以提升代码质量。具体而言，我们提出了三种创新策略：利用 XSP 提升问题理解能力，采用语法感知型 XSP（SA-XSP）减少语法错误，以及通过测试用例反馈实现逻辑错误调试的“测试用例驱动逻辑错误调试”（TDLED）方法。我们在 30 种不同规模的 LLM 上进行了实验，涵盖从 0.5B 到 671B 参数不等的模型，包括 5 个具备推理能力的模型。实验结果表明，所提出的方法可将代码准确率最高提升 27.5%，尤其在小型模型中表现显著，语法错误减少高达 60%，逻辑错误修正比例最高达 30%。"
  },
  {
    "date": "2025-12-29",
    "title": "Implementation of I2C Protocol with Adaptive Baud Rate for N Number of Bits Using VERILOG",
    "authors": "S Aparna, C. Padma",
    "publish": "2025 7th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)",
    "url": "https://doi.org/10.1109/summa68668.2025.11302299",
    "source": "IEEE",
    "abstract": "For low-speed peripherals in embedded systems, the Inter-Integrated Circuit (I2C) protocol is a popular serial communication standard. In order to accommodate variable communication speeds depending on system requirements, this paper outlines the construction of an I2C protocol with an adaptable baud rate. The design is appropriate for FPGA-based applications because it is built using Verilog HDL. N-bit data width is supported by the suggested system, enabling capability for a range of applications. To maximize efficiency and minimize battery usage, the adaptive baud rate system constantly modifies the transmission speed. Key elements of the I2C master-slave communication framework include clock stretching, data transmission/reception, start/stop condition detection, and acknowledgment processing. Results from simulation and synthesis show how well the architecture works to provide dependable, fast communication while preserving compatibility. A result from simulation and synthesis is showing how well the design works to achieve dependable, fast communication while being compatible with common I2C devices. Applications need low power consumption and adjustable data transfer speed, effective serial transmission, including FPGA-based embedded systems, real-time data gathering, and sensor interface would benefit greatly from the suggested approach.",
    "title_zh": "使用Verilog实现具有自适应波特率的I2C协议以支持N位数据传输",
    "abstract_zh": "在嵌入式系统中，针对低速外设，集成电路（I2C）协议是一种流行的串行通信标准。为了适应不同系统需求下的可变通信速率，本文阐述了一种具备可调波特率的I2C协议设计。该设计采用Verilog HDL实现，适用于基于FPGA的应用场景。所提出的系统支持N位数据宽度，能够满足多种应用需求。通过自适应波特率机制，系统可动态调整传输速度，从而在保证通信效率的同时最大限度降低功耗。I2C主从通信框架中的关键功能包括时钟拉伸、数据收发、起始/停止条件检测以及应答处理等。仿真与综合结果表明，该架构在保持与常见I2C设备兼容性的前提下，能够实现可靠且快速的通信。该设计特别适用于对低功耗和可调数据传输速率有要求的应用场景，如基于FPGA的嵌入式系统、实时数据采集以及传感器接口等，具有显著的应用价值。"
  },
  {
    "date": "2025-12-29",
    "title": "Research on the Application of Artificial Intelligence in Integrated Circuit Design",
    "authors": "Xueqi Wan, Deen Liu, Changpeng Yu, Qing Xu",
    "publish": "Proceedings of the 4th International Conference on Artificial Intelligence and Intelligent Information Processing",
    "url": "https://doi.org/10.1145/3778534.3778544",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "人工智能在集成电路设计中的应用研究",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-29",
    "title": "Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks",
    "authors": "Xing Hu, Feifei Niu, Junkai Chen, Xin Zhou, Junwei Zhang, Junda He, Xin Xia, David Lo",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3786771",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "评估与推进用于软件工程任务的大语言模型评估基准",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-29",
    "title": "Exploiting Binary Semantics: Enhancing Function Name Inference in Stripped Binaries via LLMs",
    "authors": "Kailong Wang, Han Zhang, Dongliang Fang, Zhongwei Gu, Zhanwei Song, Yongle Chen, Zhiqiang Shi, Limin Sun",
    "publish": "2025 IEEE International Performance, Computing, and Communications Conference (IPCCC)",
    "url": "https://doi.org/10.1109/ipccc66453.2025.11304644",
    "source": "IEEE",
    "abstract": "Function name inference in stripped binaries is a crucial task that supports various security applications, including vulnerability detection and malware analysis. Existing methods suffer from limited model capacity and insufficient exploitation of function semantics, which constrains their ability to comprehend binary code and leads to poor generalization on unseen binaries. To address these problems, we propose BinLLM, a novel framework that leverages large language models (LLMs) to exploit the semantic potential of binary code, thereby enhancing function name inference. Specially, BinLLM integrates three key innovations: (1) source code semantics-guided function name refinement, which mitigates the negative effects of low-quality semantic identifiers during training; (2) A context-aware data collection algorithm that seeks richer semantic dependencies to improve model training and inference performance; (3) parameter-efficient fine-tuning on a domain-specific dataset enriched with semantic knowledge to enhance the model's understanding of binary semantics. These components collectively enhance the model's performance in function name inference on unseen binaries. We evaluate BinLLM on a large-scale dataset comprising <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$2,864,719$</tex> functions across four architectures (x86-64, x86-32, ARM, MIPS) and four optimization levels (<tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathrm{O} 0-\\mathrm{O} 3$</tex>). Experimental results show that BinLLM achieves substantial improvements over state-of-the-art (SOTA) methods, with relative gains of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$320.1 \\%, 274.8 \\%$</tex>, and 297.6 % in precision, recall, and F1-score. Ablation studies further validate the effectiveness of each component in enhancing overall performance.",
    "title_zh": "利用二进制语义：通过大语言模型增强剥离二进制文件中的函数名称推断",
    "abstract_zh": "在剥离的二进制文件中进行函数名推断是一项关键任务，对漏洞检测、恶意软件分析等安全应用具有重要意义。现有方法受限于模型容量不足以及对函数语义信息利用不充分，导致其难以充分理解二进制代码，在未见过的二进制文件上泛化能力较差。为解决上述问题，我们提出了一种名为 BinLLM 的新框架，该框架利用大语言模型（LLMs）挖掘二进制代码的语义潜力，从而提升函数名推断性能。BinLLM 集成了三项关键技术革新：（1）基于源代码语义引导的函数名优化机制，有效缓解了训练过程中低质量语义标识符带来的负面影响；（2）一种上下文感知的数据收集算法，能够挖掘更丰富的语义依赖关系，从而提升模型训练与推理性能；（3）在富含语义知识的领域特定数据集上进行参数高效的微调，增强模型对二进制语义的理解能力。这些组件协同作用，显著提升了模型在未见二进制文件上的函数名推断表现。\n\n我们在一个大规模数据集上评估了 BinLLM，该数据集包含四种架构（x86-64、x86-32、ARM、MIPS）和四种优化级别（O0–O3）下的共计 2,864,719 个函数。实验结果表明，BinLLM 相较于当前最先进的方法，在精确率、召回率和 F1 分数上分别实现了 320.1%、274.8% 和 297.6% 的相对提升。消融实验进一步验证了各组件对整体性能提升的有效性。"
  },
  {
    "date": "2025-12-29",
    "title": "Cybersecurity and AI-generated programming code",
    "authors": "Willian DImitrov, Dragostin Tsecov, Valentin Ruzhenov, Alexander Kirkov, Yoana Hadzhiyska, Svetlana Syarova",
    "publish": "2025 International Conference on Big Data, Knowledge and Control Systems Engineering (BdKCSE)",
    "url": "https://doi.org/10.1109/bdkcse67969.2025.11300524",
    "source": "IEEE",
    "abstract": "The article aims to present a study with analysis and systematization of the consequences for cybersecurity following the application of artificial intelligence in generating program code. A systematization of the problems related to the security of program code caused by the use of AI in programming is proposed. It contains a brief analysis of the individual issues and recommendations for practical measures. In conclusion, an analysis is made, from which conclusions are drawn for a wide range of research topics related to the security of work with the capabilities of AI-produced program code.",
    "title_zh": "网络安全与人工智能生成的编程代码",
    "abstract_zh": "本文旨在对人工智能在生成程序代码中的应用所引发的网络安全后果进行研究、分析与系统化梳理。文章提出了一个关于人工智能编程使用导致程序代码安全问题的系统化整理，简要分析了各项具体问题，并提出了相应的实践性建议。最后，通过综合分析，得出了一系列适用于广泛研究领域中与人工智能生成代码的安全性相关问题的结论。"
  },
  {
    "date": "2025-12-29",
    "title": "Challenges and Opportunities for Circuits and Systems for Next Generation Satellite Payloads [Feature]",
    "authors": "Piero Angeletti, Salvatore D’Addio",
    "publish": "IEEE Circuits and Systems Magazine",
    "url": "https://doi.org/10.1109/mcas.2025.3623964",
    "source": "IEEE",
    "abstract": "The evolution of satellite communication systems is increasingly driven by the need for high-performance, flexible, and efficient solutions, particularly in the design and implementation of circuits and systems for next-generation satellite payloads. Software defined payloads leveraging on On-Board Digital Signal Processing (OBP), Multi-Beam Antennas (MBAs) and Beam-Forming Networks (BFNs) are pivotal technologies in addressing the challenges posed by emerging communication standards, such as 5G/6G, as well as advanced satellite communication, navigation, and remote sensing systems. The realization of these flexible payloads presents significant challenges for circuits and systems designers. These include developing high-frequency, RF and mixed-signal circuits for active phased arrays, designing power-efficient digital signal processing architectures for real-time routing and beamforming, ensuring robust power management systems, and achieving extreme miniaturization through advanced VLSI and SoC integration while maintaining radiation hardness and efficient thermal management. These demands, however, create vast opportunities for innovation in novel analog and digital circuit architectures, efficient data converters, high-speed interfaces, and integrated system-on-chip solutions that leverage advanced semiconductor processes for space-grade reliability and performance. This synergy between evolving market demands, cutting-edge payload technologies, and the continuous innovation in circuit and system design is transforming SATCOM systems, and is essential for industry’s continued growth and relevance.",
    "title_zh": "下一代卫星有效载荷的电路与系统：挑战与机遇 [专题]",
    "abstract_zh": "卫星通信系统的发展正日益受到对高性能、灵活性和高效解决方案的需求推动，尤其是在下一代卫星有效载荷的电路与系统设计与实现方面。基于星上数字信号处理（OBP）、多波束天线（MBAs）以及波束成形网络（BFNs）的软件定义有效载荷，是应对5G/6G等新兴通信标准，以及先进卫星通信、导航和遥感系统所面临挑战的关键技术。然而，实现这些灵活有效载荷对电路与系统设计师提出了巨大挑战，包括：开发用于有源相控阵的高频射频（RF）及混合信号电路；设计功耗高效的数字信号处理架构以实现实时路由与波束成形；确保稳健的电源管理系统；以及通过先进的超大规模集成（VLSI）与系统级芯片（SoC）集成实现极致小型化，同时保持抗辐射能力并具备高效的热管理性能。这些严苛要求也为新型模拟与数字电路架构、高效数据转换器、高速接口以及基于先进半导体工艺的片上集成系统解决方案带来了巨大的创新机遇，从而保障空间级应用的可靠性与高性能。市场趋势的不断演进、尖端有效载荷技术的进步，以及电路与系统设计领域的持续创新之间的协同作用，正在深刻重塑卫星通信（SATCOM）系统格局，对于产业的持续发展与保持竞争力至关重要。"
  },
  {
    "date": "2025-12-29",
    "title": "DNSHolmes: A Scalable Framework for DNS Repair Using Large Language Models",
    "authors": "Kaiqiang Hu, Haizhou Du, Ziyi Wang",
    "publish": "2025 IEEE International Performance, Computing, and Communications Conference (IPCCC)",
    "url": "https://doi.org/10.1109/ipccc66453.2025.11304647",
    "source": "IEEE",
    "abstract": "With the widespread application of large language models (LLMs), their advantages in context analysis and semantic inferencing are becoming increasingly prominent, and we are attempting to introduce them into the realm of DNS repair. As is well known, DNS, as the core of Internet infrastructure, has complex policies and a fragile system, where even a small misconfiguration can lead to catastrophic service failures. Especially in large-scale networks, analyzing detected errors and generating repair solutions often requires operators to invest a significant amount of time and effort. This paper proposes a scalable framework, DNSHolmes, designed to leverage LLMs for generating DNS configuration repair solutions. Specifically, this method first addresses the numerous potential root causes in large-scale errors by abstracting them into a small number of State Equivalence Classes (SECs). It then adopts a deterministic finite automaton (DFA) to compute a Critical Path Graph (CPG) for each class, precisely isolating the minimal set of records responsible for the failure. Crucially, the CPG serves as a focused, verifiable context for a Large Language Model (LLM), guiding it to generate accurate patches while overcoming the fundamental context-length limitations that help LLM with effective repair reasoning. Our evaluation on large-scale public datasets and a real-world campus network demonstrates that DNSHolmes can reduce operational effort by 78.4% and achieve efficient repair in large-scale DNS configuration.",
    "title_zh": "DNSHolmes：一种基于大语言模型的可扩展DNS修复框架",
    "abstract_zh": "随着大型语言模型（LLMs）的广泛应用，其在上下文分析与语义推理方面的优势日益凸显，我们正尝试将它们引入DNS修复领域。众所周知，DNS作为互联网基础设施的核心，具有复杂的策略和脆弱的系统结构，哪怕是一次微小的配置错误，都可能导致灾难性的服务中断。尤其在大规模网络环境中，分析检测到的错误并生成修复方案往往需要运维人员投入大量时间和精力。本文提出了一种可扩展的框架——DNSHolmes，旨在利用大型语言模型生成DNS配置修复方案。具体而言，该方法首先通过将大规模错误中可能存在的众多根本原因抽象为少数几类状态等价类（State Equivalence Classes, SECs），从而简化问题复杂度；随后，采用确定性有限自动机（DFA）为每一类计算出关键路径图（Critical Path Graph, CPG），精准定位导致故障的最小记录集合。尤为重要的是，CPG为大型语言模型（LLM）提供了聚焦且可验证的上下文，引导其生成准确的修复补丁，同时克服了LLM固有的上下文长度限制，从而实现有效的修复推理。我们在大规模公开数据集及一个真实校园网络环境中的评估表明，DNSHolmes可将运维工作量降低78.4%，并在大规模DNS配置修复中实现了高效、可靠的修复能力。"
  },
  {
    "date": "2025-12-29",
    "title": "Design &amp; Implementation of Karatsuba Multiplier: A complete RTL to GDS II Flow",
    "authors": "Peketi Reshma, A S Surabhi, Sujatha Hiremath",
    "publish": "2025 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",
    "url": "https://doi.org/10.1109/conecct65861.2025.11306762",
    "source": "IEEE",
    "abstract": "Efficient multiplier design is critical for DSP, cryptography, and high-performance computing. The Karatsuba multiplication algorithm reduces computation but is significantly reliant on the adder used for partial product accumulation. The Brent-Kung adder, which has an area of $408 \\mu \\mathrm{~m} 2$, a power consumption of $45.78 \\mu \\mathrm{~W}$, and a timing slack of 1 ns, is the most efficient option for this design, based on a comparative analysis of adders, including the Brent-Kung, Carry Select, and Ripple Carry Adders. This paper introduces a 32-bit Karatsuba multiplier based on a Brent-Kung (BK) adder that is optimised for power, area, and performance. In addition, scan flip-flops are used to improve testing and reliability throughout manufacturing testing. The Karatsuba algorithm is designed and implemented with the Brent-Kung adder’s parallel prefix structure, which reduces carry propagation time and increases total computational performance. Cadence EDA tools were used to implement the RTL to GDSII pipeline. The Brent-Kung adderbased design resulted in a 27.8% reduction in area compared to the conventional design employing square root carry select adder. The existing design has a slack of 0.068 ns, indicating improved timing closure. Post-layout simulations show that the BrentKung adder-based Karatsuba multiplier outperforms traditional architectures, making it an excellent choice for high-performance, energy-efficient computing applications.",
    "title_zh": "卡拉茨巴乘法器的设计与实现：完整的RTL到GDS II流程",
    "abstract_zh": "高效乘法器设计在数字信号处理（DSP）、密码学以及高性能计算中至关重要。卡氏乘法算法虽能减少计算量，但其性能高度依赖于用于部分积累加的加法器。通过对Brent-Kung、选择进位（Carry Select）和行波进位（Ripple Carry）加法器的对比分析发现，Brent-Kung加法器在面积（408 μm²）、功耗（45.78 μW）和时序余量（1 ns）方面表现最优，是该设计中最优选择。本文提出一种基于优化的Brent-Kung（BK）加法器实现的32位卡氏乘法器，重点兼顾功耗、面积与性能的优化。此外，采用扫描寄存器（scan flip-flops）以提升制造测试阶段的可测性和可靠性。通过利用Brent-Kung加法器的并行前缀结构，卡氏算法的设计与实现有效降低了进位传播延迟，显著提升了整体计算性能。本设计采用Cadence EDA工具完成从RTL到GDSII的全流程实现。结果表明，基于Brent-Kung加法器的方案相较传统采用平方根选择进位加法器的设计，面积减少了27.8%；现有设计的时序余量为0.068 ns，表明时序收敛性得到改善。后版图仿真验证了基于Brent-Kung加法器的卡氏乘法器在性能上优于传统架构，因此非常适用于对高效率与低功耗要求严苛的计算应用。"
  },
  {
    "date": "2025-12-29",
    "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations",
    "authors": "Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan",
    "publish": "IEEE Internet of Things Journal",
    "url": "https://doi.org/10.1109/jiot.2025.3649376",
    "source": "IEEE",
    "abstract": "Recent advances in Large Language Models (LLMs) have revolutionized mobile robots, including unmanned aerial vehicles (UAVs), enabling their intelligent operation within Internet of Things (IoT) ecosystems. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations in IoT applications. In this paper, we propose a closed-loop LLM-driven UAV operation code generation framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into semantic trajectory descriptions to enhance the evaluator LLM’s understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline methods in terms of success rate and completeness with the increase of task complexity.",
    "title_zh": "基于大语言模型驱动的语义观测闭环无人机运行",
    "abstract_zh": "近年来，大型语言模型（LLMs）的进展彻底改变了移动机器人技术，包括无人机（UAVs），使其能够在物联网（IoT）生态系统中实现智能化运行。然而，LLMs在逻辑推理和复杂决策方面仍面临挑战，这引发了人们对LLM驱动的无人机在物联网应用中操作可靠性的担忧。本文提出了一种闭环式LLM驱动的无人机操作代码生成框架，通过两个LLM模块——代码生成器和评估器——实现有效的反馈与迭代优化，从而保障无人机操作的可靠性。我们的框架将无人机运行中的数值状态观测转化为语义轨迹描述，以增强评估器LLM对无人机动态行为的理解，进而生成更精准的反馈。此外，该框架还引入基于仿真的优化流程，避免了在迭代优化过程中因错误代码执行而对实际无人机造成的潜在风险。我们在不同复杂度的无人机控制任务上进行了大量实验。实验结果表明，随着任务复杂度的增加，本框架能够利用LLMs实现可靠的无人机操作，在成功率和任务完成度方面显著优于基线方法。"
  },
  {
    "date": "2025-12-29",
    "title": "Design of Digital Beam Steering Network with Early Verification for Active Phased Array Radars",
    "authors": "Taniza Roy, Abhinandan Sarkar, Shreeshail, Jayabrata Chakrabarty, M Sheik Althaf",
    "publish": "2025 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",
    "url": "https://doi.org/10.1109/conecct65861.2025.11306602",
    "source": "IEEE",
    "abstract": "Modern phased array radars are equipped with large number of phase shifters along with antenna elements in the front end of the system to accomplish beam steering. Beam steering directs the main lobe of a radiation pattern by adjusting the relative phases of these phase shifters of the antenna elements. The work puts forward design of a hierarchical digital beam steering network with programmable controllers at each level responsible for computation of the beam parameters such as phase and amplitude values for each radiating element of the radar for beam steering requirements in different modes of operation of radar. The design is also responsible for the beam synchronization with respect to the other subsystems of the radar and preparation of multiple control messages per burst to be sent to other subsystems for synchronised operation of the radar. In this work development and verification of the design are conducted concurrently and integrated throughout the design process enabling early verification of the design with respect to requirements. A structured design implementation is followed comprising of stages of requirements capture, conceptual design, detailed design followed by test plans with traceability matrix and rigorous testing which improves development efficiency and minimizes time spent on testing. The designed system has the capability of automatic remote update of software & firmware of the system.",
    "title_zh": "有源相控阵雷达数字波束赋形网络的设计及早期验证",
    "abstract_zh": "现代相控阵雷达在系统前端的天线单元上配备了大量移相器，以实现波束扫描。波束扫描通过调节天线单元移相器之间的相对相位来控制辐射图样的主波束方向。本文提出了一种分层式数字波束扫描网络的设计，该设计在每一层级均配置可编程控制器，负责计算雷达在不同工作模式下所需的波束参数，如每个辐射单元的相位和幅度值，以满足波束扫描需求。该设计还负责与其他雷达子系统的波束同步，并在每个脉冲周期内生成多个控制消息，以确保雷达各子系统间的协同运行。本研究在设计开发与验证过程中采取并行集成的方法，贯穿整个设计流程，实现了对设计早期符合性要求的及时验证。设计实施采用结构化流程，包括需求捕获、概念设计、详细设计，以及带有可追溯性矩阵的测试计划和严格的测试环节，从而提高了开发效率，并减少了测试所耗费的时间。所设计的系统具备远程自动更新软件与固件的能力。"
  },
  {
    "date": "2025-12-29",
    "title": "On the Use of AI in Signal Processing Programming Applications at the Master of Science Level",
    "authors": "Constantin George Druga, Gabriel Sirbu, Dorel Aiordachioaie",
    "publish": "2025 9th International Symposium on Electrical and Electronics Engineering (ISEEE)",
    "url": "https://doi.org/10.1109/iseee67817.2025.11304834",
    "source": "IEEE",
    "abstract": "The paper presents the results of an analysis related to the use of AI tools in running signal processing courses and applications, at the Master of Science level, where high-level programming languages such as MATLAB /Octave/ Python are involved. Three AI tools were selected: ChatGPT (OpenAI), Microsoft Copilot, and Google Gemini. As a case study, three signal processing applications are considered where a programming task is involved. These are related to signal generation, function computation, and signal filtering. The solutions of AI tools are presented, analyzed, and compared with an in-house one, from the accuracy, clarity, and compactness criteria. The preliminary conclusion shows that all AI tools provide good solutions related to the considered signal processing tasks. The major concern is the choice of the parameters of the simulations when the experience and knowledge of the end user are dominant. The aspects discussed in this paper indicate the paths that should be followed when programming languages are involved for engineering applications, like signal processing. The aspects considered have a major impact on the educational process, where generative AI seems to dominate and dramatically change the paradigm of education, in general.",
    "title_zh": "人工智能在硕士层次信号处理编程应用中的使用",
    "abstract_zh": "本文介绍了在硕士层次的信号处理课程与应用中使用人工智能工具的相关分析结果，这些课程涉及高级编程语言（如MATLAB/Octave/Python）。研究选取了三种AI工具：ChatGPT（OpenAI）、Microsoft Copilot和Google Gemini。以三个信号处理应用为案例研究，这些应用均包含编程任务，分别涉及信号生成、函数计算和信号滤波。文章展示了各AI工具提供的解决方案，并从准确性、清晰性和简洁性三个维度进行分析与比较，同时与自研方案进行了对照。初步结论表明，所有AI工具在所考虑的信号处理任务中均能提供高质量的解决方案。主要关注点在于仿真参数的选择，此时最终用户的实际经验与知识起决定性作用。本文讨论的各个方面揭示了在工程应用（如信号处理）中使用编程语言时应遵循的发展路径。这些因素对教育过程具有重大影响，表明生成式人工智能正逐渐主导并深刻改变教育的整体范式。"
  },
  {
    "date": "2025-12-29",
    "title": "Modern Day LLMs : A GOLDMINE or a LANDMINE ?",
    "authors": "Amogh R S, A R Muthudhanush, Shwetha M D",
    "publish": "2025 International Conference on Intelligent Systems and Pioneering Innovations in Robotics and Electric Mobility (INSPIRE)",
    "url": "https://doi.org/10.1109/inspire67328.2025.11300550",
    "source": "IEEE",
    "abstract": "This paper presents a systematic analysis of Indirect Prompt Injection (IPI) attacks on Large Language Models (LLMs), examining their dual role as enablers of advanced AI applications and as sources of significant security risk. The study addresses three core research objectives: (1) identifying adversarial techniques that enable effective IPI execution, (2) comparing vulnerabilities across leading LLM architectures, and (3) evaluating the efficacy of existing mitigation strategies. Through empirical testing of Blackbox AI 3.1.6, ChatGPT 3.5, and Gemini 2.0, we find that multi-turn escalation, privilege manipulation, and role-conditioning consistently bypass baseline safety defenses. Our results reveal critical weaknesses in refusal consistency and context-aware filtering. To address these issues, we propose a structured evaluation framework for benchmarking model resilience and outline mitigation pathways with direct deployment relevance, contributing to adversarial robustness research and safer LLM design.",
    "title_zh": "现代大型语言模型：金矿还是定时炸弹？",
    "abstract_zh": "本文对大型语言模型（LLMs）中的间接提示注入（IPI）攻击进行了系统性分析，探讨了其在推动先进人工智能应用的同时，也带来显著安全风险的双重角色。研究聚焦于三个核心目标：（1）识别能够有效实施IPI攻击的对抗性技术；（2）比较主流LLM架构之间的漏洞差异；（3）评估现有缓解策略的有效性。通过对Blackbox AI 3.1.6、ChatGPT 3.5和Gemini 2.0进行实证测试，我们发现多轮升级、权限操纵和角色诱导等手段能够持续绕过基础安全防护机制。研究结果揭示了拒绝响应的一致性以及上下文感知过滤方面的关键缺陷。为应对这些问题，本文提出了一套结构化的评估框架，用于基准测试模型的抗脆弱性，并提出了具有直接部署意义的缓解路径，为对抗鲁棒性研究及更安全的LLM设计提供了重要贡献。"
  },
  {
    "date": "2025-12-29",
    "title": "Generative Diffusion Models for Test Pattern Synthesis in VLSI : Enhancing Fault Coverage and Silicon Lifecycle Management",
    "authors": "Tanuj Mathur, Mohan Vamsi Musunuru, Naveen Kumar Siripuram, Prabhu Muthusamy, Bhargav Kumar Konidena",
    "publish": "2025 IEEE East-West Design &amp;amp; Test Symposium (EWDTS)",
    "url": "https://doi.org/10.1109/ewdts67441.2025.11303688",
    "source": "IEEE",
    "abstract": "The challenges of making semiconductor technologies work reliably, in terms of yield, fault-tolerance, have escalated dramatically, due to the rapid scale-down into the nanoscale regime, and these challenges are even more acute in very-large-scale integration (VLSI) systems. Traditional Automatic Test Pattern Generation (ATPG) tools, well known as the backbone of digital verification, are increasingly not keeping up with the stochastic and dynamic fault model of the advanced nodes. The techniques tend to overedit to a priori fault models, like stuck-at or transition fault models, and thus are not very flexible to newer defect patterns. Furthermore, their over-reliance on exhaustive backtracking and deterministic search, inject too much computational overhead and redundant test vectors that weaken efficiency while managing silicon lifecycle (SLM). Enter recent cutting-edge developments in generative artificial intelligence (GenAI) the prospect of probabilistic models learning complicated patterns of defect incidence, and the apparent need to revisit traditional to the world understanding of diffusional phenomena. Of these, diffusion models are notable in the variety and high quality of the samples of data they are able to produce by iteratively removing noise using generative annealing, rendering them useful at preserving distributional diversity yet alleviating mode collapse, one of the more faults of generative adversarial networks (GANs). In this paper, we develop a Generative Diffusion Model (GDM) framework that is automatic test pattern synthesis oriented in nanoscale VLSI systems. Wide-scale data consisting of defect and failure trace data (combined with benchmark circuit data) is used to train fault-aware representations that generalize beyond classical fault models. Incorporating such an approach into an SLM pipeline can enable a model to be adaptively retrained using incremental defect data such that the ultimate relevance is maintained throughout the device life cycle. Experimental results on the ITC-99 benchmark circuits show a 22 percent enhancement in fault coverage over the traditional ATPG and GAN-based synthesis and a 40 percent decrease in run time, and, further, it produces statistically diverse test vectors with no addition of test vectors. The scalability of the approach to medium-to-large circuits is further demonstrated by a detailed case study and demonstrates a greater diversity of fault activation in a circuit with increased robustness. These findings indicate that there would be a paradigm shift of the application of electronic tests strategy and diffusion-driven synthesis stands tall as a viable and dynamic solution to and management of reliable microelectronic devices.",
    "title_zh": "用于VLSI测试图案生成的生成式扩散模型：提升故障覆盖率与硅片生命周期管理",
    "abstract_zh": "随着半导体技术迅速向纳米尺度发展，其在良率和容错性方面的可靠性挑战急剧加剧，这一问题在超大规模集成（VLSI）系统中尤为突出。传统的自动测试图案生成（ATPG）工具作为数字验证的基石，正越来越难以应对先进制程节点所呈现的随机性和动态故障模型。现有技术往往过度依赖于预先设定的故障模型（如 stuck-at 故障或过渡故障模型），因而对新型缺陷模式缺乏足够的灵活性。此外，它们过分依赖穷举回溯和确定性搜索，导致计算开销过大、测试向量冗余严重，从而削弱了硅片生命周期管理（SLM）的效率。\n\n近年来，生成式人工智能（GenAI）的前沿进展为解决这些问题带来了新希望：概率模型能够学习复杂缺陷发生的模式，也促使我们重新审视传统对扩散现象的理解。其中，扩散模型因其在生成高质量、多样化数据样本方面的卓越表现而备受关注——通过迭代去噪与生成退火过程，它们不仅有效保持了数据分布的多样性，还缓解了生成对抗网络（GAN）中常见的“模式崩溃”问题。\n\n本文提出一种面向纳米尺度VLSI系统的生成扩散模型（Generative Diffusion Model, GDM）框架，专注于自动测试图案合成。该框架利用大规模缺陷与失效轨迹数据（结合基准电路数据）进行训练，构建出具备故障感知能力的表征，其泛化能力超越了经典故障模型。将此方法融入SLM流程后，系统可基于增量缺陷数据实现自适应再训练，从而在整个器件生命周期内持续维持测试的相关性。\n\n在ITC-99基准电路上的实验结果表明，该方法相比传统ATPG和基于GAN的合成方案，故障覆盖率提升了22%，运行时间减少40%，且无需额外添加测试向量即可生成具有统计多样性的测试向量。通过详尽的案例研究进一步验证了该方法在中大型电路中的可扩展性，显示电路在增加鲁棒性的同时，实现了更广泛的故障激活能力。\n\n这些发现表明，电子测试策略的应用将迎来范式转变，以扩散驱动的测试合成正成为一种可行且动态的解决方案，为可靠微电子器件的设计、制造与全生命周期管理提供了强有力支持。"
  },
  {
    "date": "2025-12-29",
    "title": "Logic for Bus Interconnect Testing",
    "authors": "Vladimir Hahanov, David Devadze, Zaza Davitadze, Svetlana Chumachenko, Eugenia Litvinova, Volodymyr Obrizan",
    "publish": "2025 IEEE East-West Design &amp;amp; Test Symposium (EWDTS)",
    "url": "https://doi.org/10.1109/ewdts67441.2025.11303649",
    "source": "IEEE",
    "abstract": "Chiplet is the technological and structural architecture of an emerging computer system. It is an architecture that integrates heterogeneous components such as SoC, SiP, multichip, package, silicon die, and CPU. These structural components form the logical architectures: Chiplet GPU, Chiplet Memory, Chiplet I/O, Chiplet Interconnects. For the market, chip-let has a clear advantage over Monolithic Chips, which is determined by the following properties: Improved Yield, Flexibility, Scalability, and Heterogeneous Integration. As part of emerging computing, a Chiplet is just logic, represented by structures and functions, and nothing else. Chiplet Interconnects are no exception – this is also logic. A systematic approach to modeling, simulation, and testing the logic of Chiplet Interconnects, the model of which is represented by a logical vector, is proposed. The basic paradigm is that any logic should be tested systematically against logical faults that are in the good-value model of the structure or functionality, either explicitly or implicitly. Any physical defects in components or bus Interconnects in the Chiplet architecture manifest as logic faults. Defects violate the logic of any computing device, including bus structures. This means that any physical defects manifest themselves as logical faults. This means generating system tests that cover combinations of logic faults. A Modelling test card is offered, which is a logical relationship between a comprehensive test and all combinations of logic faults. A logic vector is all you need to model a Chiplet Interconnects test map in three matrix operations. As well as any structure and functionality, it can be represented by a logical vector for modeling a test map. In addition, a logical vector is also a data structure for graph rendering of the validity of a process or phenomenon model. A logical vector is the simplest model that is convenient for simulating on a computer and rendering the results for a human. Modeling and rendering on the logical vector harmoniously complement each other in solving market problems. Examples and processing time of bus, graph structures, and functionalities for modeling a test map and graph rendering models are given. All mechanisms for modeling and rendering test maps for functions and structures are embedded in the Python language codes, which are exposed in cyberspace as an open-source application.",
    "title_zh": "总线互连测试逻辑",
    "abstract_zh": "芯粒（Chiplet）是新兴计算机系统的一种技术与结构架构。它是一种将异构组件（如SoC、SiP、多芯片封装、封装体、硅晶片、CPU等）集成在一起的架构。这些结构化组件构成了逻辑架构：芯粒GPU、芯粒内存、芯粒I/O、芯粒互连。从市场角度看，芯粒相较于传统整体式芯片（Monolithic Chips）具有明显优势，这主要由以下特性决定：提升良率、灵活性、可扩展性以及异构集成能力。\n\n作为新兴计算的一部分，芯粒本质上仅是逻辑层面的存在，表现为结构与功能，不包含其他内容。芯粒互连也不例外——它同样属于逻辑范畴。本文提出一种系统化的方法，用于建模、仿真和测试芯粒互连的逻辑，其模型以逻辑向量表示。基本理念是：任何逻辑都应针对结构或功能的良好值模型中所隐含或显式的逻辑故障进行系统性测试。组件或芯粒架构中总线互连的任何物理缺陷，都会表现为逻辑故障。缺陷破坏了任何计算设备（包括总线结构）的逻辑完整性。这意味着，所有物理缺陷最终均表现为逻辑故障。因此，必须生成能够覆盖多种逻辑故障组合的系统级测试。\n\n为此，本文提出一种“建模测试卡”概念，即全面测试与所有逻辑故障组合之间的逻辑关系。仅需一个逻辑向量，即可通过三次矩阵运算完成芯粒互连测试图的建模。此外，任何结构与功能均可通过逻辑向量进行建模，以构建测试图。同时，逻辑向量也是一种用于图形化呈现过程或现象模型有效性的数据结构。逻辑向量是最简单的模型形式，便于在计算机上进行仿真，并将结果直观地呈现给用户。\n\n逻辑向量的建模与可视化相互协调，共同解决市场中的实际问题。文中提供了总线、图结构及功能建模测试图与图形渲染模型的实例，并给出了相应的处理时间数据。所有用于功能与结构测试图建模与可视化的机制均已嵌入Python语言代码中，并以开源应用的形式公开于网络空间，供全球开发者使用与改进。"
  },
  {
    "date": "2025-12-29",
    "title": "Research on an Intelligent Evaluation System for Large Language Models via Simulation-Based Training in Vertical Domains",
    "authors": "Zejian Fu, Jieying Wei, Tao Chen, Guoqiang Wei, Hongyu Liang, Bingzhao Wei",
    "publish": "2025 2nd International Conference on Intelligent Computing and Data Mining (ICDM)",
    "url": "https://doi.org/10.1109/icdm68174.2025.11309472",
    "source": "IEEE",
    "abstract": "Large model technology represents a critical research direction in deep neural networks. Leveraging the groundbreaking innovations of the Transformer architecture and large-scale parameter optimization, it has demonstrated remarkable capabilities in fields such as natural language processing and data analysis. This paper systematically reviews theories and techniques related to large models in vertical domains. To address challenges in constructing an intelligent evaluation system for large models in simulation training, a dynamic rank adjustment-based LoRA fusion approach is proposed. By integrating simulated training datasets and a knowledge base of evaluation criteria, a Retrieval-Augmented Generation based intelligent evaluation system for simulation training is developed. Experimental results show that the proposed system enhances task transfer capability while ensuring professional rigor.",
    "title_zh": "基于垂直领域仿真训练的大语言模型智能评估系统研究",
    "abstract_zh": "大模型技术是深度神经网络领域的一个关键研究方向。依托Transformer架构的突破性创新以及大规模参数优化，该技术在自然语言处理、数据分析等领域展现出卓越的能力。本文系统地回顾了垂直领域内大模型相关的理论与技术。为应对构建仿真训练中大模型智能评估体系所面临的挑战，本文提出了一种基于动态秩调整的LoRA融合方法。通过整合仿真训练数据集与评估标准知识库，构建了一个基于检索增强生成（Retrieval-Augmented Generation）的仿真训练智能评估系统。实验结果表明，所提出的系统在确保专业严谨性的同时，显著提升了任务迁移能力。"
  },
  {
    "date": "2025-12-29",
    "title": "Your Copied Data is Under Monitoring: A Study of Clipboard Usage in Android Applications",
    "authors": "Yongliang Chen, Jiayimei Wang, Ruoqin Tang, Chaoshun Zuo, Lei Xue, Weitao Xu, Xiapu Luo, Qingchuan Zhao",
    "publish": "IEEE Transactions on Dependable and Secure Computing",
    "url": "https://doi.org/10.1109/tdsc.2025.3648998",
    "source": "IEEE",
    "abstract": "Clipboard usage is prevalent in mobile applications nowadays. However, insufficient access control on the clipboard in mobile operating systems exposes its contained data to high risks where one application can read the data, store it locally, or even send it to remote servers. Unfortunately, the literature only has ad-hoc studies in this respect and lacks a comprehensive and systematic study of the entire mobile application ecosystem. Therefore, this paper proposes an automated tool, ClipboardScope+, that leverages the principled static program analysis to uncover the clipboard data usage in mobile applications at scale by defining a usage as a combination of two aspects, i.e., how the clipboard data is validated and where does it go. It defines four primary categories of clipboard data operation, namely spot-on, grand-slam, selective, and cherry-pick, based on the clipboard usage in an application. ClipboardScope+ is evaluated on over <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1.2 million</i> mobile applications available on Google Play, spanning the years 2022 and 2023. It uncovered an increase of 5.9% in behaviors of storing and transferring clipboard data over the one-year time, most of which occur automatically in background services. We also conducted a comprehensive case study to characterize different clipboard usages and reveal their privacy issues. Moreover, we uncovered a prevalent programming habit of using the <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">SharedPreferences</monospace> object to store historical data, which can become an unnoticeable privacy leakage channel.",
    "title_zh": "您的复制数据正在被监控：对Android应用程序中剪贴板使用情况的研究",
    "abstract_zh": "如今，剪贴板的使用在移动应用中十分普遍。然而，移动操作系统对剪贴板访问控制不足，使得其中的数据面临较高风险：一个应用程序可能读取剪贴板内容、将其本地存储，甚至发送至远程服务器。遗憾的是，现有文献仅对此进行了零散的研究，缺乏对整个移动应用生态系统的全面、系统性分析。因此，本文提出了一种自动化工具 ClipboardScope+，该工具基于严谨的静态程序分析技术，能够大规模地揭示移动应用中剪贴板数据的使用情况。其核心思想是将剪贴板数据的使用行为定义为两个方面的组合：一是剪贴板数据如何被验证，二是数据最终流向何处。根据应用中剪贴板的使用方式，ClipboardScope+ 定义了四种主要的操作类别：精准匹配（spot-on）、全盘接收（grand-slam）、选择性处理（selective）和挑拣式获取（cherry-pick）。我们对 Google Play 平台上超过 120 万款移动应用（涵盖 2022 至 2023 年）进行了评估，发现一年内自动存储和传输剪贴板数据的行为增加了 5.9%，其中绝大多数行为由后台服务自动完成。我们还开展了一项全面的案例研究，深入刻画了不同剪贴板使用模式，并揭示了其中存在的隐私问题。此外，我们还发现一种普遍存在的编程习惯：开发者常使用 `SharedPreferences` 对象来存储历史数据，这可能成为一种难以察觉的隐私泄露渠道。"
  },
  {
    "date": "2025-12-29",
    "title": "Rail Routing for Power Distribution Networks in Advanced Multi-Layered Printed Circuit Boards",
    "authors": "Wei-Che Tseng, Zong-Ying Cai, Yi-Ping Huang, Yu-Hsiang Lo, Yao-Wen Chang, Yang Lu, Jerry Bai, Bin-Chyi Tseng",
    "publish": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "url": "https://doi.org/10.1109/tcad.2025.3649456",
    "source": "IEEE",
    "abstract": "Power distribution networks (PDNs) are designed to deliver sufficient and stable power to printed circuit board (PCB) components. With increasing current/voltage demands on modern PCBs, power rails in PDN require more space for metals. Existing design techniques often adopt greedy heuristics to optimize the metal shapes of power rails. However, they consider little spatial competition among different power rails, so routing spaces may be distributed unevenly, causing redundant metal usage, current/voltage violations, and even routing failures in high-current designs. To remedy this problem, we propose a multi-layered PDN design flow to distribute rail spaces according to current/voltage demands appropriately. To our knowledge, this is the first work to consider the spatial competition issue in PCB power rail routing. Specifically, we generate a multi-layered topology and minimize the metal area and the via usage with mathematical programming. To resolve the overlap regions by rail detouring, we also propose a resistance-aware routing algorithm. Experimental results show that our work significantly outperforms a baseline algorithm extended from the state-of-the-art rail router in the metal area and runtime, achieving respective reductions of 55% and 71%, without any current/voltage violations.",
    "title_zh": "先进多层印刷电路板中电力分配网络的布线路径规划",
    "abstract_zh": "电源分配网络（PDN）的设计旨在为印刷电路板（PCB）上的元器件提供充足且稳定的电力。随着现代PCB对电流/电压需求的不断增加，PDN中的电源轨需要更多的金属布线空间。现有的设计方法通常采用贪心启发式算法来优化电源轨的金属形状，但这些方法很少考虑不同电源轨之间的空间竞争问题，导致布线空间分布不均，从而引发金属资源冗余、电流/电压违规，甚至在高电流设计中造成布线失败。为解决这一问题，我们提出了一种多层PDN设计流程，能够根据电流/电压需求合理分配电源轨的空间。据我们所知，这是首个系统性考虑PCB电源轨布线中空间竞争问题的工作。具体而言，我们生成一个多层拓扑结构，并通过数学规划方法最小化金属面积和过孔使用量。为了消除电源轨重叠区域，我们还提出了一种考虑电阻特性的布线算法。实验结果表明，与基于最先进电源轨路由器扩展得到的基线算法相比，我们的方法在金属面积和运行时间上均有显著提升，分别减少了55%和71%，且未出现任何电流/电压违规情况。"
  },
  {
    "date": "2025-12-29",
    "title": "Agentic AI Frameworks Under the Microscope: What Works, What Doesn’t",
    "authors": "Karthik Vaidhyanathan, Davide Taibi",
    "publish": "IEEE Software",
    "url": "https://doi.org/10.1109/ms.2025.3622209",
    "source": "IEEE",
    "abstract": "AI agents are rapidly transforming the software engineering landscape, supported by a wave of new frameworks. Our review of the 10 most representative general-purpose agentic AI frameworks reveals a maturing but fragmented landscape. While some frameworks provide enterprise-ready capabilities, most remain immature, with limited memory models, basic reasoning, and uneven ecosystem support. Significant improvements are needed before these frameworks can enable reliable and large-scale adoption in practice.—Davide Taibi",
    "title_zh": "代理型人工智能框架的深度剖析：哪些有效，哪些无效",
    "abstract_zh": "AI代理正在迅速改变软件工程领域，这得益于一系列新兴框架的推动。我们对十大最具代表性的通用型智能体AI框架进行的综述显示，该领域虽在逐步成熟，但仍呈现碎片化状态。尽管部分框架已具备企业级应用能力，但大多数仍处于不成熟阶段，其记忆模型有限、推理能力基础，生态系统支持也参差不齐。在这些框架能够真正实现可靠且大规模的实际应用之前，仍需进行显著改进。——Davide Taibi"
  },
  {
    "date": "2025-12-29",
    "title": "SoC Implementation of Wildfire Infrared Monitoring and Early Detection System",
    "authors": "Shalmon Hermesh, Shahnam Mirzaei",
    "publish": "IGARSS 2025 - 2025 IEEE International Geoscience and Remote Sensing Symposium",
    "url": "https://doi.org/10.1109/igarss55030.2025.11313927",
    "source": "IEEE",
    "abstract": "This paper details the design and implementation of a wildfire infrared monitoring and early detection system (WIMEDS) on reconfigurable hardware. The proposed system aims to enhance the monitoring and detection of fires, with a particular focus on wildfires. Due to the complex nature of wildfire detection and classification, RGB imaging can be challenging. In contrast, infrared (IR) imaging simplifies this process, as fire captured by an IR camera displays a distinct pattern. This allows for more effective monitoring, detection, processing, and authentication of the images. The system is comprised of four primary components: image capture, software/hardware interface, image processing engine, and alarm module. The hardware module was implemented using the Vivado HLS Design Suite and targeted to the ZYNQ XC7Z020 SoC platform. The design efficiently utilizes FPGA resources, consuming less than 12% of Slice LUTs, 2% of memory LUTs, 5% of Slice registers, 8% of DSP blocks, and 4% of block RAMs. Implementation results demonstrate the system's inherent flame detection capabilities, effectively distinguishing hot flames from the surrounding environment.",
    "title_zh": "野火红外监测与早期预警系统的片上系统（SoC）实现",
    "abstract_zh": "本文详细介绍了在可重构硬件上设计与实现的野火红外监测与早期预警系统（WIMEDS）。该系统旨在提升对火灾，特别是野火的监测与检测能力。由于野火检测与分类具有高度复杂性，基于RGB成像的方法面临较大挑战。相比之下，红外（IR）成像能够显著简化这一过程，因为红外相机捕捉到的火焰会呈现出独特的热分布模式，从而更有效地实现图像的监测、检测、处理与验证。该系统由四个主要模块构成：图像采集、软硬件接口、图像处理引擎以及报警模块。硬件部分采用Vivado HLS设计套件实现，并部署于ZYNQ XC7Z020 SoC平台。设计充分优化了FPGA资源利用率，各项资源占用率均低于以下阈值：Slice LUTs不足12%，内存LUTs仅占2%，Slice寄存器占5%，DSP模块占8%，块RAM占4%。实验结果表明，该系统具备出色的火焰识别能力，能够有效区分高温火焰与周围环境。"
  },
  {
    "date": "2025-12-29",
    "title": "A Novel Hardware Efficient FPGA Implementation of Multi-Linear Regression Algorithm",
    "authors": "Velayutham Sudharsan, Shirshendu Roy",
    "publish": "2025 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",
    "url": "https://doi.org/10.1109/conecct65861.2025.11306597",
    "source": "IEEE",
    "abstract": "Multi-linear regression (MLR) algorithm is simple but one of the powerful machine learning algorithms for prediction where output linearly depends on the independent variables. This work proposes a novel and detailed hardware architecture for the MLR algorithm. The architecture supports both the training and testing phases of prediction operation. A linear system is solved in MLR to find the co-efficient vector. In this work, this linear system in the training phase is solved by the conjugate gradient (CG) algorithm which is an iterative algorithm. CG algorithm supports maximum resource sharing and gives better accuracy than other iterative algorithms. The architecture also uses a Gold-Schmidt iterative divider to reduce hardware complexity. The architecture is implemented on a field programmable gate array (FPGA) for validation of performance with a freely available database of student performance analysis with 5 independent variables. A relative signal-to-ratio (RSNR) of 36 dB is achieved in this work in estimating the co-efficient vector using the CG algorithm and an RSNR of 28 dB is obtained in the prediction of 256 samples. The proposed architecture is hardware efficient, supports online sample-wise training, and has less overall training time compared to all existing works. The architecture is easily applicable to all real-time applications and scalable for any parameter values.",
    "title_zh": "一种新型硬件高效的FPGA多线性回归算法实现",
    "abstract_zh": "多元线性回归（MLR）算法虽然简单，却是预测任务中一种强大且有效的机器学习方法，其输出与自变量呈线性关系。本文提出了一种新颖且详尽的MLR算法硬件架构，该架构支持预测操作的训练和测试两个阶段。在MLR中，需要求解一个线性方程组以获得系数向量。本研究在训练阶段采用共轭梯度（CG）算法来求解该线性系统，该算法是一种迭代算法，具有最大化的资源复用能力，并且相比其他迭代算法能提供更高的精度。此外，该架构还采用了Gold-Schmidt迭代除法器，有效降低了硬件复杂度。为验证性能，该架构在现场可编程门阵列（FPGA）上实现，并使用一个公开可用的学生学业表现分析数据库进行测试，该数据库包含5个自变量。实验结果表明，利用CG算法估计系数向量时，获得了36 dB的相对信噪比（RSNR）；在对256个样本进行预测时，RSNR达到28 dB。所提出的架构具有较高的硬件效率，支持在线逐样本训练，且整体训练时间显著短于现有所有相关工作。该架构易于应用于各类实时场景，并可灵活扩展以适应任意参数值。"
  },
  {
    "date": "2025-12-29",
    "title": "Optimizing Asynchronous Event Dispatch in Modern C++ Publish/Subscribe Systems",
    "authors": "Alex I. Tsvetanov",
    "publish": "2025 33rd National Conference with International Participation (TELECOM)",
    "url": "https://doi.org/10.1109/telecom66943.2025.11304088",
    "source": "IEEE",
    "abstract": "The paper extends the C++20 header-only library pubsub-lib, by integrating two low-overhead asynchronous dispatch strategies: C++ standard execution policies (std::execution::par, par_unseq) and Intel oneAPI TBB. Both reuse thread pools to minimize overhead. Benchmarks on a 12-core AMD Ryzen under Ubuntu/WSL show up to 100× latency reduction and significant throughput gains over std::async, with sub-microsecond per-subscriber times even at 1000 subscribers. The research concludes with guidelines for choosing dispatch strategies based on performance and portability.",
    "title_zh": "现代C++发布/订阅系统中异步事件分发的优化",
    "abstract_zh": "该论文扩展了C++20的头文件仅有的发布/订阅库pubsub-lib，集成了两种低开销的异步调度策略：C++标准执行策略（std::execution::par、par_unseq）以及Intel oneAPI TBB。这两种策略均复用线程池以最大限度降低开销。在Ubuntu/WSL环境下基于12核AMD Ryzen处理器的基准测试显示，与std::async相比，延迟最高可降低100倍，吞吐量显著提升，即使在1000个订阅者的情况下，每个订阅者的处理时间也低于微秒级。研究最后提出了根据性能和可移植性选择调度策略的指导原则。"
  },
  {
    "date": "2025-12-29",
    "title": "Gate-Breaker: An LLM-Powered Netlist-to-RTL Reverse Engineering Tool",
    "authors": "Md Omar Faruque, Peter Jamieson, Ahmad Patooghy, Abdel-Hameed A. Badawy",
    "publish": "2025 IEEE International Performance, Computing, and Communications Conference (IPCCC)",
    "url": "https://doi.org/10.1109/ipccc66453.2025.11304682",
    "source": "IEEE",
    "abstract": "The escalating sophistication of hardware intellectual property (IP) theft, a multi-billion dollar problem for the semiconductor industry, demands novel approaches to both understanding attack vectors and fortifying defenses. This paper evaluates the potential of Large Language Models (LLMs) to reverse engineer Register Transfer Level (RTL) designs from gate-level netlists. We introduce a framework for netlist-to-RTL conversion, leveraging pattern recognition and code generation capabilities of modern LLMs. Our evaluation of four available LLM models across 156 circuit benchmarks reveals that LLMs can indeed recover functional RTL with reasonable accuracy until the RTL benchmarks become our classified 4th quartile of code complexity. We also provide a similarity metrics-based methodology to evaluate and ascertain the quality of reverse engineering. Among the evaluated models, OpenAI's O3-Mini emerged as the best performer with 75.1 % overall success rate. While performance degrades significantly on the most complex 4th quartile (53.4 % success rate), when O3-Mini does succeed on these challenging designs, it maintains a high Abstract Syntax Tree (AST) similarity of 0.924 and achieves a moderate 0.871 Control Flow Graph (CFG) similarity.",
    "title_zh": "破门者：一种基于大语言模型的网表到RTL逆向工程工具",
    "abstract_zh": "硬件知识产权（IP）盗窃的复杂性日益加剧，已成为半导体行业价值数十亿美元的重大问题，亟需采用创新方法来深入理解攻击路径并加强防御措施。本文评估了大型语言模型（LLMs）从门级网表中逆向工程出寄存器传输级（RTL）设计的潜力。我们提出了一种基于现代LLM模式识别与代码生成能力的网表到RTL转换框架。通过对156个电路基准测试集在四种可用LLM模型上的评估发现，LLMs能够在RTL基准测试的代码复杂度达到我们分类的第四分位数之前，以合理准确度恢复出功能正确的RTL代码。此外，我们还提出了一种基于相似性度量的方法论，用于评估和验证逆向工程的质量。在所评估的模型中，OpenAI的O3-Mini表现最佳，整体成功率达到75.1%。尽管在最复杂的第四分位（代码复杂度最高）上性能显著下降，成功率为53.4%，但当O3-Mini成功处理这些复杂设计时，其抽象语法树（AST）相似度仍高达0.924，控制流图（CFG）相似度也达到了0.871的中等水平。"
  },
  {
    "date": "2025-12-29",
    "title": "An Integrated Sensing-Computing Processor for Spectral-Based Chinese Herbal Medicine Classification With Algorithm-FPGA Co-Design",
    "authors": "Hui Zhang, Hai Wang, Bingrui Zhao, Tongzhi Niu, Yunkang Cao, Yaonan Wang",
    "publish": "IEEE Transactions on Industrial Electronics",
    "url": "https://doi.org/10.1109/tie.2025.3639807",
    "source": "IEEE",
    "abstract": "Hyperspectral imaging has demonstrated strong potential for the classification of Chinese herbal medicines (CHM). However, conventional push-broom systems rely on back-end PCs for spectral analysis, resulting in substantial data transfer overhead and limiting their applicability in real-time, energy-efficient edge scenarios. To address this, we propose an integrated sensing–computing processor. A complete hyperspectral imaging electronics system is developed, along with an dual-branch model, push-broom vision transformer (PBViT), which combines a frequency-domain encoding branch with an improved ViT architecture and contains only 0.075 MB of parameters. On the CHM dataset, PBViT achieves state-of-the-art performance with an OA of 0.978, AA of 0.942, and Kappa of 0.950, surpassing traditional machine learning, convolutional, and ViT-based methods. Leveraging quantization-aware training (QAT), the model is compressed to 8-bit precision with negligible accuracy loss. We further develop an FPGA-based accelerator for PBViT, achieving 502 GOPS throughput and 32.2 <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\mu \\mathbf{s}$</tex-math></inline-formula> latency per <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$9\\boldsymbol{\\times} 9$</tex-math></inline-formula> patch, with a power consumption of 14.42 W. This yields energy efficiency improvements of 6.24<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\boldsymbol{\\times}$</tex-math></inline-formula> and 63<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\boldsymbol{\\times}$</tex-math></inline-formula> over an RTX 4090 Laptop GPU and an Intel i9-13900HX CPU, respectively. The proposed processor delivers high efficiency for real-time, edge-side pharmaceutical identification and can also be applied to fields such as the food industry and agriculture.",
    "title_zh": "基于光谱的中药分类集成感知-计算处理器及其算法与FPGA协同设计",
    "abstract_zh": "高光谱成像在中药材（CHM）分类方面展现出巨大潜力。然而，传统的推扫式系统依赖后端计算机进行光谱分析，导致数据传输开销大，难以在实时、低功耗的边缘场景中应用。为解决这一问题，我们提出了一种集成感知-计算处理器。开发了完整的高光谱成像电子系统，并设计了一种双分支模型——推扫式视觉变换器（PBViT），该模型结合了频域编码分支与改进的视觉变换器（ViT）架构，参数量仅0.075 MB。在中药材数据集上，PBViT取得了当前最优性能，总体准确率（OA）达0.978，平均准确率（AA）为0.942，Kappa系数为0.950，显著优于传统机器学习、卷积神经网络及基于ViT的方法。通过量化感知训练（QAT），模型被压缩至8位精度，且精度损失可忽略不计。此外，我们还开发了基于FPGA的PBViT加速器，实现502 GOPS的吞吐量，每9×9图像块处理延迟仅为32.2 μs，功耗仅为14.42 W。相比RTX 4090笔记本GPU和Intel i9-13900HX CPU，其能效分别提升了6.24倍和63倍。所提出的处理器能够高效支持实时、边缘侧的药品识别，在食品工业、农业等领域也具有广泛应用前景。"
  },
  {
    "date": "2025-12-29",
    "title": "Advanced Credential-Stealing Malware Detection: Static and Dynamic Analysis of Snake Keylogger",
    "authors": "M Akshaya Krishna, P Swetha, Prasad B Honnavalli",
    "publish": "2025 IEEE 25th International Symposium on Computational Intelligence and Informatics (CINTI)",
    "url": "https://doi.org/10.1109/cinti67731.2025.11311767",
    "source": "IEEE",
    "abstract": "In recent years, there has been a significant rise in the deployment of information-stealing malware, with keyloggers emerging as a persistent threat to individuals, enterprises, and critical information systems. Among these, Snake Keylogger, a.NET-based malware first identified in November 2020, has gained notoriety for its credential-stealing and keylogging capabilities. It primarily propagates through phishing and spear phishing campaigns, typically utilizing malicious Office documents or PDFs to deliver downloader scripts via PowerShell. Once executed, Snake Keylogger captures keystrokes, steals saved credentials, extracts clipboard data, and takes screenshots to exfiltrate sensitive information. As of 2025, Snake Keylogger has continued to evolve, adopting advanced evasion techniques such as process hollowing, the use of suspended child processes for payload injection, and heavily obfuscated code, all of which complicate detection and response efforts. Notably, security reports indicate increasing prevalence and spikes in zero-day detections, underscoring its continued threat to both personal and organizational cybersecurity. In this work, the results of static and dynamic analysis of a snake keylogger are presented. The findings from this analysis contribute to the development of more robust detection, defence, and mitigation strategies against Snake Keylogger and similarly evolving malware familie",
    "title_zh": "高级凭证窃取型恶意软件检测：Snake键盘记录器的静态与动态分析",
    "abstract_zh": "近年来，信息窃取型恶意软件的部署显著增加，其中键盘记录器已成为对个人、企业及关键信息系统持续存在的威胁。在诸多此类恶意软件中，Snake Keylogger（一种基于.NET的恶意软件）自2020年11月首次被发现以来，因其窃取凭据和记录键盘输入的能力而声名狼藉。该恶意软件主要通过钓鱼攻击和鱼叉式钓鱼活动传播，通常利用带有恶意内容的Office文档或PDF文件，通过PowerShell执行下载器脚本。一旦运行，Snake Keylogger会捕获用户按键、窃取已保存的凭据、提取剪贴板数据，并截取屏幕图像，以将敏感信息外泄。截至2025年，Snake Keylogger仍在持续演进，采用诸如进程空洞化（process hollowing）、使用挂起的子进程进行载荷注入以及高度混淆的代码等高级逃避技术，极大增加了检测与响应的难度。值得注意的是，安全报告指出其出现频率持续上升，零日漏洞检测数量也屡次出现高峰，凸显其对个人及组织网络安全构成的持续威胁。本文呈现了对Snake Keylogger进行静态与动态分析的研究成果。这些分析结果有助于推动更强大、更有效的检测、防御与缓解策略的发展，以应对Snake Keylogger及其同类不断演化的恶意软件家族。"
  },
  {
    "date": "2025-12-29",
    "title": "Hierarchical Chain-of-Thought and Mixture-of-Experts for Efficient Code Generation",
    "authors": "Tiantian Huang",
    "publish": "Proceedings of the 4th International Conference on Artificial Intelligence and Intelligent Information Processing",
    "url": "https://doi.org/10.1145/3778534.3778592",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "用于高效代码生成的分层思维链与专家混合模型",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-29",
    "title": "Hardware Trojan Detection with Explainable Graph Learning Using XGBoost Algorithm",
    "authors": "C Sneha, M Nirmala Devi",
    "publish": "2025 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",
    "url": "https://doi.org/10.1109/conecct65861.2025.11306889",
    "source": "IEEE",
    "abstract": "In today’s world of interdependencies, hardware security is viewed as a crucial component in system designs and production. As Integrated circuits (ICs) become more complex with the globalization of the semiconductor supply chain, one of the significant threats to hardware security is the Hardware Trojans (HTs). Hardware Trojans are malicious alterations intentionally implanted into a circuit to compromise its performance, reliability or security. There are many Machine- Learning (ML) based hardware trojan detection techniques proposed over the years which yielded better results than traditional methods but they still continue to encounter challenges with ad-hoc feature selection, detection accuracy and black-box nature of machine learning models. The proposed work presents a comprehensive structure to detect HTs at the gate-level netlist using machine learning techniques based on Graph Learning (GL). To overcome constant feature engineering associated with structural features, the gate-level-netlist is converted into Data Flow Graph (DFG) from which structural and graph centrality measures are extracted along with functional features thereby considering heterogenous features to increase the detection accuracy and coverage. It provides efficient techniques to overcome ad-hoc feature selection by incorporating techniques such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanation (LIME) where they also contribute towards the explainability of the blackbox model’s interpretations serving dual purpose. The eXtreme Gradient Boosting (XGBoost) model is used as a classifier. The experimental results projected that the proposed SHAP based XGBoost model evaluated on combinational circuits from TrustHUB benchmarks achieves a remarkable accuracy of 99.16%.",
    "title_zh": "基于XGBoost算法的可解释图学习硬件木马检测",
    "abstract_zh": "在当今相互依存的世界中，硬件安全被视为系统设计与生产中的关键组成部分。随着集成电路（IC）的复杂性日益增加，以及半导体供应链的全球化，硬件安全面临的一个重大威胁便是硬件木马（HTs）。硬件木马是被故意植入电路中的恶意修改，旨在破坏电路的性能、可靠性或安全性。多年来，研究人员提出了多种基于机器学习（ML）的硬件木马检测方法，其效果优于传统手段，但仍面临特征选择随意、检测精度不足以及机器学习模型“黑箱”特性等挑战。\n\n本文提出了一种全面的框架，利用基于图学习（GL）的机器学习技术，在门级网表层面实现硬件木马的检测。为克服传统结构特征工程带来的持续问题，本研究将门级网表转换为数据流图（DFG），从中提取结构特征与图中心性度量，并结合功能特征，综合考虑异构特征以提升检测的准确率与覆盖率。同时，通过引入SHapley Additive exPlanations（SHAP）和局部可解释模型无关解释（LIME）等技术，有效解决了随意特征选择的问题，不仅提升了模型性能，还增强了对“黑箱”模型决策过程的可解释性，实现了双重目标。\n\n实验结果表明，基于SHAP的XGBoost分类器在TrustHUB基准测试中的组合电路数据集上取得了高达99.16%的检测准确率，展现出卓越的性能表现。"
  }
]