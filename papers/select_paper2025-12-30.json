[
  {
    "date": "2025-12-30",
    "title": "Demand-Oriented Autonomous Navigation for Mobile Robots With Vector-Semantic Mapping in Urban Environments",
    "authors": "Ying Zhang, Danni Zhu, Shaohan Bian, Haibao Yan, Cui-Hua Zhang, F. Richard Yu, Changchun Hua",
    "publish": "IEEE Transactions on Vehicular Technology",
    "url": "https://doi.org/10.1109/tvt.2025.3649304",
    "source": "IEEE",
    "abstract": "Autonomous navigation of mobile robots in urban environments is crucial for independent travel for the elderly or disabled populations. However, existing robot urban navigation solutions usually rely on the precise coordinates of the target location, and do not comply with traffic rules during navigation. To this end, this paper proposes a demand-oriented robot navigation solution in urban environments based on vector-semantic map (V-S Map), where V-S Map integrates geometrically precise vector layer generated using a Transformer architecture, and task-related semantic layer derived via optical character recognition and point cloud registration. Unlike current mapping approaches, V-S map uniquely incorporates directional road attributes and task-relevant semantic information to address traffic violations in urban navigation while establishing the relevance of tasks to the environment. Furthermore, we develop an LLM-based command parsing strategy that combines optimized prompt engineering with regular expression matching against location lexicons extracted from semantic layer. This strategy eliminates traditional coordinate-input constraints, enhancing human-robot natural language interaction capabilities. Extensive experiments in simulation and real-world environments verify the effectiveness and feasibility of the proposed solution in terms of ambiguous command parsing and robot navigation performance, achieving an 80% overall task success rate with a 63.78% path-violation weighted success rate.",
    "title_zh": "面向需求的自主导航移动机器人在城市环境中的矢量语义映射",
    "abstract_zh": "在城市环境中实现移动机器人的自主导航，对于老年人或残障人士的独立出行至关重要。然而，现有的机器人城市导航方案通常依赖于目标位置的精确坐标，且在导航过程中不遵守交通规则。为此，本文提出了一种基于向量-语义地图（V-S Map）的需求导向型城市环境机器人导航解决方案。该V-S地图通过Transformer架构生成几何精度高的矢量层，并结合光学字符识别与点云配准技术提取任务相关的语义层，实现了对道路方向属性和任务相关语义信息的融合。与现有地图构建方法不同，V-S地图独特地整合了方向性道路特征和任务相关的语义信息，既解决了城市导航中的交通违规问题，又建立了任务与环境之间的语义关联。此外，我们设计了一种基于大语言模型（LLM）的指令解析策略，该策略结合优化的提示工程与从语义层中提取的位置词典进行正则表达式匹配，从而摆脱了传统依赖坐标输入的限制，显著提升了人机自然语言交互能力。在仿真与真实环境中的大量实验验证了该方案在模糊指令解析和机器人导航性能方面的有效性与可行性，整体任务成功率高达80%，路径违规加权成功率也达到63.78%。"
  },
  {
    "date": "2025-12-30",
    "title": "Llm4mcu-Onto: Leveraging Llms for Automated Ontology Generation From Microcontroller Reference Manual",
    "authors": "Asmita Asmita, Grisha Bandodkar, Sujan Ghimire, Shaurya Srivastav, Soheil Salehi, Houman Homayoun",
    "publish": "2025 IEEE 43rd International Conference on Computer Design (ICCD)",
    "url": "https://doi.org/10.1109/iccd65941.2025.00089",
    "source": "IEEE",
    "abstract": "This research addresses the challenges faced by firmware developers, security researchers, and enthusiasts who work with low-level microcontroller (MCU) documentation, which often spans hundreds of complex pages. Current structured approaches, such as System View Description (SVD), are widely used but suffer from manual, labor-intensive creation processes and inconsistent vendor adherence to CMSIS-SVD standards. We propose an automated solution using Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG), capable of effectively parsing and extracting structured information, including text, tables, and images from MCU reference manuals/ datasheets. To mitigate hallucination issues inherent in LLMs, we fine-tuned models using a dataset derived from CMSIS-SVD files, which we will open-source for community benefit. We also experimented with few-shot models. Additionally, we developed a standardized structured ontology that is automatically populated with information extracted through LLM assistance from the reference manuals of the corresponding MCUs. Our approach was evaluated using OpenAI's GPT-4o under one-shot, few-shot, and fine-tuning scenarios, all incorporating RAG. We also experimented with the open-source LLM model CodeLlama. The results highlight substantial improvements in automatically extracting peripheral details and information from MCU reference manuals. Thus, it helps reduce manual effort and time. The key contribution of our work lies in the tailored adaptation of existing AI techniques to address the specific challenges of embedded systems documentation. We perform standardized ontology creation and multimodal parsing. We leverage RAG with MCU-specific finetuning and few-shot learning to generate structured information from hundreds of pages of MCU documentation. This opens the door to potential applications such as more accurate firmware code generation and reverse engineering for security analysis.",
    "title_zh": "Llm4mcu-Onto：利用大语言模型从微控制器参考手册中自动构建本体",
    "abstract_zh": "本研究针对固件开发者、安全研究人员及爱好者在处理低级微控制器（MCU）文档时所面临的挑战，这些文档通常包含数百页复杂的内容。目前广泛采用的结构化方法，如系统视图描述（SVD），虽已普及，但其创建过程高度依赖人工，耗时费力，且各厂商对CMSIS-SVD标准的遵循程度不一。为此，我们提出一种基于大语言模型（LLM）与检索增强生成（RAG）相结合的自动化解决方案，能够高效解析并提取MCU参考手册/数据手册中的结构化信息，包括文本、表格和图像等内容。为缓解LLM固有的幻觉问题，我们利用从CMSIS-SVD文件中提取的数据集对模型进行了微调，并将该数据集开源，以惠及社区。同时，我们也测试了少样本学习（few-shot）模型的效果。此外，我们构建了一个标准化的结构化本体（ontology），并通过LLM辅助从相应MCU的参考手册中自动填充信息。我们的方法在OpenAI的GPT-4o模型下进行了评估，涵盖单样本（one-shot）、少样本及微调三种场景，均结合了RAG技术；同时我们也测试了开源模型CodeLlama的表现。实验结果表明，该方法在自动提取MCU外设细节及相关信息方面取得了显著提升，有效降低了人工工作量和时间成本。本研究的核心贡献在于：将现有的AI技术进行针对性优化，专门应对嵌入式系统文档的特殊挑战。我们实现了标准化本体的构建与多模态内容解析，通过结合MCU领域特化的微调以及少样本学习策略，利用RAG从数百页MCU文档中生成结构化信息。这一成果为未来应用打开了可能性，例如更精准的固件代码自动生成，以及用于安全分析的逆向工程等。"
  },
  {
    "date": "2025-12-30",
    "title": "NaviMap: Partial Order-Guided Neural Architecture via Deep Q-Networks for Efficient CGRA Mapping",
    "authors": "Mingyang Kou, Jun Zeng, Xinyu Peng, Weiqing Ji, Hailong Yao",
    "publish": "2025 IEEE 43rd International Conference on Computer Design (ICCD)",
    "url": "https://doi.org/10.1109/iccd65941.2025.00059",
    "source": "IEEE",
    "abstract": "Coarse-Grained Reconfigurable Architectures (CGRAs) have emerged as promising solutions for energyefficient computing in edge devices and datacenter accelerators. While offering substantial performance benefits, their adoption is hindered by the NP-hard loop mapping problem during compilation. In this paper, we present NaviMap, a neuralsymbolic framework combining partial order-aware graph embeddings with deep reinforcement learning. Experimental results demonstrate that NaviMap achieves a <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$1.95 \\times$</tex> speedup in solving CGRA mapping problems compared with state-of-the-art methods, while producing mappings with equivalent or superior performance.",
    "title_zh": "NaviMap：基于偏序引导的深度Q网络神经架构，用于高效CGRA映射",
    "abstract_zh": "粗粒度可重构架构（CGRAs）已成为边缘设备和数据中心加速器中实现能效计算的有前景解决方案。尽管其在性能方面具有显著优势，但编译过程中的NP难循环映射问题严重阻碍了其广泛应用。本文提出NaviMap，一种结合偏序感知图嵌入与深度强化学习的神经符号框架。实验结果表明，与现有最先进方法相比，NaviMap在求解CGRA映射问题时实现了1.95倍的加速，同时生成的映射方案在性能上达到或优于现有方法。"
  },
  {
    "date": "2025-12-30",
    "title": "AI-Powered Comprehensive Code Quality Analyzer Tool",
    "authors": "Savitri Hunasheekatti, Suprit Iti",
    "publish": "2025 IEEE International Conference for Women in Innovation, Technology &amp;amp; Entrepreneurship (ICWITE)",
    "url": "https://doi.org/10.1109/icwite64848.2025.11306974",
    "source": "IEEE",
    "abstract": "WatsonX, a comprehensive data and AI platform, adeptly addresses contemporary challenges by meticulously training, validating, tuning, and deploying data to drive impactful business outcomes. Developing the quality and reusable code is important part of the software development. The intricate task of timely analyzing the code quality poses a significant challenge for software development teams, directly influencing business operations. This paper introduces an innovative solution leveraging AI, particularly harnessing generative AI techniques with the large language models like 'Granite-3.2' through the platform. The Code Quality Analyzer Tool is designed to automate the evaluation of code quality for developers. This tool identifies potential issues, provides an objective score, and suggests whether the code is ready for deployment. Additionally, the tool offers reverse engineering capabilities, allowing developers to trace issues back to specific lines in the code. The implementation of this solution holds the promise of significantly enhancing code quality and review process and expediting the entire development workflow.",
    "title_zh": "基于人工智能的综合代码质量分析工具",
    "abstract_zh": "WatsonX 是一个全面的数据与人工智能平台，通过精心地训练、验证、调优和部署数据，有效应对当今的各类挑战，推动具有深远影响的业务成果。在软件开发中，编写高质量且可复用的代码是至关重要的环节。然而，及时分析代码质量这一复杂任务对开发团队而言仍是一项重大挑战，直接影响企业的运营效率。本文提出了一种创新解决方案，利用人工智能技术，特别是借助大型语言模型（如“Granite-3.2”）的生成式AI能力，通过WatsonX平台实现。所提出的代码质量分析工具旨在自动化评估开发者的代码质量。该工具能够识别潜在问题，提供客观评分，并判断代码是否具备部署条件。此外，该工具还具备逆向工程功能，使开发者能够追溯问题至代码中的具体行。该解决方案的实施有望显著提升代码质量与审查效率，加速整个开发流程。"
  },
  {
    "date": "2025-12-30",
    "title": "A Multi-Agent AI Framework for Agile Workflow Automation, Issue Resolution, and Developer Performance Evaluation",
    "authors": "Chathurya Adapa, Anjana A R K, Rafsal Rahim, Ajay Victor",
    "publish": "2025 IEEE International Conference for Women in Innovation, Technology &amp;amp; Entrepreneurship (ICWITE)",
    "url": "https://doi.org/10.1109/icwite64848.2025.11306978",
    "source": "IEEE",
    "abstract": "The rise of agentic Artificial Intelligence (AI)autonomous systems capable of perceiving, reasoning, and acting with minimal human intervention-has opened new avenues for optimizing engineering workflows. In this paper, we propose an intelligent, agentic AI-driven framework to automate and enhance agile team collaboration and task management. Our system consists of a network of agentic AI modules integrated with real-time stand-up call transcription services. The primary AI agent autonomously parses daily meeting transcripts with an extraction accuracy of approximately 95 %, updating project management tools and minimizing manual task updates by automating about 60 % of ticket status changes. A secondary agent analyzes blockers with over 85 % contextual relevance, reducing blocker resolution time by up to 40 %. Additionally, performancetracking agents helped save approximately <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$22-25$</tex> developer hours annually and reduced manager evaluation time by over 75 %, cutting it from 6 hours to 1.3 hours per developer per year. Overall, the framework demonstrates around a 20 % improvement in sprint planning accuracy and a 30% reduction in manual project tracking effort, introducing a novel multi-agent orchestration approach where AI agents autonomously extract, synchronize daily agile ceremonies establishing an end-to-end, real-time intelligent framework currently missing in Agile automation research.",
    "title_zh": "一种用于敏捷工作流自动化、问题解决及开发者绩效评估的多智能体AI框架",
    "abstract_zh": "随着具有自主感知、推理与行动能力的代理型人工智能（AI）系统兴起——这些系统能够在极少人工干预的情况下自主运行——工程工作流程的优化迎来了新的机遇。本文提出了一种智能的、基于代理型AI的框架，旨在自动化并增强敏捷团队的协作与任务管理。该系统由一系列集成实时站会通话转录服务的代理型AI模块组成。主要AI代理能够以约95%的提取准确率自动解析每日会议记录，更新项目管理工具，从而自动化约60%的工单状态变更，显著减少手动任务更新的工作量。次要代理对阻碍因素进行分析，上下文相关性超过85%，使阻碍问题的解决时间缩短高达40%。此外，性能追踪代理每年可节省约22至25名开发人员工时，同时将管理者评估时间减少超过75%，从原先每人每年6小时降至1.3小时。总体而言，该框架在冲刺计划准确性方面实现了约20%的提升，在手动项目跟踪工作量上减少了30%。本研究引入了一种新颖的多代理协同机制，使AI代理能够自主提取并同步每日敏捷仪式内容，构建了一个端到端、实时的智能化框架，填补了当前敏捷自动化研究中此类完整智能系统的空白。"
  },
  {
    "date": "2025-12-30",
    "title": "AI-Enhanced Management of Standard Operating Procedures: An Innovative Approach Integrating Large Language Models and Knowledge Graphs",
    "authors": "Nikos Karacapilidis, Nikolaos Giarelis, Charalampos Mastrokostas",
    "publish": "2025 16th International Conference on Information, Intelligence, Systems &amp;amp; Applications (IISA)",
    "url": "https://doi.org/10.1109/iisa66859.2025.11311202",
    "source": "IEEE",
    "abstract": "This paper presents an innovative solution to address inefficiencies in Standard Operating Procedure (SOP) management by integrating Large Language Models (LLMs) and Knowledge Graphs (KGs) to create an intelligent, contextaware digital assistant. Traditional SOP systems lack adaptability, contextual understanding, and dynamic functionality, leading to operational inefficiencies and compliance risks. By leveraging the interpretability of KGs and the generative capabilities of LLMs, the proposed solution offers enhanced functionality, including dynamic SOP chaining, realtime conflict detection, and actionable insights for decisionmaking. Through a specific example of use, we demonstrate how the proposed digital assistant enhances operational efficiency, reduces errors, and fosters adherence to regulatory standards. This work contributes to the evolution of SOP management by building on prominent AI advancements.",
    "title_zh": "基于大语言模型与知识图谱融合的SOP智能管理：一种创新性方法",
    "abstract_zh": "本文提出了一种创新解决方案，旨在解决标准操作程序（SOP）管理中的低效问题。该方案通过整合大型语言模型（LLMs）与知识图谱（KGs），构建了一个智能、上下文感知的数字助手。传统的SOP系统缺乏适应性、上下文理解能力以及动态功能，导致运营效率低下和合规风险增加。本方案利用知识图谱的可解释性与大型语言模型的生成能力，实现了多项增强功能，包括动态SOP链式联动、实时冲突检测以及支持决策的可操作洞察。通过一个具体的应用实例，我们展示了该数字助手如何提升运营效率、减少错误，并促进对监管标准的遵守。本研究推动了SOP管理的发展，基于当前前沿的人工智能技术成果，为未来智能化管理提供了新思路。"
  },
  {
    "date": "2025-12-30",
    "title": "MALLS: Multi-Agent LLMs for Synthetic Hardware Vulnerability Generation and Detection",
    "authors": "Jonti Talukdar, Agastya Seth, Sanmitra Banerjee, Farshad Firouzi, Krishnendu Chakrabarty",
    "publish": "2025 IEEE 43rd International Conference on Computer Design (ICCD)",
    "url": "https://doi.org/10.1109/iccd65941.2025.00116",
    "source": "IEEE",
    "abstract": "LLMs have demonstrated promising capabilities in generating RTL code from high-level functional descriptions of hardware modules. However, their effectiveness is constrained by the lack of high-quality, diverse datasets particularly for applications in IP design, verification, and security analysis. To address this limitation, we introduce MALLS, a multi-agent framework in which specialized LLM agents namely, a generator and a discriminator collaborate in an adversarial yet cooperative setting to improve the quality and correctness of RTL designs and curate a high quality synthetic hardware vulnerability dataset. In this architecture, the generator agent is responsible for producing RTL implementations from initial seed examples through in-context learning, while the discriminator agent assesses the generator's output for functional correctness and the presence of security vulnerabilities. This interaction creates a dynamic feedback loop, enabling both agents to iteratively improve through each other's responses leading to self-supervised learning. A hard bank of examples is maintained in a database which include instances that were difficult to generate or detect by either agents. By generating paired positive (correct) and negative (buggy) examples, the system learns to distinguish subtle design flaws and generalize across diverse RTL patterns while generating high quality synthetic examples of hardware vulnerabilities. Experimental results show that using the hard bank of examples produced by the adversarial multi-LLM setup improves both vulnerability generation and detection performance.",
    "title_zh": "MALLS：用于合成硬件漏洞生成与检测的多智能体大语言模型",
    "abstract_zh": "大语言模型（LLMs）在从硬件模块的高层功能描述生成RTL代码方面已展现出令人瞩目的能力。然而，其实际效果受到高质量、多样化数据集匮乏的制约，尤其是在IP设计、验证和安全分析等应用场景中。为解决这一局限性，我们提出了MALLS——一种多智能体框架，其中专门化的LLM智能体（包括生成器与判别器）在对抗性但协作的环境中协同工作，以提升RTL设计的质量与正确性，并构建一个高质量的合成硬件漏洞数据集。在此架构中，生成器智能体通过上下文学习，从初始种子示例出发生成RTL实现；而判别器智能体则对生成结果进行评估，检查其功能正确性及是否存在安全漏洞。这种交互形成了动态反馈回路，使两个智能体能够通过彼此的响应不断迭代优化，从而实现自监督学习。系统还维护一个“难题库”（hard bank），其中包含那些任一智能体都难以生成或检测的典型实例。通过生成成对的正例（正确）与负例（含缺陷）样本，系统能够学习识别细微的设计缺陷，并在多种RTL模式间实现泛化，同时生成高质量的合成硬件漏洞实例。实验结果表明，由对抗性多LLM设置生成的难题库显著提升了漏洞生成与检测的性能。"
  },
  {
    "date": "2025-12-30",
    "title": "The Missing S: Securing the Model Context Protocol",
    "authors": "Michelle Hamjaya, Anthony Sai Richardo, Darren Leonard, Franz Adeta Junior",
    "publish": "2025 1st International Conference on Artificial Intelligence Technology (ICoAIT)",
    "url": "https://doi.org/10.1109/icoait67446.2025.11308830",
    "source": "IEEE",
    "abstract": "The Model Context Protocol (MCP) standardizes AI models’ integration and interaction with external tools and resources. However, its rapid adoption, particularly in allowing custom server and tool development, introduces significant security vulnerabilities, with command injection emerging as a dominant threat, accounting for 43% of observed vulnerabilities in popular MCP servers. While existing research has highlighted these dangers, focused investigation into their detection and mitigation, specifically within the MCP environment, remains limited. This research addresses this gap by evaluating the performance of five transformer-based models, such as BERT, RoBERTa, ALBERT, DistilBERT, and ELECTRA, for detecting command injection attacks in MCP environments. The dataset for this study was curated from real-world MCP server usage, public prompt injection datasets, and custom-created examples specifically designed to cover diverse MCP-specific command injection attack vectors. Our findings found that transformer-based models demonstrate an exceptional performance against command injection attacks, with all evaluated models achieving validation accuracy exceeding 98.8%. Among these models, DistilBERT emerged as the highest performing model, achieving 99.92% validation and test accuracy. By integrating real-world MCP usage with a broad spectrum of targeted attack vectors, the dataset established a robust foundation for training models capable of effectively mitigating command injection threats within the MCP ecosystem.",
    "title_zh": "缺失的S：保障模型上下文协议",
    "abstract_zh": "模型上下文协议（MCP）标准化了AI模型与外部工具及资源的集成与交互。然而，其快速普及，尤其是在支持自定义服务器和工具开发方面，引入了显著的安全漏洞，其中命令注入已成为主要威胁，占观察到的热门MCP服务器漏洞的43%。尽管现有研究已指出这些风险，但针对MCP环境中此类威胁的检测与缓解措施的深入研究仍十分有限。本研究填补了这一空白，评估了五种基于Transformer的模型——包括BERT、RoBERTa、ALBERT、DistilBERT和ELECTRA——在MCP环境中检测命令注入攻击的表现。本研究的数据集源自真实MCP服务器使用场景、公开的提示注入数据集，以及为覆盖多种MCP特有命令注入攻击向量而专门构建的示例。研究结果表明，基于Transformer的模型在应对命令注入攻击方面表现出卓越性能，所有评估模型在验证集上的准确率均超过98.8%。其中，DistilBERT表现最佳，验证准确率和测试准确率分别达到99.92%。通过结合真实MCP使用场景与广泛多样的针对性攻击向量，该数据集为训练能够有效缓解MCP生态系统中命令注入威胁的模型奠定了坚实基础。"
  },
  {
    "date": "2025-12-30",
    "title": "Policy-Aware Generative AI for Safe, Auditable Data Access Governance",
    "authors": "Shames Al Mandalawi, Muzakkiruddin Ahmed Mohammed, Hendrika Maclean, Mert Can Cakmak, John R. Talburt",
    "publish": "2025 17th International Conference on Knowledge and System Engineering (KSE)",
    "url": "https://doi.org/10.1109/kse68178.2025.11309632",
    "source": "IEEE",
    "abstract": "Enterprises need access decisions that satisfy least privilege, comply with regulations, and remain auditable. We present a policy aware controller that uses a large language model (LLM) to interpret natural language requests against written policies and metadata, not raw data. The system, implemented with Google Gemini 2.0 Flash, executes a six-stage reasoning framework (context interpretation, user validation, data classification, business purpose test, compliance mapping, and risk synthesis) with early hard policy gates and deny by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls and a machine readable rationale. We evaluate on fourteen canonical cases across seven scenario families using a privacy preserving benchmark. Results show Exact Decision Match improving from <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$10 / 14$</tex> to <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$13 / 14(92.9 \\%)$</tex> after applying policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny families dropping to 0, and Functional Appropriateness and Compliance Adherence at 14/14. Expert ratings of rationale quality are high, and median latency is under one minute. These findings indicate that policy constrained LLM reasoning, combined with explicit gates and audit trails, can translate human readable policies into safe, compliant, and traceable machine decisions.",
    "title_zh": "面向策略的生成式人工智能在安全、可审计的数据访问治理中的应用",
    "abstract_zh": "企业需要满足最小权限原则、符合监管要求且可审计的访问决策。我们提出了一种政策感知控制器，利用大型语言模型（LLM）对自然语言请求进行解读，依据书面政策和元数据，而非原始数据。该系统基于 Google Gemini 2.0 Flash 实现，采用六阶段推理框架（上下文理解、用户验证、数据分类、业务目的测试、合规性映射与风险综合），并设置早期硬性策略关卡，遵循默认拒绝原则。系统返回 APPROVE、DENY 或 CONDITIONAL 决策，并附带引用的控制措施及机器可读的推理理由。我们在涵盖七个场景类别的十四个典型案例上进行了评估，使用隐私保护基准测试。结果表明，应用策略关卡后，精确决策匹配率从 $10 / 14$ 提升至 $13 / 14$（92.9%），DENY 类别的召回率达到 1.00，必须拒绝类别的误批准率降至 0，功能适当性与合规性均达到 14/14。专家对推理理由质量的评分较高，中位延迟低于一分钟。这些发现表明，经过策略约束的 LLM 推理，结合显式的策略关卡与审计追踪，能够将人类可读的政策有效转化为安全、合规且可追溯的机器决策。"
  },
  {
    "date": "2025-12-30",
    "title": "AI Driven Self Healing Software Architectures for Automated Code Diagnostics and Repair",
    "authors": "C. Maddilety, Gondiparla Lokesh, A. Vani Vathsala, Madhavaram Swapna, K. Samunnisa, M Bhavsingh",
    "publish": "2025 International Conference on Sustainable Communication Networks and Application (ICSCN)",
    "url": "https://doi.org/10.1109/icscn67106.2025.11308520",
    "source": "IEEE",
    "abstract": "Modern software systems deployed in safety- critical and real-time environments demand continuous operation and resilience to unforeseen faults. Traditional fault-handling mechanisms rely on reactive, rule-based strategies and manual debugging, which are insufficient for dynamic, autonomous systems. Inspired by biological repair mechanisms, this study proposes a self-healing software architecture capable of autonomous fault detection, diagnosis, and patching at runtime. The primary objective of this research is to design and evaluate a biologically inspired, AI-driven framework that performs real-time software fault recovery with minimal human intervention. The proposed architecture integrates a transformer-based fault localization engine with a neural patch synthesis model trained on the ManySStuBs4J dataset. Runtime fault detection and control are handled through a modular feedback mechanism that ensures safe patch deployment and rollback. Evaluation is conducted using the Defects4J and ManySStuBs4J datasets across 395 real-world bugs and over 150,000 bug-fix pairs. Experimental results demonstrate that the proposed system achieves a fault localization accuracy of 88.6%, patch generation success rate of 74.2%, and patch correctness rate of 68.3%, while maintaining a mean patch generation latency of 310.4ms —substantially outperforming state-of-the- art baselines including GenProg, DeepFix, and RLRepair. This architecture advances the state of autonomous software repair by combining biological principles with machine learning, enabling scalable, low-latency, and deployable self-healing capabilities for mission-critical applications.",
    "title_zh": "面向自动代码诊断与修复的AI驱动自愈软件架构",
    "abstract_zh": "在安全关键型和实时环境中部署的现代软件系统需要持续运行，并具备应对意外故障的弹性。传统的故障处理机制依赖于反应式、基于规则的策略以及人工调试，对于动态、自主的系统而言已显不足。受生物体修复机制的启发，本研究提出一种自愈型软件架构，能够在运行时自主完成故障检测、诊断与修复。本研究的主要目标是设计并评估一种受生物启发、由人工智能驱动的框架，实现无需人工干预的实时软件故障恢复。所提出的架构集成了基于Transformer的故障定位引擎，以及在ManySStuBs4J数据集上训练的神经修补生成模型。运行时的故障检测与控制通过模块化的反馈机制实现，确保补丁的安全部署与回滚。实验基于Defects4J和ManySStuBs4J数据集，在395个真实世界缺陷及超过15万对缺陷修复案例上进行评估。实验结果表明，该系统在故障定位准确率方面达到88.6%，补丁生成成功率74.2%，补丁正确率68.3%，平均补丁生成延迟仅为310.4毫秒，显著优于GenProg、DeepFix和RLRepair等当前最先进的基线方法。该架构通过融合生物学原理与机器学习技术，推动了自主软件修复的发展，为任务关键型应用提供了可扩展、低延迟且可部署的自愈能力。"
  },
  {
    "date": "2025-12-30",
    "title": "Comparative Study of Chatbot Architectures Using LLMs and RAG: Evaluating FAISS-Based Knowledge Retrieval Schemes",
    "authors": "Herlawati Herlawati, Ben Rahman, Rahmadya Trias Handayanto, Prima Dina Atika, Dwipa Handayani, Maimunah Maimunah, Endang Retnoningsih, Syahbaniar Rofiah, Didik Setiyadi",
    "publish": "2025 Tenth International Conference on Informatics and Computing (ICIC)",
    "url": "https://doi.org/10.1109/icic68054.2025.11309633",
    "source": "IEEE",
    "abstract": "The use of Large Language Models (LLMs) for chatbot applications is currently widespread. The availability of various models with specific characteristics tailored to different needs has made these models increasingly popular. However, challenges remain, particularly the high computational resource requirements, such as the need for a Graphics Processing Unit (GPU). This has led to the need for optimization through emerging technologies, one of which is Retrieval-Augmented Generation (RAG). RAG allows users to avoid resource-intensive fine-tuning processes. Several RAG methods are already in use, such as Facebook AI Similarity Search (FAISS), Weaviate, Elasticsearch, Qdrant, etc., but the implementation techniques vary widely and still require experimentation to achieve optimal performance while minimizing computational demands. This is especially important considering that chatbots are often integrated into web servers, which are typically focused on web programming rather than artificial intelligence (AI). This study evaluates several chatbot schemes utilizing RAG with a FAISS index for a Question and Answer (Q/A)-based chatbot. The model used is a lightweight one with minimal GPU requirements, namely Gemma:2b-Instruct. The tested schemes include: (i) a single FAISS index, (ii) a single FAISS index with special chunking, (iii) two FAISS indices handling Q/A and non-Q/A in a serial manner, and (iv) two FAISS indices handling Q/A and non-Q/A in parallel. Experimental results show that using special chunking to handle Q/A improves accuracy, and the use of two FAISS indices in parallel minimizes the dominance of the Q/A index over the non-Q/A index.",
    "title_zh": "基于大语言模型与RAG的聊天机器人架构对比研究：评估基于FAISS的知识检索方案",
    "abstract_zh": "目前，大型语言模型（LLMs）在聊天机器人应用中的使用已十分普遍。由于存在多种具有特定功能、针对不同需求量身定制的模型，这些模型正变得越来越受欢迎。然而，仍面临诸多挑战，尤其是对计算资源的高要求，例如需要配备图形处理器（GPU）。这促使人们通过新兴技术进行优化，其中一种重要方法便是检索增强生成（Retrieval-Augmented Generation, RAG）。RAG使用户能够避免耗时且资源密集型的微调过程。目前已有一些RAG实现方法，如Facebook AI相似性搜索（FAISS）、Weaviate、Elasticsearch、Qdrant等，但其具体实现方式差异较大，仍需通过实验不断探索以达到最佳性能，同时尽可能降低计算开销。这一点尤为重要，因为聊天机器人通常被集成到Web服务器中，而这类服务器主要专注于Web开发，而非人工智能（AI）相关任务。\n\n本研究评估了几种基于RAG的聊天机器人方案，采用FAISS索引构建问答（Q/A）型聊天机器人。所使用的模型为轻量级模型Gemma:2b-Instruct，其对GPU的需求极低。测试的方案包括：（i）单一FAISS索引；（ii）使用特殊分块处理的单一FAISS索引；（iii）两个FAISS索引按串行方式分别处理问答与非问答内容；（iv）两个FAISS索引并行处理问答与非问答内容。实验结果表明，采用特殊分块方法处理问答内容可有效提升准确率；而采用两个FAISS索引并行处理，则能有效降低问答索引对非问答索引的主导影响，从而实现更均衡的检索表现。"
  },
  {
    "date": "2025-12-30",
    "title": "Design and Research on LeNet5 Convolutional Neural Network Accelerator Based on HLS",
    "authors": "Chengyi Peng",
    "publish": "2025 IEEE 8th International Conference on Information Systems and Computer Aided Education (ICISCAE)",
    "url": "https://doi.org/10.1109/iciscae66104.2025.11307394",
    "source": "IEEE",
    "abstract": "This study presents the design and implementation of a LeNet-5 convolutional neural network (CNN) accelerator based on High-Level Synthesis (HLS) technology, focusing on hierarchical architecture analysis and non-linear mapping mechanisms to establish a theoretical foundation for hardware optimization. Modular designs incorporating HLS-specific strategies are developed for convolutional, pooling, and fully connected layers, achieving significant improvements validated through MNIST dataset evaluation: 98.5 % prediction accuracy (matching PyTorch's <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\text{98.7 \\%}), \\text{60}$</tex>-microsecond single-frame latency at <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\text{150~MHz,~} \\text{56 \\%}$</tex> reduction in DDR accesses with 3.2 times computational density enhancement in convolutional layers via Dynamic Window Cache (DWC), 68% logical resource reduction in pooling layers using zero-delay dual buffering and piecewise Sigmoid approximation, and 50 % memory bandwidth reduction with 8 times throughput improvement in fully connected layers through mixed-precision quantization and block-wise MAC operations. The accelerator consumes 1.8 W dynamically with 3.2 TOPS/W energy efficiency, providing an edge-deployable solution that demonstrates superior inference speed, energy efficiency, and resource utilization compared to CPU and RTL baselines.",
    "title_zh": "基于HLS的LeNet5卷积神经网络加速器的设计与研究",
    "abstract_zh": "本研究提出了一种基于高层次综合（HLS）技术的LeNet-5卷积神经网络（CNN）加速器的设计与实现，重点围绕分层架构分析和非线性映射机制，为硬件优化建立了理论基础。针对卷积、池化和全连接层，设计了融合HLS特有策略的模块化结构，在MNIST数据集上的评估验证了显著性能提升：预测准确率达到98.5%（与PyTorch的98.7%相当），在150 MHz频率下单帧延迟仅为60微秒；通过动态窗口缓存（DWC）技术，卷积层实现DDR访问次数减少56%，计算密度提升3.2倍；池化层采用零延迟双缓冲与分段Sigmoid近似方法，逻辑资源消耗降低68%；全连接层通过混合精度量化与分块MAC操作，实现内存带宽降低50%，吞吐量提升8倍。该加速器动态功耗仅为1.8 W，能效达3.2 TOPS/W，提供了一种具备优异推理速度、能效比和资源利用率的边缘部署解决方案，相较CPU及RTL基线表现更优。"
  },
  {
    "date": "2025-12-30",
    "title": "AI - Driven Design Pattern Recommendation System for Enhanced Software Architecture",
    "authors": "Sangeeta Singh",
    "publish": "2025 International Conference on Sustainable Communication Networks and Application (ICSCN)",
    "url": "https://doi.org/10.1109/icscn67106.2025.11308595",
    "source": "IEEE",
    "abstract": "The ever growing complexity of software systems makes it imperative to use automated techniques in proposing and deploying design patterns to enhance the maintainability, modularity and lessen the coupling of the software. Conventional methods may be mostly based on manual detection or small to medium amount of heuristic-driven tools which may be time consuming, error prone and may not generalize across different code bases. In response to this, we introduce an artificial intelligence-based recommendation system that combines multimodal software representations, knowledge graphs, and self-supervised pretraining to identify, suggest, and generate design pattern refactorings. The system builds a rich repository of design patterns, and quality attributes, trade-offs and transformation rules are represented in a machine-readable form. A high-quality dataset is built by heuristic-based static analysis, expert verification, and synthetic data augmentation using source code, documentation, commit histories, and runtime logs. GNNs and transformer models are used to supplement multi-modal embeddings based on ASTs, control-data flow, dependency networks, and token-level embeddings to reflect structural, behavioral, and semantic nuances. Knowledge graph captures patterns, anti-patterns, and refactoring rules, and makes prescriptive recommendations. Experimental analysis into the Design Patterns Detection Benchmark Dataset shows that the system outperforms the current methods with 92.4-percent precision, 90.8-percent recall, 91.6-percent F1-score, and 93.2-percent accuracy and the shortest runtime of 1.8 seconds. Findings verify the fact that the suggested methodology does not only lead to increased accuracy in prediction, but also to increased practical efficiency, providing a powerful, scalable solution to automated design pattern recommendation and refactoring.",
    "title_zh": "基于人工智能的软件架构设计模式推荐系统",
    "abstract_zh": "日益复杂的软件系统使得采用自动化技术来提出并部署设计模式成为当务之急，以提升软件的可维护性、模块化程度，并降低耦合度。传统的手段大多依赖人工检测或少量基于启发式规则的工具，不仅耗时费力、容易出错，且难以在不同代码库之间实现泛化。针对这一挑战，我们提出了一种基于人工智能的推荐系统，该系统融合了多模态软件表示、知识图谱与自监督预训练技术，能够识别、建议并生成设计模式重构方案。该系统构建了一个丰富的设计模式资源库，将质量属性、权衡取舍以及转换规则以机器可读的形式进行表达。通过基于启发式的静态分析、专家验证以及利用源代码、文档、提交历史和运行日志进行合成数据增强，构建了一个高质量的数据集。图神经网络（GNN）与Transformer模型结合AST结构、控制-数据流关系、依赖网络及词级嵌入，生成多模态嵌入，以捕捉代码的结构、行为与语义特征。知识图谱则用于记录设计模式、反模式及重构规则，并提供具有指导意义的推荐。在设计模式检测基准数据集上的实验分析表明，本系统在精度（92.4%）、召回率（90.8%）、F1分数（91.6%）和准确率（93.2%）方面均优于现有方法，且运行时间最短，仅需1.8秒。研究结果证实，所提出的方案不仅显著提升了预测准确性，还大幅提高了实际应用效率，为自动化设计模式推荐与重构提供了一种强大且可扩展的解决方案。"
  },
  {
    "date": "2025-12-30",
    "title": "CABENCH: Benchmarking Composable Ai for Solving Complex Tasks through Composing Ready-to-Use Models",
    "authors": "Tung-Thuy Pham, Duy-Quan Luong, Minh-Quan Duong, Trung-Hieu Nguyen, Thu-Trang Nguyen, Son Nguyen, Hieu Dinh Vo",
    "publish": "2025 17th International Conference on Knowledge and System Engineering (KSE)",
    "url": "https://doi.org/10.1109/kse68178.2025.11309631",
    "source": "IEEE",
    "abstract": "Composable AI offers a scalable and effective paradigm for solving complex AI tasks by decomposing them into sub-tasks, each handled by ready-to-use models. However, systematically evaluating methods under this setting remains largely unexplored. This paper introduces CABench, the first public benchmark for composable AI, comprising 70 realistic tasks, along with a curated pool of 700 models across multiple modalities and domains. We also propose an evaluation framework to enable end-to-end assessment of composable AI solutions. To establish initial baselines, we provide human-designed reference solutions and compare their performance with two LLM-based approaches. Our results demonstrate the promise of composable AI in addressing real-world challenges while underscoring the need for methods capable of automatically generating effective execution pipelines. This work lays the foundation for future research toward building scalable, efficient AI systems through principled reuse and orchestration of existing models.",
    "title_zh": "CABENCH：通过组合现成模型来解决复杂任务的可组合AI基准测试",
    "abstract_zh": "可组合AI通过将复杂AI任务分解为子任务，并由现成可用的模型分别处理，提供了一种可扩展且高效的解决方案范式。然而，在这一设置下系统性地评估相关方法的研究仍处于空白状态。本文提出了CABench，这是首个面向可组合AI的公开基准，包含70个真实场景的任务，以及跨多种模态和领域的700个精选模型。我们还提出了一种评估框架，以实现对可组合AI解决方案的端到端评估。为了建立初步基线，我们提供了人工设计的参考解决方案，并将其性能与两种基于大语言模型（LLM）的方法进行对比。实验结果表明，可组合AI在应对现实世界挑战方面展现出巨大潜力，同时也凸显了开发能够自动构建高效执行流程方法的迫切需求。本研究为未来通过有原则地复用与编排现有模型来构建可扩展、高效的AI系统奠定了坚实基础。"
  },
  {
    "date": "2025-12-30",
    "title": "Hybrid MEMS-FPGA Architectures with Transformer Models for Real-Time Speech and Language Processing",
    "authors": "Pallati Narsimhulu, Rajanikanth Aluvalu, Premkumar Chithaluru",
    "publish": "2025 2nd International Conference on Electronic Circuits and Signaling Technologies (ICECST)",
    "url": "https://doi.org/10.1109/icecst66106.2025.11307658",
    "source": "IEEE",
    "abstract": "Speech and language applications require high accuracy and fast response, yet many embedded processors struggle with the growing complexity of modern recognition models. Conventional solutions often produce delays or demand heavy energy use, while large computing platforms reduce portability. This creates a gap for designs that can combine speed, energy efficiency, and scalability in compact devices. This work introduces a hybrid platform that integrates Micro-Electro-Mechanical Systems (MEMS) microphones with Field Programmable Gate Arrays (FPGAs) for speech and language processing. MEMS microphones capture signals with clarity, while FPGA units accelerate intensive tasks. Classical algorithms such as Mel-Frequency Cepstral Coefficients (MFCC) are applied for feature extraction, and modern approaches such as lightweight Transformer models and compact large language models (LLMs) are deployed on FPGA resources for classification and sequence modeling. The hybrid integration of deep neural network (DNN) and Transformer-based modules allows accurate representation of acoustic and contextual patterns while maintaining real-time response. The platform is evaluated with recognition word error rate (WER), latency, signal-to-noise ratio (SNR) improvement, and energy consumption. Results show that combining MEMS sensing with FPGA acceleration and Transformer-based architectures improves both SNR and contextual understanding while reducing power usage. The system demonstrates significant improvement in latency and WER compared with traditional embedded setups, while remaining suitable for portable and low-energy environments. These findings highlight the value of blending MEMS hardware, FPGA accelerators, and state-of-the-art Artificial intelligence (AI) methods to create a practical path for next-generation speech and language technologies.",
    "title_zh": "基于Transformer模型的混合MEMS-FPGA架构在实时语音与语言处理中的应用",
    "abstract_zh": "语音与语言应用需要高精度和快速响应，但许多嵌入式处理器在应对现代识别模型日益增长的复杂性时面临挑战。传统解决方案往往导致延迟或消耗大量能量，而大型计算平台则牺牲了便携性。这为设计一种能够在紧凑设备中兼顾速度、能效与可扩展性的方案留下了空白。本文提出了一种混合平台，将微机电系统（MEMS）麦克风与现场可编程门阵列（FPGA）相结合，用于语音与语言处理。MEMS麦克风以高清晰度捕捉音频信号，而FPGA单元则加速计算密集型任务。经典算法如梅尔频率倒谱系数（MFCC）用于特征提取，现代方法如轻量级Transformer模型和小型大语言模型（LLMs）则部署于FPGA资源上，实现分类与序列建模。深度神经网络（DNN）与基于Transformer的模块的混合集成，能够在保持实时响应的同时，准确表征声学与上下文模式。该平台通过词错误率（WER）、延迟、信噪比（SNR）提升以及能耗等指标进行评估。结果表明，结合MEMS传感、FPGA加速与基于Transformer的架构，在提升SNR和上下文理解能力的同时，显著降低了功耗。相较于传统嵌入式系统，该系统在延迟和WER方面均有显著改善，同时仍适用于便携式和低功耗环境。这些发现凸显了将MEMS硬件、FPGA加速器与前沿人工智能（AI）方法融合的潜力，为下一代语音与语言技术提供了一条切实可行的发展路径。"
  },
  {
    "date": "2025-12-30",
    "title": "Supporting Pipelined Memory Accesses in Processor Synthesis",
    "authors": "Essien Taylor, Colin Schilf, Sebastian Phemister, Russ Joseph",
    "publish": "2025 IEEE 43rd International Conference on Computer Design (ICCD)",
    "url": "https://doi.org/10.1109/iccd65941.2025.00099",
    "source": "IEEE",
    "abstract": "In deep pipelines, cache accesses must be split over many cycles to accommodate high-clock rates. Given the variable memory access times, the volume of in-flight cache operations, and potential for memory data hazards, implementing the control logic necessary to provide correct operation and maximize performance emerges as a major design challenge. This paper introduces Forecast, a tool for synthesizing deep microprocessor pipelines with support for high-performance cache organizations. Given a functional description of a processor datapath and directives on how to split logic and state accesses into stages, Forecast automatically generates the complete pipeline datapath and fully functional control logic to correctly sequence operations across pipeline stages while maintaining correct operational semantics. Forecast produces logic to enable speculatively scheduled pipelined cache accesses, support buffered stores, and detect and resolve dependencies in the memory system. This tool allows designers to construct deeper pipelined processors with larger capacity and higher throughput caches, improving the performance of these automatically generated processors.",
    "title_zh": "在处理器综合中支持流水线内存访问",
    "abstract_zh": "在深度流水线中，为了适应高时钟频率，缓存访问必须分散到多个周期中。由于内存访问时间的可变性、飞行中的缓存操作数量庞大，以及潜在的内存数据冒险，实现能够确保正确运行并最大化性能的控制逻辑成为一项重大的设计挑战。本文介绍了Forecast——一种用于生成支持高性能缓存组织的深度微处理器流水线的工具。给定处理器数据通路的功能描述以及如何将逻辑和状态访问拆分到各个阶段的指令，Forecast能够自动生成完整的流水线数据通路和功能完备的控制逻辑，以正确地在流水线各阶段间调度操作，同时保持正确的操作语义。Forecast还生成了支持推测性调度流水线缓存访问、缓冲存储操作，以及检测和解决内存系统中依赖关系的逻辑。该工具使设计者能够构建更深的流水线处理器，配备更大容量和更高吞吐量的缓存，从而提升这些自动生成处理器的性能。"
  },
  {
    "date": "2025-12-30",
    "title": "Timing-Driven Multi-Bit Flip-Flop Allocation Utilizing Design-Technology Co-Optimization Techniques",
    "authors": "Yeongyeong Shin, Sehyeon Chung, Taewhan Kim",
    "publish": "2025 IEEE 43rd International Conference on Computer Design (ICCD)",
    "url": "https://doi.org/10.1109/iccd65941.2025.00066",
    "source": "IEEE",
    "abstract": "It has been well-known that using MBFFs (multi-bit flip-flops) offers a substantial reduction on the dynamic power consumption in digital circuits. However, their big size and limited mobility of their individual flip-flops lead to negative effects on optimizing circuit timing. This work overcomes this fundamental drawback in physical design automation. To this end, we propose two DTCO (design-technology co-optimization) techniques for optimizing timing on MBFF-intensive designs. Precisely, (1) at post-place stage, we make use of the diverse MBFF cell layout structures available with different I/O pin positions in way to remap each MBFF instance to an MBFF structure that is best suited for facilitating optimizing timing on wires, and (2) at post-route stage, we propose to use a new practically feasible scheme of MBFF cell layouts supporting mixed-VTs (threshold-voltages) on individual flip-flops, by which we are able to resolve the timing violations in a fine-grained manner at the cost of much less power overhead than that of the conventional method of VT assignment on MBFF cell instances. In the meantime, through experiments with benchmark circuits, it is shown that in comparison with the final chip implementations produced by the prior MBFF allocation methods i.e., commercial EDA method of MBFF allocation and state-of-the-art MBFF allocation method, Step 1 (i.e., DTCO at post-place) of our DTCO-driven MBFF allocation method is able to reduce the amount of total negative time slack by 11.41% and 10.82% on average, while Step 2 (i.e, our DTCO at post-route) reduces the power consumption of MBFFs by 4.63% over that of the prior methods of conventional VT assignment with no timing degradation.",
    "title_zh": "利用设计-技术协同优化技术的时序驱动多比特触发器分配",
    "abstract_zh": "众所周知，使用多比特触发器（MBFFs）能够在数字电路中显著降低动态功耗。然而，其较大的尺寸以及单个触发器位置灵活性差的问题，会对电路时序优化产生负面影响。本文克服了物理设计自动化中的这一根本性缺陷。为此，我们提出了两种面向MBFF密集型设计的DTCO（设计-工艺协同优化）技术，以优化时序性能。具体而言：（1）在布局后阶段，我们利用不同I/O引脚位置所对应的多样化MBFF单元版图结构，将每个MBFF实例重新映射到最适合于优化布线时序的MBFF结构；（2）在布线后阶段，我们提出一种新的、实际可行的MBFF单元版图方案，支持在单个触发器上实现混合阈值电压（mixed-VTs），从而以远低于传统MBFF单元阈值电压分配方法的功耗开销，实现对时序违例的细粒度修复。通过基准电路实验验证，与先前的MBFF分配方法（包括商用EDA工具的MBFF分配方法和当前最先进的MBFF分配方法）所生成的最终芯片实现相比，我们提出的DTCO驱动的MBFF分配方法中的第一步（即布局后的DTCO优化）平均可减少总负时序余量11.41%和10.82%；第二步（即布线后的DTCO优化）在不造成时序退化的情况下，使MBFF的功耗降低了4.63%。"
  },
  {
    "date": "2025-12-30",
    "title": "Adaptive Two-Layer Inspection Framework for Mitigating Security Risks in Large-Scale Vertical Domain Language Models",
    "authors": "Wei Liang, Zhengkai Guo, Junqiang Li, Xiaocui Li, Junfeng Yang, Yangyan Zeng, Xiaokang Zhou",
    "publish": "2025 IEEE International Symposium on Product Compliance Engineering - Asia (ISPCE-ASIA)",
    "url": "https://doi.org/10.1109/ispce-asia69076.2025.11312923",
    "source": "IEEE",
    "abstract": "Large language models (LLMs) are trained on large amounts of data with diverse and imbalance distribution, therein leading to capability bottlenecks. In vertical domains, incremental domain-specific pre-training has become a common method to capability enhancement. However, without subsequent safety alignment, injecting domain data can perturb the base model’s distribution and increase jailbreak success rates, which may increase the risk of security vulnerabilities in LLMs. Prior work largely emphasizes domain performance gaining from fine-tuning or incremental pre-training, with limited attention to safety risks induced by distributional shifts. In this study, we address this gap by proposing an adaptive twolayer inspection framework that enforces safety checks at both the input and output token levels. On the input side, semantic embeddings and clustering-based intent tagging drive dynamic rule injection against adversarial prompts. On the output side, a speculative decoding guard performs per-token safety classification with thresholding and resampling-based fallback. Experiments on open-source LLMs demonstrate that our framework reduces Attack Success Rate (ASR) under multiple temperatures and remains effective after domain-specific post-training, mitigating Boundary Risk Exposure (BRE) fragmentation while preserving task utility.",
    "title_zh": "大规模垂直领域语言模型安全风险缓解的自适应双层检测框架",
    "abstract_zh": "大型语言模型（LLMs）在包含多样性和分布不均衡的海量数据上进行训练，由此导致能力瓶颈。在垂直领域中，增量式领域特定预训练已成为提升模型能力的常用方法。然而，若缺乏后续的安全对齐，注入领域数据可能扰乱基础模型的分布，从而提高越狱攻击的成功率，增加大模型潜在的安全风险。以往研究主要关注通过微调或增量预训练提升领域性能，而对由分布偏移引发的安全风险关注较少。本文针对这一空白，提出一种自适应的两层检测框架，在输入和输出两个层面均实施安全检查。在输入端，基于语义嵌入与聚类的意图标签机制实现对抗性提示的动态规则注入；在输出端，采用推测解码防护机制对每个token进行安全分类，并结合阈值判断与重采样策略作为回退方案。在开源大模型上的实验表明，该框架在多种温度设置下均能有效降低攻击成功率（ASR），且在完成领域特定后训练后仍保持有效性，显著缓解了边界风险暴露（BRE）的碎片化问题，同时维持了任务实用性。"
  },
  {
    "date": "2025-12-30",
    "title": "PCB Stack-Up Recognition Using LLMs",
    "authors": "Jie Li, Shiming Qin, Tingting Liu, Yuyu Zhu, Haoran Li, Cailiang Fu",
    "publish": "2025 International Workshop on Advanced Interconnects (WAI)",
    "url": "https://doi.org/10.1109/wai67900.2025.11309322",
    "source": "IEEE",
    "abstract": "With the increasing integration of electronic products, printed circuit board (PCB) are evolving toward higher speeds and more layered structures. However, the diversity of stack-up rules has led to significant challenges in the interoperability of design simulation and manufacturing processes across different vendors and users. To address this issue, this paper proposes an intelligent stack-up recognition method based on large language models (LLMs), which automatically converts PCB stack-up information into a standardized format defined by users or manufacturers. Experimental results demonstrate that online LLMs with over 500 billion parameters achieve a high recognition accuracy, exceeding 92%. In scenarios demanding data confidentiality or facing computational constraints, locally deployed lightweight models are a preferable alternative. However, models with no more than 8 billion parameters exhibit a significantly lower accuracy of below 37%, which is insufficient for practical purposes. Therefore, further investigation is essential to improve the viability of local lightweight models for this application.",
    "title_zh": "使用大语言模型进行PCB堆叠识别",
    "abstract_zh": "随着电子产品集成度的不断提高，印刷电路板（PCB）正朝着更高传输速率和更复杂的多层结构方向发展。然而，叠层规则的多样化给不同供应商和用户之间设计仿真与制造流程的互操作性带来了巨大挑战。为解决这一问题，本文提出了一种基于大语言模型（LLM）的智能叠层识别方法，可将PCB叠层信息自动转换为用户或制造商定义的标准格式。实验结果表明，参数量超过5000亿的在线大语言模型能够实现超过92%的高识别准确率。在对数据保密性有要求或面临计算资源受限的场景下，本地部署的轻量级模型则成为更优选择。然而，参数量不超过80亿的轻量级模型准确率显著降低，低于37%，难以满足实际应用需求。因此，有必要进一步研究，以提升本地轻量级模型在此类应用中的可行性。"
  },
  {
    "date": "2025-12-30",
    "title": "Implementation of Fair Chance Round Robin Arbiter and its Comparative Analysis",
    "authors": "Anuradha Shenoy, Anika Kirana Venkat, Adithya S Chakravarty, Smitha Gayathri D",
    "publish": "2025 IEEE International Conference for Women in Innovation, Technology &amp;amp; Entrepreneurship (ICWITE)",
    "url": "https://doi.org/10.1109/icwite64848.2025.11307144",
    "source": "IEEE",
    "abstract": "Efficient arbitration is highly necessary in digital systems where multiple requesters compete for shared resources. Traditional methods such as priority-based and round-robin arbiters which are simple to implement but either suffer from starvation or lack the adaptability required to provide for fair access. This paper presents the design and implementation of a novel arbitration scheme Fair Chance Round Robin (FCRR) which combines the structure of priority arbitration with a dynamic masking mechanism to ensure fair access. This is developed using Verilog HDL and tested on the Zybo-Z7 FPGA platform where FCRR addresses the shortcomings of static arbiters by integrating return-to-zero latches that temporarily blocks previously served requesters. And the comparative synthesis and simulation analysis demonstrate that FCRR achieves improved fairness with minimal hardware overhead. It achieves competitive performance, optimal logic resource utilization and reduced risk of starvation, making it a viable option for real-time systems, Network-on-Chip (NoC) designs and embedded systems.",
    "title_zh": "公平机会轮转仲裁器的实现及其对比分析",
    "abstract_zh": "在多个请求者竞争共享资源的数字系统中，高效的仲裁机制至关重要。传统的优先级仲裁和轮询仲裁方法虽然实现简单，但要么存在饥饿问题，要么缺乏灵活性，难以保证公平访问。本文提出了一种新型仲裁方案——公平机会轮询（Fair Chance Round Robin, FCRR），该方案结合了优先级仲裁的结构与动态屏蔽机制，以确保公平性。FCRR采用Verilog HDL进行设计与实现，并在Zybo-Z7 FPGA平台上完成测试。该方案通过引入“归零锁存器”机制，临时阻塞先前已被服务过的请求者，从而克服了静态仲裁器的缺陷。对比合成与仿真分析结果表明，FCRR在实现显著提升公平性的同时，仅带来极小的硬件开销，具备优异的性能表现、最优的逻辑资源利用率以及较低的饥饿风险。因此，FCRR可作为实时系统、片上网络（NoC）设计及嵌入式系统中的可行仲裁解决方案。"
  },
  {
    "date": "2025-12-30",
    "title": "Evaluating Large Language Models for Line-Level Vulnerability Localization",
    "authors": "Jian Zhang, Chong Wang, Anran Li, Weisong Sun, Ceng Zhang, Wei Ma, Yang Liu",
    "publish": "IEEE Transactions on Software Engineering",
    "url": "https://doi.org/10.1109/tse.2025.3649250",
    "source": "IEEE",
    "abstract": "Recently, Automated Vulnerability Localization (AVL) has attracted growing attention, aiming to facilitate diagnosis by pinpointing the specific lines of code responsible for vulnerabilities. Large Language Models (LLMs) have shown potential in various domains, yet their effectiveness in line-level vulnerability localization remains underexplored. <p xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">In this work, we present the first comprehensive empirical evaluation of LLMs for AVL. Our study examines 19 leading LLMs suitable for code analysis, including ChatGPT and multiple open-source models, spanning encoder-only, encoder-decoder, and decoder-only architectures, with model sizes from 60M to 70B parameters. We evaluate three paradigms—few-shot prompting, discriminative fine-tuning, and generative fine-tuning—with and without Low-Rank Adaptation (LoRA), on both a BigVul-derived dataset for C/C++ and a smart contract vulnerability dataset.</p> <p xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Our results show that discriminative fine-tuning achieves substantial performance gains over existing learning-based AVL methods when sufficient training data is available. In low-data settings, prompting advanced LLMs such as ChatGPT proves more effective. We also identify challenges related to input length and unidirectional context during fine-tuning, and propose two remedial strategies: a sliding window approach and right-forward embedding, both of which yield significant improvements. Moreover, we provide the first assessment of LLM generalizability in AVL, showing that certain models can transfer effectively across Common Weakness Enumerations (CWEs) and projects. However, performance degrades notably for newly discovered vulnerabilities containing unfamiliar lexical or structural patterns, underscoring the need for continual adaptation. These findings offer practical guidance for deploying LLM-based AVL systems in realistic software security workflows.</p>",
    "title_zh": "评估大型语言模型在行级漏洞定位中的应用",
    "abstract_zh": "近年来，自动化漏洞定位（Automated Vulnerability Localization, AVL）引起了越来越多的关注，其目标是通过精确定位导致漏洞的具体代码行来辅助漏洞诊断。大型语言模型（Large Language Models, LLMs）在多个领域展现出巨大潜力，但其在代码级漏洞定位方面的有效性仍缺乏深入探索。本文首次对LLMs在AVL任务中的表现进行了全面的实证评估。我们的研究涵盖了19种适用于代码分析的主流LLM，包括ChatGPT以及多种开源模型，覆盖了仅编码器（encoder-only）、编码器-解码器（encoder-decoder）和仅解码器（decoder-only）三类架构，模型参数规模从60M到70B不等。我们在C/C++语言的BigVul衍生数据集和智能合约漏洞数据集上，评估了三种范式——少样本提示（few-shot prompting）、判别式微调（discriminative fine-tuning）和生成式微调（generative fine-tuning），并对比了是否采用低秩适配（LoRA）的效果。\n\n实验结果表明，在拥有充足训练数据的情况下，判别式微调显著优于现有的基于学习的AVL方法。而在数据稀缺的场景下，直接使用如ChatGPT等先进的大模型进行提示则更为有效。我们还识别出在微调过程中存在的输入长度限制和单向上下文依赖等问题，并提出了两种改进策略：滑动窗口方法和右向嵌入（right-forward embedding），两者均带来了显著的性能提升。此外，我们首次系统评估了LLMs在AVL任务中的泛化能力，发现某些模型能够在不同的通用弱点枚举（Common Weakness Enumerations, CWE）类别及项目之间实现有效迁移。然而，对于包含陌生词汇或结构模式的新发现漏洞，模型性能明显下降，这凸显了持续适应与更新的必要性。这些发现为在真实软件安全工作流中部署基于LLM的AVL系统提供了切实可行的指导。"
  },
  {
    "date": "2025-12-30",
    "title": "2025 5th International Conference on Code Quality (ICCQ)",
    "authors": "N/A",
    "publish": "N/A",
    "url": "https://doi.org/10.1109/iccq65694.2025",
    "source": "IEEE",
    "abstract": "",
    "title_zh": "2025 年第五届国际代码质量会议（ICCQ）",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-30",
    "title": "Power Estimation Framework for Power Consumption Analysis of SoC with Multiple Operating Modes and Supply Domains",
    "authors": "Siddharth Katare",
    "publish": "2025 IEEE International Conference on Recent Advances in Systems Science and Engineering (RASSE)",
    "url": "https://doi.org/10.1109/rasse64831.2025.11315479",
    "source": "IEEE",
    "abstract": "Accurate power estimation in System-on-Chip (SoC) designs is a persistent challenge due to increasing architectural complexity, multiple power domains, and dynamic operating conditions. Traditional spreadsheet-based methods are often inadequate for capturing the nuanced behavior of hierarchical systems, especially when components operate under varying voltage, temperature, and process scenarios. As SoCs scale in size and functionality, the need for a flexible, scalable, and precise power modeling framework becomes critical. Designers require tools that can simulate real-world operating modes, support iterative development, and provide actionable insights into power distribution across the chip. This paper introduces a Python-based framework that addresses these challenges by integrating a graphical user interface (GUI) with backend computational logic for hierarchical power estimation. The framework employs a master-instance methodology. Higher-level master cells instantiate lower-level cells, forming a scalable hierarchy that accurately models power behavior across multiple levels of abstraction. Each instantiated cell retains its own power mode configuration, enabling accurate power estimate under varying conditions. Results are visualized in a hierarchical tree format, offering clear insights into power distribution across domains and components. This structure supports drill-down analysis, allowing designers to identify power-intensive modules and make informed decisions about redesigns or optimizations.",
    "title_zh": "面向具有多种工作模式和供电域的片上系统（SoC）功耗分析的功耗估算框架",
    "abstract_zh": "在片上系统（SoC）设计中，精确的功耗估算始终是一项持续的挑战，这主要源于日益复杂的架构、多电源域以及动态工作条件等因素。传统的基于电子表格的方法往往难以捕捉分层系统中组件的细微行为特征，尤其是在不同电压、温度和工艺条件下运行时。随着SoC在规模和功能上的不断扩展，建立一个灵活、可扩展且高精度的功耗建模框架变得至关重要。设计师需要能够模拟真实工作模式、支持迭代开发，并提供关于芯片上功耗分布的可操作洞察力的工具。本文提出了一种基于Python的框架，通过将图形用户界面（GUI）与后端计算逻辑相结合，有效应对上述挑战，实现分层功耗估算。该框架采用“主-实例”（master-instance）方法：高层主单元实例化低层单元，形成可扩展的层次结构，从而在多个抽象层级上准确建模功耗特性。每个实例化单元均可保留独立的功耗模式配置，能够在不同工作条件下实现精准的功耗估算。结果以分层树状图形式呈现，清晰展示各域和组件之间的功耗分布情况。该结构支持逐层深入分析，使设计者能够快速识别高功耗模块，并据此做出合理的重构或优化决策。"
  },
  {
    "date": "2025-12-30",
    "title": "Challenges &amp; Opportunities with LLM-Assisted Visualization Retargeting",
    "authors": "Luke S. Snyder, Chenglong Wang, Steven M. Drucker",
    "publish": "2025 IEEE Visualization and Visual Analytics (VIS)",
    "url": "https://doi.org/10.1109/vis60296.2025.00034",
    "source": "IEEE",
    "abstract": "Despite the ubiquity of visualization examples published on the web, retargeting existing custom chart implementations to new datasets remains difficult, time-intensive, and tedious. The adaptation process assumes author familiarity with both the implementation of the example as well as how the new dataset might need to be transformed to fit into the example code. With recent advances in Large Language Models (LLMs), automatic adaptation of code can be achieved from high-level user prompts, reducing the barrier for visualization retargeting. To better understand how LLMs can assist retargeting and its potential limitations, we characterize and evaluate the performance of LLM assistance across multiple datasets and charts of varying complexity, categorizing failures according to type and severity. In our evaluation, we compare two approaches: (1) directly instructing the LLM model to fully generate and adapt code by treating code as text inputs and (2) a more constrained program synthesis pipeline where the LLM guides the code construction process by providing structural information (e.g., visual encodings) based on properties of the example code and data. We find that both approaches struggle when new data has not been appropriately transformed, and discuss important design recommendations for future retargeting systems.",
    "title_zh": "大语言模型辅助可视化重定向的挑战与机遇",
    "abstract_zh": "尽管网络上充斥着各种可视化示例，但将现有的自定义图表实现迁移到新数据集仍然困难、耗时且繁琐。这一适应过程要求作者既熟悉示例代码的实现方式，又了解如何对新数据集进行转换以适配示例代码。随着大型语言模型（LLMs）的最新进展，可以通过高层次的用户提示实现代码的自动适配，从而降低可视化迁移的门槛。为了更深入地理解LLM在迁移过程中的作用及其潜在局限性，我们对LLM辅助在多种数据集和不同复杂度图表上的表现进行了系统性分析与评估，并根据故障类型和严重程度对其进行分类。在我们的评估中，比较了两种方法：(1) 直接向LLM指令，将其视为文本输入，完全生成并适配代码；(2) 一种更为受限的程序合成流程，即LLM基于示例代码和数据的特性，提供结构信息（如视觉编码），引导代码构建过程。我们发现，当新数据未经过适当转换时，两种方法均面临困难，并据此提出了未来迁移系统设计的重要建议。"
  },
  {
    "date": "2025-12-30",
    "title": "Proceedings of the 18th ACM Workshop on Artificial Intelligence and Security",
    "authors": "N/A",
    "publish": "N/A",
    "url": "https://doi.org/10.1145/3733799",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "第18届ACM人工智能与安全研讨会论文集",
    "abstract_zh": "None"
  }
]