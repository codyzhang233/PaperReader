[
  {
    "date": "2026-01-05",
    "title": "Automatic Assertion Mining in Assertion-Based Verification: Techniques, Challenges, and Future Directions",
    "authors": "Mohammad Reza Heidari Iman, Giorgio Di Natale, Katell Morin-Allory",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02248v1",
    "source": "arXiv",
    "abstract": "Functional verification increasingly relies on Assertion-Based Verification (ABV), which has become a key approach for verifying hardware designs due to its efficiency and effectiveness. Central to ABV are automatic assertion miners, which apply different techniques to generate assertions automatically. This paper reviews the most recent, advanced, and widely adopted assertion miners, offering a comparative analysis of their methodologies. The goal is to provide researchers and verification practitioners with insights into the capabilities and limitations of existing miners. By identifying their shortcomings, this work also points toward directions for developing more powerful and advanced assertion miners in the future.",
    "title_zh": "基于断言的验证中的自动断言挖掘：技术、挑战与未来方向",
    "abstract_zh": "功能验证越来越依赖于断言驱动的验证（Assertion-Based Verification, ABV），由于其高效性和有效性，ABV已成为验证硬件设计的关键方法。ABV的核心是自动断言挖掘工具，这些工具采用不同的技术手段，能够自动生成断言。本文综述了近年来最先进且广泛应用的断言挖掘工具，并对其方法学进行了比较分析。研究旨在为研究人员和验证实践者提供对现有挖掘工具能力与局限性的深入理解。通过识别现有工具的不足之处，本文也为未来开发更强大、更先进的断言挖掘工具指明了发展方向。"
  },
  {
    "date": "2026-01-05",
    "title": "HFRWKV: A High-Performance Fully On-Chip Hardware Accelerator for RWKV",
    "authors": "Liu Shijie, Zeng Zhenghao, Jiao Han, Huang Yihua",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02135v1",
    "source": "arXiv",
    "abstract": "RWKV is a modern RNN architecture that approaches the performance of Transformers, with the advantage of processing long contexts at a linear memory cost. However, its sequential computation pattern struggles to efficiently leverage GPU parallelism, which leads to low compute resource utilization. Furthermore, frequent off-chip weight accesses create a memory bottleneck. To address these challenges, we propose HFRWKV, an FPGA-based hardware accelerator specifically designed for RWKV. Within the matrix operation module, we propose a novel hardware-friendly hybrid-precision quantization strategy, which enhances performance while maintaining acceptable accuracy. For the complex operations including exponentiation and division, we introduce a method featuring reusable architectures combined with lookup tables or piecewise linear approximation, which is algorithmically refined to effectively balance precision and hardware resource consumption. Based on this foundation, we adopt a fully on-chip computing system integrating parallel matrix-vector processing array and an efficient pipeline architecture. Through computation reordering and chunked double buffering, it effectively eliminates data transfer bottlenecks and improves overall throughput. We implement HFRWKV on the Alveo U50 and U280 platform. Experimental results show that compared to a CPU, a throughput improvement of 63.48$\\times$ and an energy efficiency improvement of 139.17$\\times$. Compared to GPUs, achieves a throughput improvement of 32.33$\\times$ and an energy efficiency improvement of 171.36$\\times$.",
    "title_zh": "HFRWKV：一种高性能全芯片硬件加速的RWKV实现",
    "abstract_zh": "RWKV 是一种现代的循环神经网络（RNN）架构，其性能可媲美 Transformer，同时具备在长上下文处理中线性内存开销的优势。然而，其固有的串行计算模式难以高效利用 GPU 的并行能力，导致计算资源利用率低下。此外，频繁的片外权重访问也造成了严重的内存瓶颈。为解决上述挑战，我们提出了 HFRWKV——一种专为 RWKV 设计的基于 FPGA 的硬件加速器。在矩阵运算模块中，我们提出了一种新颖的、面向硬件友好的混合精度量化策略，在保持可接受精度的同时显著提升了性能。针对指数运算、除法等复杂操作，我们引入了一种可复用架构结合查表或分段线性逼近的方法，并通过算法优化，有效平衡了精度与硬件资源消耗。在此基础上，我们构建了一个完全集成于芯片内部的计算系统，包含并行的矩阵-向量处理阵列和高效的流水线架构。通过计算顺序重排与分块双缓冲技术，有效消除了数据传输瓶颈，大幅提升了整体吞吐量。我们在 Alveo U50 和 U280 平台上实现了 HFRWKV。实验结果表明，与 CPU 相比，HFRWKV 实现了 63.48 倍的吞吐量提升和 139.17 倍的能量效率提升；与 GPU 相比，分别实现了 32.33 倍的吞吐量提升和 171.36 倍的能量效率提升。"
  },
  {
    "date": "2026-01-05",
    "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios",
    "authors": "Defei Xia, Bingfeng Pi, Shenbin Zhang, Song Hua, Yunfei Wei, Lei Zuo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01857v1",
    "source": "arXiv",
    "abstract": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.",
    "title_zh": "极智代理：面向真实场景中的经验驱动精度优化",
    "abstract_zh": "随着由大型语言模型（LLMs）驱动的代理系统不断发展，提升自主代理在上下文理解、工具使用和响应生成等方面的任务性能变得日益关键。尽管先前的研究已在基于LLM的代理整体架构设计方面取得进展，但对其内部推理与工具使用流程的系统性优化仍处于探索阶段。本文提出了一种基于真实世界实践经验的代理框架，并包含三项核心创新：（1）一种自适应提示生成策略，能够根据代理状态和任务目标动态调整，从而提升系统的可靠性与鲁棒性；（2）一种上下文感知的工具编排模块，可根据用户意图和上下文环境实现工具分类、语义检索与自适应调用；（3）一种分层记忆机制，通过动态摘要与压缩，整合会话记忆、任务历史及外部摘要信息，显著提升信息的相关性与处理效率。在此基础上，我们构建了一个名为Jenius-Agent的端到端框架，集成了三项关键技术优化：基于模型上下文协议（MCP）的工具支持、文件输入/输出（I/O）功能以及执行反馈机制。实验结果表明，该框架在任务准确率上提升了20%，同时降低了token消耗、响应延迟和工具调用失败率。目前该框架已部署于Jenius平台（https://www.jenius.cn），为构建轻量、可扩展且协议兼容的鲁棒型自主代理提供了一套高效解决方案。"
  },
  {
    "date": "2026-01-05",
    "title": "AR-MOT: Autoregressive Multi-object Tracking",
    "authors": "Lianjie Jia, Yuhan Wu, Binghao Ran, Yifan Wang, Lijun Wang, Huchuan Lu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01925v1",
    "source": "arXiv",
    "abstract": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
    "title_zh": "AR-MOT：自回归多目标跟踪",
    "abstract_zh": "随着多目标跟踪（MOT）任务不断向更通用、多模态的方向发展，现有MOT方法所采用的僵化且任务特定的架构日益限制其在多样化任务中的适用性，并制约了对新型跟踪范式的灵活适应能力。大多数现有方法依赖于固定的输出头和定制化的跟踪流程，难以扩展到更复杂或指令驱动的任务场景。为解决上述问题，我们提出AR-MOT，一种新颖的自回归范式，将MOT建模为大语言模型（LLM）框架下的序列生成任务。该设计使模型能够通过灵活的序列构造输出结构化结果，而无需任何任务特定的输出头。为增强区域级别的视觉感知能力，我们引入了一种基于预训练检测器的物体标记器（Object Tokenizer）；为缓解全局特征与局部区域特征之间的错位问题，我们提出了区域感知对齐（Region-Aware Alignment, RAA）模块；为支持长期跟踪，我们设计了时间记忆融合（Temporal Memory Fusion, TMF）模块，用于缓存历史物体标记。AR-MOT具有强大的可扩展性：通过简单修改输出序列格式即可集成新模态或新指令，而无需改变模型架构本身。在MOT17和DanceTrack数据集上的大量实验验证了该方法的可行性，其性能达到甚至媲美当前最先进水平，同时为构建更加通用和灵活的MOT系统奠定了基础。"
  },
  {
    "date": "2026-01-05",
    "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
    "authors": "Falcon LLM Team, Iheb Chaabane, Puneesh Khanna, Suhail Mohmad, Slim Frikha, Shi Hu, Abdalgader Abubaker, Reda Alami, Mikhail Lubinets, Mohamed El Amine Seddik, Hakim Hacid",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02346v1",
    "source": "arXiv",
    "abstract": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
    "title_zh": "Falcon-H1R：通过混合模型推动推理边界，实现高效的测试时扩展",
    "abstract_zh": "本文介绍了Falcon-H1R，这是一个参数量为7B的推理优化模型，证明了小型语言模型（SLMs）在实现具有竞争力的推理性能方面具备可行性。Falcon-H1R以其出色的参数效率著称，在多种高推理强度基准测试中，其表现始终与参数量为其2至7倍的当前最先进（SOTA）推理模型相当甚至更优。这些结果凸显了精心的数据筛选以及针对性训练策略（通过高效的SFT和强化学习扩展）的重要性——在不增加模型规模的前提下，仍能带来显著的性能提升。此外，Falcon-H1R通过结合混合并行架构设计带来的快速推理、更高的token效率以及更优的准确性，突破了推理效率的三维极限。这种独特组合使Falcon-H1R-7B成为构建可扩展高级推理系统的一个实用核心组件，尤其适用于需要大量思维链生成及并行测试时扩展的场景。借助最近提出的DeepConf方法，Falcon-H1R实现了最先进的测试时扩展效率，在准确率和计算成本方面均取得显著改进。因此，Falcon-H1R表明：通过有针对性的模型训练和架构选择，紧凑型模型同样能够实现强大且可扩展的推理能力。"
  },
  {
    "date": "2026-01-05",
    "title": "Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations",
    "authors": "Alexander Korn, Lea Zaruchas, Chetan Arora, Andreas Metzger, Sven Smolka, Fanyu Wang, Andreas Vogelsang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01954v1",
    "source": "arXiv",
    "abstract": "Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.",
    "title_zh": "报告大语言模型提示在自动化软件工程中的应用：基于当前实践与期望的指南",
    "abstract_zh": "大型语言模型，尤其是仅使用解码器的生成式模型（如GPT），正被越来越多地用于自动化软件工程任务。这些模型主要通过自然语言提示进行引导，因此提示工程成为影响系统性能和行为的关键因素。尽管这类模型在软件工程研究中的作用日益重要，但关于提示设计、测试和优化的相关决策却很少以系统化或透明的方式记录，这阻碍了不同研究之间的可复现性和可比性。为弥补这一空白，我们开展了一项两阶段的实证研究：首先，我们分析了自2022年以来在顶级三大软件工程会议发表的近300篇论文，评估当前提示设计、测试与优化的报告情况；其次，我们对这些会议的105名程序委员会成员进行了调查，以了解他们对基于大语言模型的研究中提示报告的期望。基于研究发现，我们制定了一套结构化的报告指南，明确区分了必要、理想和卓越的报告要素。研究结果揭示了当前实践与评审者期望之间存在显著差距，尤其是在版本披露、提示合理性说明以及有效性威胁方面。我们提出该指南，旨在推动基于大语言模型的软件工程研究在透明度、可复现性和方法严谨性方面的持续改进。"
  },
  {
    "date": "2026-01-05",
    "title": "LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment",
    "authors": "Arsham Khosravani, Alireza Hosseinpour, Arshia Akhavan, Mehdi Keshani, Abbas Heydarnoori",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01780v1",
    "source": "arXiv",
    "abstract": "Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.",
    "title_zh": "LIA：面向自动问题分配的大规模语言模型监督微调",
    "abstract_zh": "问题分配是软件维护中的一个关键流程，涉及对新提交的问题报告进行验证并分配给合适的开发人员。然而，在大型开源项目中，每月会报告成千上万的新问题，此时手动分配问题往往存在不一致且容易出错的问题。现有的自动化方法虽已展现出一定潜力，但许多方法严重依赖大量特定项目的训练数据或关系信息，而这些信息通常稀疏且噪声较多，从而限制了其实际效果。为应对这些挑战，我们提出了LIA（基于大语言模型的Issue分配）方法，该方法采用监督微调技术，将大语言模型DeepSeek-R1-Distill-Llama-8B适配于自动问题分配任务。LIA利用大语言模型在自然语言及软件相关文本上的预训练语义理解能力，直接从问题标题和描述中学习生成按优先级排序的开发者推荐列表。该排序基于模型对历史问题与开发者分配模式的学习，通过借鉴以往任务中的规律，推断出最有可能处理新问题的开发人员。经过全面评估，我们证明LIA在性能上显著优于其基础预训练模型以及当前最先进的基线方法：相较于DeepSeek-R1-Distill-Llama-8B的基础模型，LIA在Hit@1指标上最高提升达+187.8%；同时，在Hit@1得分上比四种领先的分配方法最高提升+211.2%。这些结果充分展示了领域适配的大语言模型在软件维护任务中的有效性，并确立了LIA作为一项实用且高性能的问题分配解决方案。"
  },
  {
    "date": "2026-01-05",
    "title": "Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches",
    "authors": "Sai Varun Kodathala, Rakesh Vunnam",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01774v1",
    "source": "arXiv",
    "abstract": "Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.",
    "title_zh": "大型语言模型能否解决工程方程？直接预测与求解器辅助方法的系统性比较",
    "abstract_zh": "在工程实践中，需要通过迭代数值方法求解的超越方程无处不在，从流体力学中的摩擦系数计算到轨道位置确定均涉及此类问题。本文系统评估了大型语言模型（LLM）是否能够通过直接数值预测求解这些方程，抑或采用一种混合架构——即结合LLM的符号运算能力与经典迭代求解器——更为有效。我们在涵盖七个工程领域的100个问题上测试了六种最先进的模型（GPT-5.1、GPT-5.2、Gemini-3-Flash、Gemini-2.5-Lite、Claude-Sonnet-4.5、Claude-Opus-4.5），比较了直接预测与“求解器辅助计算”两种方式：前者由LLM直接输出结果，后者则由LLM负责建立控制方程并提供初始条件，再由牛顿-拉夫逊迭代法完成数值求解。\n\n结果显示，直接预测方式下各模型的平均相对误差为0.765至1.262；而求解器辅助计算方式的误差降至0.225至0.301，误差降低幅度达67.9%至81.8%。领域特异性分析表明，在电子学领域，由于指数型方程对初值高度敏感，误差改善高达93.1%；而在流体力学领域，改进仅约7.2%，这得益于LLM在模式识别方面的良好表现。上述发现表明，当前的大型语言模型在符号运算和领域知识检索方面表现出色，但在高精度要求的迭代算术任务中仍存在明显局限。因此，其最有效的应用方式应是作为连接经典数值求解器的智能接口，而非独立的计算引擎。"
  },
  {
    "date": "2026-01-05",
    "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models",
    "authors": "Yihao Liang, Ze Wang, Hao Chen, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Emad Barsoum, Zicheng Liu, Niraj K. Jha",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02236v1",
    "source": "arXiv",
    "abstract": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM",
    "title_zh": "CD4LM：一致性蒸馏与自适应解码用于扩散语言模型",
    "abstract_zh": "自回归大语言模型在众多基准测试中取得了优异表现，但其解码过程仍受制于对先前生成标记的顺序依赖，导致延迟问题。扩散语言模型（DLMs）虽可实现并行生成，却面临一个根本性的静态到动态的错配：训练阶段优化的是固定调度下的局部转移，而高效推理则需要通过未见状态进行自适应的“长跳”精炼。我们的目标是使DLMs实现高度并行的解码，在函数评估次数极低的前提下仍保持生成质量。为此，我们提出CD4LM框架，通过离散空间一致性蒸馏（DSCD）和置信度自适应解码（CAD）实现训练与推理的解耦。不同于常规目标，DSCD训练学生模型具备轨迹不变性，能够将多种噪声状态直接映射至干净分布。这种内在鲁棒性使得CAD可根据标记置信度动态分配计算资源，激进地跳过步骤，而不会出现传统启发式加速方法常见的质量下降。在GSM8K数据集上，CD4LM以5.18倍的时钟速度提升达到了LLaDA基线的性能；在代码与数学类基准测试中，其严格超越了现有准确率-效率帕累托前沿，平均提速达3.62倍的同时还提升了整体准确率。代码已开源，地址为：https://github.com/yihao-liang/CDLM"
  },
  {
    "date": "2026-01-05",
    "title": "Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation",
    "authors": "Meiman Xiao, Ante Wang, Qingguo Hu, Zhongjian Miao, Huangjun Shen, Longyue Wang, Weihua Luo, Jinsong Su",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01768v1",
    "source": "arXiv",
    "abstract": "Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.",
    "title_zh": "大型语言模型能否追踪其输出长度？一种精确长度调控的动态反馈机制",
    "abstract_zh": "精确控制生成文本的长度是现实应用中的常见需求。然而，尽管大型语言模型（LLMs）在遵循人类指令方面取得了显著进展，仍难以准确完成这一任务。本文表明，LLMs 常常无法正确衡量输入文本的长度，从而导致对长度约束的遵守效果不佳。为解决此问题，我们提出一种新颖的长度调控方法，在生成过程中引入动态长度反馈，实现对目标长度的自适应调整。在摘要生成和人物传记生成任务上的实验表明，该无需训练的方法显著提升了达到目标词元、单词或句子数量的精确度，且不损害生成质量。此外，我们还证明，通过进一步的监督微调，该方法能够有效推广至更广泛的文本生成任务中。"
  },
  {
    "date": "2026-01-05",
    "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming",
    "authors": "Nguyet-Anh H. Lang, Eric Lang, Thanh Le-Cong, Bach Le, Quyet-Thang Huynh",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02060v1",
    "source": "arXiv",
    "abstract": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.",
    "title_zh": "消亡还是繁荣？对大语言模型在函数式编程代码生成中的综合评估",
    "abstract_zh": "函数式编程为构建可靠且安全的软件系统提供了坚实的基础，但由于学习曲线陡峭，其应用尚未广泛普及。近年来，大型语言模型（LLMs）在代码生成方面的进展为降低这一门槛带来了新机遇。然而，对LLM的大量评估主要集中在命令式编程语言上，其在函数式编程语言（FP）中的能力仍鲜有深入研究。为填补这一空白，我们提出了FPEval——一个基于全新基准FPBench的综合性评估框架。FPBench包含721个编程任务，覆盖三个主流函数式语言：Haskell、Ocaml和Scala，并分为三个难度等级。FPEval提供了全面的评估基础设施，包括完整的测试套件进行测试验证，以及静态分析工具，用于评估代码的功能正确性、代码风格与可维护性。\n\n利用该框架，我们评估了当前最先进的LLM（包括GPT-3.5、GPT-4o和GPT-5）在函数式编程语言及Java（作为命令式编程的基准）中的代码生成能力。结果表明，随着模型演进，LLM在函数式编程中的表现显著提升；然而，在纯函数式语言（Haskell和OCaml）中，错误率仍远高于混合式语言（Scala）或命令式语言（Java）。此外，LLM频繁生成不符合函数式编程习惯的代码，表现出明显的命令式编程模式，这引发了对代码风格和长期可维护性的担忧。\n\n最后，我们发现，当提供静态分析反馈并结合针对常见问题的手动设计指令时，LLM能够部分实现自我修复，有效改善代码的正确性和质量。"
  },
  {
    "date": "2026-01-05",
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "authors": "Jiawen Zhang, Lipeng He, Kejia Chen, Jian Lou, Jian Liu, Xiaohu Yang, Ruoxi Jia",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01887v1",
    "source": "arXiv",
    "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
    "title_zh": "一击制胜的安全性：通过单一实例修补微调后的大语言模型",
    "abstract_zh": "微调与安全对齐的大语言模型（LLMs）可能会显著损害其安全性。以往的方法通常需要大量安全样本或校准集，这不仅在重新对齐过程中带来巨大的计算开销，还导致模型效用明显下降。与此相反，我们发现仅需一个安全示例，即可完全恢复模型的安全对齐，且无需牺牲模型效用，成本极低。令人惊讶的是，这种修复效果不受微调阶段所使用有害示例数量或底层模型规模的影响，并且仅需几个训练周期即可实现收敛。此外，我们揭示了安全梯度的低秩结构，从而解释了为何如此高效的修正成为可能。我们在五个经过安全对齐的大语言模型及多个数据集上验证了这一发现，充分证明了该方法的普适性。"
  },
  {
    "date": "2026-01-05",
    "title": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers",
    "authors": "Shuoming Zhang, Jiacheng Zhao, Qiuchu Yu, Chunwei Xia, Zheng Wang, Xiaobing Feng, Huimin Cui",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02045v1",
    "source": "arXiv",
    "abstract": "This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.",
    "title_zh": "新的编译器堆栈：大语言模型与编译器协同作用的综述",
    "abstract_zh": "本调查系统地概述了LLM赋能编译这一新兴领域，通过回答若干关键研究问题，为该领域的发展提供了清晰的图景。我们首先解答了LLM如何被集成的问题，提出了一个全面且多维度的分类体系，将相关工作依据其设计哲学（选择器、翻译器、生成器）、LLM方法论、代码抽象层次以及所解决的具体任务类型进行分类。在探讨这些方法带来的进步时，我们识别出三大主要优势：编译器开发的民主化、新型优化策略的发现，以及编译器传统功能边界的拓展。最后，在分析该领域的挑战与机遇时，我们强调了确保正确性与实现可扩展性这两大核心障碍，并指出混合系统的发展是最具前景的前进方向。通过提供上述答案，本调查为研究人员和实践者构建了一条基础性的路线图，指引着新一代由LLM驱动、具备智能性、自适应性与协同性的编译工具的发展方向。"
  },
  {
    "date": "2026-01-05",
    "title": "AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI",
    "authors": "Runze Zheng, Yuqing Zheng, Zhengyi Cheng, Long Luo, Haoxiang Luo, Gang Sun, Hongfang Yu, Dusit Niyato",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02021v1",
    "source": "arXiv",
    "abstract": "The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI.",
    "title_zh": "AgentVNE：基于大模型增强的图强化学习在边缘智能代理AI中的亲和性感知多智能体部署应用",
    "abstract_zh": "智能体互联网正推动边缘计算向代理型人工智能（Agentic AI）和边缘通用智能（Edge General Intelligence, EGI）演进。然而，在资源受限的边缘基础设施上部署多智能体服务（Multi-Agent Service, MAS），面临严峻挑战。MAS 服务工作流由复杂的跨节点交互、动态内存累积以及协同工具使用驱动，呈现出链式拓扑依赖关系和严格的亲和性约束，对实时响应能力要求极高，远超传统虚拟网络嵌入（VNE）算法在静态资源环境下的处理能力。为应对这一问题，我们提出 AgentVNE——一种基于云边协同的双层架构框架。首先，AgentVNE 利用大语言模型（LLM）识别隐含语义约束，并生成基于亲和性的资源增强策略，以解决物理依赖难题；其次，构建一种具备资源相似性感知能力的神经网络，采用预训练与PPO微调相结合的策略，精准捕捉动态工作流与异构网络之间的拓扑相似性。通过融合语义感知与拓扑推理，该机制有效弥合了动态服务需求与物理基础设施之间的鸿沟。仿真结果表明，AgentVNE 将工作流通信延迟降低至基线水平的40%以下，并在高负载场景下将服务接纳率提升约5%-10%。本研究为代理型人工智能的语义感知部署提供了基础性解决方案。"
  },
  {
    "date": "2026-01-05",
    "title": "The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities",
    "authors": "Matteo Esposito, Andrea Janes, Valentina Lenarduzzi, Davide Taibi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01944v1",
    "source": "arXiv",
    "abstract": "In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored. We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.",
    "title_zh": "人工智能库的无形之手：塑造开源项目与社区",
    "abstract_zh": "20世纪80年代初，开源软件（OSS）作为一种革命性理念在专有软件主导的背景下应运而生。这一最初具有颠覆性的思想，如今已成为计算机科学的基石。在各类开源项目中，人工智能（AI）正日益显现其存在感与重要性。然而，尽管AI技术日益流行，其在开源项目中的应用及其影响仍鲜有深入研究。本文旨在评估Python和Java开源项目中AI库的采用情况，并探讨这些库如何塑造开发实践，包括技术生态和社区参与等方面。为此，我们将对15.77万个潜在的开源仓库进行大规模分析，结合仓库级指标与软件度量指标，对比采用AI库与未采用AI库的项目之间的差异。我们预期能够识别出在开发活跃度、社区参与度以及代码复杂性等方面存在的可衡量差异，从而为AI集成如何重塑软件开发实践提供基于证据的深刻见解。"
  },
  {
    "date": "2026-01-05",
    "title": "DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems",
    "authors": "Willem Röpke, Samuel Coward, Andrei Lupu, Thomas Foster, Tim Rocktäschel, Jakob Foerster",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01931v1",
    "source": "arXiv",
    "abstract": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.",
    "title_zh": "DéjàQ：多样化、可学习且可验证问题的开放性演化",
    "abstract_zh": "近年来，推理模型在数学和编程领域取得了令人瞩目的进展。然而，大多数方法依赖于静态数据集，这可能导致模型产生记忆现象，并限制其泛化能力。我们提出了DéjàQ框架，该框架突破了这一传统范式，通过在模型训练过程中协同演化多样化的合成数学问题，实现动态调整。这一进化过程会根据模型在训练中的表现进行自适应调整，从而优化问题的可学习性。我们提出了两种由大语言模型（LLM）驱动的变异策略：一种是通过改变上下文细节来修改问题，另一种则是直接对问题结构进行调整。研究发现，模型能够生成新颖且有意义的问题，而这些由LLM驱动的变异显著提升了强化学习（RL）训练的效果。我们还分析了DéjàQ的关键特性，包括生成问题的有效性以及计算开销。实验结果表明，动态演化训练数据在提升数学推理能力方面具有巨大潜力，并展现出广泛的应用前景。为此，我们将开源相关代码以支持进一步研究。"
  },
  {
    "date": "2026-01-05",
    "title": "A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach",
    "authors": "Mikel Robredo, Matteo Esposito, Fabio Palomba, Rafael Peñaloza, Valentina Lenarduzzi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01921v1",
    "source": "arXiv",
    "abstract": "Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest. Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect. Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect. Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.",
    "title_zh": "缺陷正在诞生：我们离目标还有多近？一种时间敏感的预测方法",
    "abstract_zh": "背景。缺陷预测一直是经验软件工程领域研究人员高度关注的主题。以往的研究已成功实现了对即将出现的缺陷最准确的预测，并通过即时预测方法识别出缺陷发生前的特征与异常。随着软件系统持续演化，迫切需要具备时间敏感性的方法，以在缺陷显现之前进行预测。  \n目标。本研究旨在探讨时间敏感性技术在缺陷预测中的有效性，同时探究缺陷发生前的早期征兆。  \n方法。我们将训练多种时间敏感的预测技术，用于预测软件项目的未来缺陷密度，并识别缺陷发生前的早期症状。  \n预期成果。我们的预期成果将转化为实证证据，证明所提出方法在早期评估缺陷易发性方面的有效性。"
  },
  {
    "date": "2026-01-05",
    "title": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems",
    "authors": "Nenad Petrovic, Vahid Zolfaghari, Fengjunjie Pan, Alois Knoll",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02215v1",
    "source": "arXiv",
    "abstract": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.",
    "title_zh": "基于大语言模型的汽车系统功能安全与安全设计",
    "abstract_zh": "本文提出了一种由大语言模型（LLM）赋能的工作流，以支持软件定义汽车（SDV）的软件开发，涵盖安全感知的系统拓扑设计以及事件驱动的决策代码分析。在代码分析方面，我们采用事件链模型，为功能安全的系统化验证提供了形式化基础，同时考虑了关键组件之间交换消息的语义有效性，包括CAN总线和车辆信号规范（VSS）。针对拓扑结构的安全性分析，则结合了模型驱动工程（MDE）方法与对象约束语言（OCL）规则，实现协同优化。评估过程中，综合考虑了本地部署及专有解决方案，在与高级驾驶辅助系统（ADAS）相关的应用场景中进行了验证。"
  },
  {
    "date": "2026-01-05",
    "title": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics",
    "authors": "Markus Borg, Nadim Hagatulah, Adam Tornhill, Emma Söderberg",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02200v1",
    "source": "arXiv",
    "abstract": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.",
    "title_zh": "机器的代码，而非仅人类的代码：通过代码健康度指标量化人工智能友好性",
    "abstract_zh": "我们正进入一个混合时代，人类开发者与AI编程代理共同在同一个代码库中协作。尽管行业实践长期以来一直致力于优化代码以提升人类的理解性，但如今确保不同能力的大型语言模型（LLM）能够可靠地编辑代码变得愈发重要。在本研究中，我们通过基于LLM的重构方法，对来自编程竞赛的5,000个Python文件数据集进行了分析，探讨了“AI友好型代码”这一概念。研究发现，CodeHealth——一个针对人类理解性校准的质量度量指标——与AI重构后的语义保持性之间存在显著关联。研究结果证实，对人类友好的代码同样也更适配AI工具。这些发现表明，组织可以利用CodeHealth来指导AI干预的风险区域：在低风险区域可放心使用AI，在高风险区域则需加强人工监督。投资于代码的可维护性不仅有助于人类开发者，也为大规模AI应用做好了准备。"
  },
  {
    "date": "2026-01-05",
    "title": "Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration",
    "authors": "Max Unterbusch, Andreas Vogelsang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01952v1",
    "source": "arXiv",
    "abstract": "Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a \"defect\" is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback.",
    "title_zh": "通过人与大语言模型协作实现上下文自适应的需求缺陷预测",
    "abstract_zh": "传统的自动化需求评估通常依赖于普遍模式作为缺陷性的代理指标，通过基于规则的启发式方法或在大规模标注数据集上训练的机器学习分类器来实现。然而，“缺陷”的定义本质上具有情境依赖性，会因项目、领域和利益相关者理解的不同而变化。本文提出一种人机大语言模型协作（Human-LLM Collaboration, HLC）方法，将缺陷预测视为一个动态适应过程，而非静态分类任务。HLC利用大语言模型的思维链（Chain-of-Thought）推理能力，在反馈循环中不断优化：用户对预测结果及其解释进行验证，这些经验证的示例通过少样本学习方式自适应地引导后续预测。我们在包含1,266个标注的梅赛德斯-奔驰需求的QuRE基准数据集上，针对“弱词异味”这一典型问题进行了评估。结果表明，HLC能够有效适应已验证示例的输入，仅需20个左右的验证样本即可迅速提升性能。与仅使用标签的传统少样本提示相比，结合已验证解释信息的HLC显著优于标准少样本提示和微调后的BERT模型，同时保持了较高的召回率。这些结果凸显了大语言模型在上下文学习与思维链推理方面的优势，使得自适应分类方法得以突破“一刀切”模型的局限，为能够持续从利益相关者反馈中学习的智能工具提供了新的可能性。"
  },
  {
    "date": "2026-01-05",
    "title": "A New Benchmark for the Appropriate Evaluation of RTL Code Optimization",
    "authors": "Yao Lu, Shang Liu, Hangan Zhou, Wenji Fang, Qijun Zhang, Zhiyao Xie",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01765v1",
    "source": "arXiv",
    "abstract": "The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.",
    "title_zh": "RTL代码优化恰当评估的新基准",
    "abstract_zh": "人工智能的快速发展越来越依赖于高效的集成电路（IC）设计。近期研究探索了利用大语言模型（LLMs）生成寄存器传输级（RTL）代码的可能性，但现有基准主要评估语法正确性，而未能充分衡量在功耗、性能和面积（PPA）方面的优化质量。本文提出 RTL-OPT，一个用于评估大语言模型在RTL优化能力方面的基准。RTL-OPT 包含 36 个手工设计的数字电路实例，涵盖组合逻辑、流水线数据通路、有限状态机以及存储器接口等多种实现类别。每个任务提供一对 RTL 代码：一个次优版本和一个人工优化的参考版本，后者体现了行业验证过的优化模式，这些模式无法通过传统综合工具捕捉。此外，RTL-OPT 集成了一套自动化评估框架，可验证功能正确性并量化 PPA 改进效果，从而为硬件设计优化生成模型提供标准化且有意义的评估手段。"
  },
  {
    "date": "2026-01-05",
    "title": "Context-Free Recognition with Transformers",
    "authors": "Selim Jerad, Anej Svete, Sophie Hao, Ryan Cotterell, William Merrill",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01754v1",
    "source": "arXiv",
    "abstract": "Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\\mathcal{O}(\\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\\mathcal{O}(\\log n)$ looping layers and $\\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.",
    "title_zh": "使用Transformer进行上下文无关的识别",
    "abstract_zh": "Transformer 模型在处理符合某种语法的结构化输入任务（如自然语言和代码）方面表现出色。然而，它们如何处理语法结构仍不明确。事实上，在标准复杂度假设下，标准 Transformer 无法识别上下文无关语言（CFLs），而 CFLs 是描述语法的一种经典形式化工具；甚至无法识别正则语言——作为 CFL 的一个子类（Merrill 等，2022）。Merrill & Sabharwal（2024）表明，具有 $\\mathcal{O}(\\log n)$ 层循环（相对于输入长度 $n$）的 Transformer 可以识别正则语言，但关于上下文无关语言的识别问题仍悬而未决。在本文中，我们证明：具有 $\\mathcal{O}(\\log n)$ 循环层和 $\\mathcal{O}(n^6)$ 填充标记（padding tokens）的循环 Transformer 能够识别所有上下文无关语言。然而，使用 $\\mathcal{O}(n^6)$ 填充标记进行训练和推理在实际中可能不可行。幸运的是，我们进一步发现，对于诸如无歧义上下文无关语言等自然子类，Transformer 上的识别问题变得更具可操作性，仅需 $\\mathcal{O}(n^3)$ 的填充标记。我们通过实验验证了上述结果，并展示了在理论上需要对数深度的语言上，循环机制确实能带来性能提升。总体而言，我们的研究揭示了 Transformer 识别上下文无关语言的内在复杂性：尽管一般情况下的识别可能需要难以承受的大量填充，但在自然约束（如无歧义性）下，可以实现高效且实用的识别算法。"
  },
  {
    "date": "2026-01-05",
    "title": "AI Agent Systems: Architectures, Applications, and Evaluation",
    "authors": "Bin Xu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01743v1",
    "source": "arXiv",
    "abstract": "AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.",
    "title_zh": "人工智能代理系统：架构、应用与评估",
    "abstract_zh": "AI代理——将基础模型与推理、规划、记忆及工具使用相结合的系统——正迅速成为自然语言意图与现实世界计算之间的一种实用接口。本综述综合了AI代理架构领域的新兴发展，涵盖三个方面：(i) 深思熟虑与推理（如思维链式分解、自我反思与验证，以及考虑约束条件的决策）；(ii) 规划与控制（从反应式策略到分层和多步规划器）；(iii) 工具调用与环境交互（包括信息检索、代码执行、API调用及多模态感知）。我们以前人工作为基础，构建了一个统一的分类体系，涵盖代理组件（策略/大语言模型核心、记忆模块、世界模型、规划器、工具路由器和评判者）、协调模式（单代理与多代理；集中式与分布式协调）以及部署场景（离线分析与在线交互式辅助；安全关键任务与开放性任务）。我们讨论了关键的设计权衡——延迟与准确性、自主性与可控性、能力与可靠性之间的平衡——并指出评估过程因非确定性、长周期信用分配、工具与环境的多样性，以及隐藏成本（如重试次数和上下文增长）而变得复杂。最后，我们总结了测量与基准测试实践（任务套件、人类偏好与效用指标、约束条件下的成功率、鲁棒性与安全性），并指出了若干开放挑战，包括对工具操作的验证与防护机制、可扩展的记忆与上下文管理、代理决策的可解释性，以及在真实负载下实现可复现的评估。"
  },
  {
    "date": "2026-01-05",
    "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
    "authors": "Dasol Choi, DongGeon Lee, Brigitta Jesica Kartono, Helena Berndt, Taeyoun Kwon, Joonwon Jang, Haon Park, Hwanjo Yu, Minsuk Kahng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01836v1",
    "source": "arXiv",
    "abstract": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
    "title_zh": "COMPASS：评估大语言模型中组织特定政策对齐的框架",
    "abstract_zh": "随着大型语言模型在医疗、金融等高风险企业应用中的广泛部署，确保其遵守组织特定政策已变得至关重要。然而，现有的安全评估仅关注普遍性危害，忽视了组织层面的合规要求。本文提出COMPASS（公司/组织政策对齐评估），这是首个系统性评估大语言模型是否符合组织允许列表（allowlist）和禁止列表（denylist）政策的框架。我们将COMPASS应用于八个不同行业场景，生成并验证了5,920个测试查询，通过精心设计的边缘案例，全面检验模型在常规合规性和对抗鲁棒性方面的表现。对七种前沿大模型的评估揭示出一个根本性的不对称现象：模型在处理合法请求时表现可靠（准确率超过95%），但在执行禁止性规则方面却严重失效——对于对抗性违禁行为，仅能拒绝13%至40%的违规请求。这些结果表明，当前的大语言模型尚不具备政策关键型部署所需的稳健性，凸显了COMPASS作为组织级人工智能安全评估不可或缺工具的重要价值。"
  },
  {
    "date": "2026-01-05",
    "title": "MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines",
    "authors": "Berke Ates, Filip Dobrosavljević, Theodoros Theodoridis, Zhendong Su",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02218v1",
    "source": "arXiv",
    "abstract": "Compilers are essential for the performance and correct execution of software and hold universal relevance across various scientific disciplines. Despite this, there is a notable lack of tools for testing and evaluating them, especially within the adaptable Multi-Level Intermediate Representation (MLIR) context. This paper addresses the need for a tool that can accommodate MLIR's extensibility, a feature not provided by previous methods such as Csmith. Here we introduce MLIR-Smith, a novel random program generator specifically designed to test and evaluate MLIR-based compiler optimizations. We demonstrate the utility of MLIR-Smith by conducting differential testing on MLIR, LLVM, DaCe, and DCIR, which led to the discovery of multiple bugs in these compiler pipelines. The introduction of MLIR-Smith not only fills a void in the realm of compiler testing but also emphasizes the importance of comprehensive testing within these systems. By providing a tool that can generate random MLIR programs, this paper enhances our ability to evaluate and improve compilers and paves the way for future tools, potentially shaping the wider landscape of software testing and quality assurance.",
    "title_zh": "MLIR-Smith：一种用于评估编译器流水线的新型随机程序生成器",
    "abstract_zh": "编译器对于软件的性能和正确执行至关重要，并在各个科学领域具有普遍的应用价值。然而，目前缺乏针对编译器进行测试与评估的工具，尤其是在可扩展的多级中间表示（MLIR）环境中尤为明显。本文提出了一种能够适应MLIR可扩展性的新工具，弥补了以往方法（如Csmith）的不足。我们介绍了MLIR-Smith，这是一种专为测试和评估基于MLIR的编译器优化而设计的新型随机程序生成器。通过使用MLIR-Smith对MLIR、LLVM、DaCe和DCIR进行差异性测试，成功发现了这些编译器流水线中的多个缺陷。MLIR-Smith的引入不仅填补了编译器测试领域的空白，更凸显了全面测试在这些系统中的重要性。通过提供一种能够生成随机MLIR程序的工具，本文显著提升了我们评估和改进编译器的能力，并为未来相关工具的发展铺平了道路，有望深刻影响更广泛的软件测试与质量保证领域。"
  },
  {
    "date": "2026-01-05",
    "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG",
    "authors": "Jingyu Liu, Jiaen Lin, Yong Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01896v1",
    "source": "arXiv",
    "abstract": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.",
    "title_zh": "解决RAG中噪声过滤的固有难题",
    "abstract_zh": "检索增强生成（Retrieval-Augmented Generation, RAG）已成为一种广泛采用的方法，通过引入外部知识来增强大语言模型（LLM）的能力，同时减少幻觉现象。然而，在RAG过程中，常常会引入噪声或不相关的文档，这可能降低模型性能，甚至导致生成幻觉内容。尽管已有多种方法被提出以过滤此类噪声，但我们认为，从检索到的内容中识别无关信息本质上具有挑战性，且有限数量的Transformer层难以彻底解决这一问题。因此，检索器无法完全剔除无关文档。由此，大语言模型必须具备对这类噪声的鲁棒性，但我们的实验表明，传统的微调方法往往无法有效使模型在处理信息时选择性地利用相关部分，而忽略无关内容，其根源在于注意力机制的结构限制。为应对这一挑战，我们提出了一种新颖的微调方法，旨在提升模型在检索文档中区分相关信息与无关信息的能力。在多个基准测试上的大量实验结果表明，该方法显著增强了大语言模型的鲁棒性和整体性能。"
  },
  {
    "date": "2026-01-05",
    "title": "K-EXAONE Technical Report",
    "authors": "Eunbi Choi, Kibong Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Hyunjik Jo, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Haeju Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Heuiyeen Yeen, Hwan Chang, Stanley Jungkyu Choi, Yejin Choi, Jiwon Ham, Kijeong Jeon, Geunyeong Jeong, Gerrard Jeongwon Jo, Yonghwan Jo, Jiyeon Jung, Naeun Kang, Dohoon Kim, Euisoon Kim, Hayeon Kim, Hyosang Kim, Hyunseo Kim, Jieun Kim, Minu Kim, Myoungshin Kim, Unsol Kim, Youchul Kim, YoungJin Kim, Chaeeun Lee, Chaeyoon Lee, Changhun Lee, Dahm Lee, Edward Hwayoung Lee, Honglak Lee, Jinsang Lee, Jiyoung Lee, Sangeun Lee, Seungwon Lim, Solji Lim, Woohyung Lim, Chanwoo Moon, Jaewoo Park, Jinho Park, Yongmin Park, Hyerin Seo, Wooseok Seo, Yongwoo Song, Sejong Yang, Sihoon Yang, Chang En Yea, Sihyuk Yi, Chansik Yoon, Dongkeun Yoon, Sangyeon Yoon, Hyeongu Yun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.01739v1",
    "source": "arXiv",
    "abstract": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
    "title_zh": "K-EXAONE 技术报告",
    "abstract_zh": "本技术报告介绍了由LG AI Research开发的大规模多语言语言模型K-EXAONE。K-EXAONE基于专家混合（Mixture-of-Experts）架构，总参数量达2360亿，在推理过程中激活230亿参数。该模型支持长达256,000个标记的上下文窗口，涵盖韩语、英语、西班牙语、德语、日语和越南语六种语言。我们在一个涵盖推理、代理能力、通用能力、韩语特化以及多语言能力的综合性基准测试套件上对K-EXAONE进行了评估。在各项评测中，K-EXAONE的表现与同规模的开源模型相当。作为一款旨在推动人工智能造福人类的先进专有基础模型，K-EXAONE被定位为适用于广泛工业与科研应用的强大AI基础模型。"
  },
  {
    "date": "2026-01-05",
    "title": "NQC2: A Non-Intrusive QEMU Code Coverage Plugin",
    "authors": "Nils Bosbach, Alwalid Salama, Lukas Jünger, Mark Burton, Niko Zurstraßen, Rebecca Pelke, Rainer Leupers",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02238v1",
    "source": "arXiv",
    "abstract": "Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems. To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.",
    "title_zh": "NQC2：一种非侵入式的QEMU代码覆盖率插件",
    "abstract_zh": "代码覆盖率分析已成为软件开发中的标准方法，有助于评估测试套件的有效性、识别未充分测试的代码段，并发现性能瓶颈。当需要测量嵌入式系统软件的代码覆盖率时，传统方法很快就会遇到局限。一种常用的方法是在源代码中插入额外代码，以在运行时收集并导出覆盖率信息。然而，这些插入的代码通常依赖于操作系统的存在以及文件系统的支持来保存数据，而裸机程序在嵌入式系统上运行时并不具备这些特性。为解决这一问题，我们提出了NQC2，这是一个针对QEMU的插件。NQC2可在运行时从QEMU中提取覆盖率信息，并将其存储到主机上的文件中。该方法甚至兼容经过修改的QEMU版本，且无需对目标软件进行代码注入。与Xilinx提供的类似方案相比，NQC2的性能最高可提升8.5倍。"
  },
  {
    "date": "2026-01-05",
    "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
    "authors": "Yingte Shu, Yuchuan Tian, Chao Xu, Yunhe Wang, Hanting Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02076v1",
    "source": "arXiv",
    "abstract": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
    "title_zh": "基于置信度感知滑动窗口的扩散语言模型延迟承诺解码",
    "abstract_zh": "扩散语言模型（DLMs）最近作为自回归模型的一种有力替代方案出现，能够实现并行文本生成。为了提升推理效率并保持KV缓存的兼容性，先前的工作通常采用基于块的扩散策略，即逐块解码生成 tokens。然而，这种范式存在一个结构性缺陷，我们称之为“边界诱导的上下文截断”（Boundary-Induced Context Truncation, BICT）：在块边界附近尚未解码的 tokens 被迫提前确定，即使此时它们本可以借助邻近的未来上下文信息显著降低不确定性。这一限制降低了解码置信度，损害了生成质量，尤其在需要精确推理的任务中表现明显，如数学问题求解和代码生成。\n\n为此，我们提出了一种全新的、无需训练的解码策略——**延迟承诺解码**（Deferred Commitment Decoding, DCD）。DCD 通过维护一个基于置信度的滑动窗口来处理被掩码的 tokens，对不确定性较低的 tokens 早期进行解码，而将不确定性较高的 tokens 延迟解码，直到获得足够的上下文证据为止。该设计实现了解码窗口内的有效双向信息流动，同时不牺牲推理效率。\n\n在多个扩散语言模型、基准测试和缓存配置下的大量实验表明，与固定块状扩散方法相比，DCD 在平均性能上提升了 1.39% 的生成准确率，且耗时相当；在某些场景下，提升幅度最高可达 9.0%。这些结果表明，基于不确定性延迟 token 承诺是一种简单却极为有效的原则，能够显著提升扩散语言模型解码的质量与效率。"
  },
  {
    "date": "2026-01-05",
    "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
    "authors": "Zhuofan Shi, Hubao A, Yufei Shao, Mengyan Dai, Yadong Yu, Pan Xiang, Dongliang Huang, Hongxu An, Chunxiao Xin, Haiyang Shen, Zhenyu Wang, Yunshan Na, Gang Huang, Xiang Jing",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02075v1",
    "source": "arXiv",
    "abstract": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
    "title_zh": "MDAgent2：面向分子动力学代码生成与知识问答的大型语言模型",
    "abstract_zh": "分子动力学（MD）模拟在材料科学中对于理解原子尺度行为至关重要，然而编写LAMMPS脚本仍是一项高度专业化且耗时的任务。尽管大语言模型（LLMs）在代码生成和领域特定问题回答方面展现出巨大潜力，但在MD场景中的表现受限于领域数据稀缺、先进LLMs部署成本高昂以及代码可执行性低等问题。基于我们先前的MDAgent工作，本文提出MDAgent2，这是首个能够实现MD领域内知识问答与代码生成的端到端框架。我们构建了一个面向领域的数据构建流程，生成了三个高质量数据集，分别涵盖MD知识、问答任务和代码生成。基于这些数据集，我们采用三阶段后训练策略——持续预训练（CPT）、监督微调（SFT）和强化学习（RL），训练出两个领域适配的模型：MD-Instruct与MD-Code。此外，我们提出了MD-GRPO，一种闭环强化学习方法，利用模拟结果作为奖励信号，并将低奖励轨迹回收用于持续优化。我们还构建了MDAgent2-RUNTIME，一个可部署的多智能体系统，集成代码生成、执行、评估与自我修正功能。结合本文提出的MD-EvalBench——首个针对LAMMPS代码生成与问答任务的基准测试体系，我们的模型与系统在性能上超越多个强基线。本研究系统地展示了大语言模型在工业仿真任务中的适应性与泛化能力，为“人工智能+科学”及工业级仿真的自动化代码生成奠定了方法论基础。  \nURL: https://github.com/FredericVAN/PKU_MDAgent2"
  },
  {
    "date": "2026-1-5",
    "title": "A Deep Learning Model for Predicting Transformation Legality",
    "authors": "Avani Tiwari, Yacine Hakimi, Riyadh Baghdadi",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315253",
    "source": "IEEE",
    "abstract": "Compilers must check the legality of code transformations to guarantee the correctness of applying a sequence of code transformations to a given code. While such a legality check needs to be precisely computed in general, we can use an approximate legality prediction model in certain cases, such as training a reinforcement learning (RL) agent for schedule prediction. In this paper, we propose an approximate method for legality checks. We propose a novel DL model for predicting the legality of transformations. The model takes the code representation and a list of transformations as input and predicts whether applying those transformations to the code is legal. We implement and evaluate the proposed model, demonstrating its effectiveness. Our evaluation shows an F1 score of 0.91 on a test set of randomly generated programs. To further evaluate the model in a practical scenario, we used the model to replace the legality check used during the training of an RL agent designed for automatic code optimization. We demonstrate that such a replacement enables the agent to train on twice as many steps, resulting in faster training and reducing resource usage by approximately $80 \\%$ for CPU and 35% for RAM. The agent trained using this approach maintains comparable performance, with only a $4 \\%$ reduction on benchmarks from the Polybench suite compared to the traditional method.",
    "title_zh": "一种用于预测转换合法性的深度学习模型",
    "abstract_zh": "编译器必须检查代码变换的合法性，以确保对给定代码应用一系列代码变换的正确性。虽然在一般情况下，这种合法性检查需要精确计算，但在某些场景下，我们可采用近似合法性预测模型，例如在训练用于调度预测的强化学习（RL）代理时。本文提出了一种近似合法性检查的方法。我们设计了一种新型深度学习（DL）模型，用于预测代码变换的合法性。该模型以代码表示和一组变换列表作为输入，预测将这些变换应用于代码是否合法。我们实现了该模型并进行了评估，证明了其有效性。评估结果显示，在随机生成程序的测试集上，该模型的F1得分为0.91。为进一步验证该模型在实际场景中的表现，我们将其用于替代一个专为自动代码优化设计的强化学习代理在训练过程中所使用的传统合法性检查机制。实验表明，这种替换使代理能够训练两倍于以往的步数，从而实现更快的训练速度，并使CPU资源使用量减少约80%，内存使用量减少35%。采用该方法训练出的代理在性能上与传统方法相比仅在Polybench基准测试中出现约4%的下降，整体表现相当。"
  },
  {
    "date": "2026-1-5",
    "title": "DetectorsForge: A Software Product Line for Transfer-learning in Code Smells Detection",
    "authors": "Syrine Wardi, Rania Mzid, Tewfik Ziadi",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315279",
    "source": "IEEE",
    "abstract": "Code smells are indicators of potential design issues in source code that can affect the overall software quality. Detecting these smells has been the focus of extensive research efforts using both traditional metric-based techniques and, more recently, deep learning models. Among these, Large Language Models (LLMs) have emerged as powerful tools due to their strong capabilities in code understanding and classification tasks. LLMs such as CodeT5 and CodeBERT were employed to detect various smells with promising results. However the integration of these models into software engineering workflows remains challenging as current approaches often lack traceability, modularity, and reusability, making it difficult to track results, compare configurations, or systematically explore model variations. To address these limitations, we introduce in this paper DetectorsForge, a Software Product Line (SPL) that enables the systematic configuration, customization, and execution of LLM-based code smell detectors. DetectorsForge unifies models, datasets, transfer learning techniques, and evaluation methods to adapt pre-trained models to new downstream tasks. These components are integrated within a reusable architecture that supports the automated derivation of code smell detection variants. Experimental results demonstrate the applicability of our approach for generating various code smell detectors.",
    "title_zh": "DetectorsForge：一种用于代码异味检测的迁移学习软件产品线",
    "abstract_zh": "代码异味是源代码中潜在设计问题的指示信号，可能影响软件的整体质量。检测这些异味一直是研究的重点，研究人员曾采用传统的基于度量的方法，以及近年来兴起的深度学习模型。在这些方法中，大型语言模型（LLMs）因其在代码理解与分类任务中的强大能力而崭露头角。例如，CodeT5 和 CodeBERT 等模型已被用于检测多种代码异味，并取得了令人鼓舞的结果。然而，将这些模型集成到软件工程工作流中仍面临挑战，因为现有方法往往缺乏可追溯性、模块性和可重用性，导致难以追踪结果、比较配置或系统化地探索模型变体。为解决上述局限，本文提出 DetectorsForge——一个软件产品线（SPL），能够系统地配置、定制和执行基于大语言模型的代码异味检测器。DetectorsForge 将模型、数据集、迁移学习技术与评估方法统一整合，以适应预训练模型在新下游任务中的应用。这些组件被集成在一个可重用的架构中，支持代码异味检测器变体的自动化生成。实验结果表明，该方法在生成多种代码异味检测器方面具有良好的适用性。"
  },
  {
    "date": "2026-1-5",
    "title": "Attack Classification Using Retrieval-Augmented Generation and Large Language Models",
    "authors": "Ivan Kawaminami, Mohammad Wali Ur Rahman, Jin Bai, Salim Hariri",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315325",
    "source": "IEEE",
    "abstract": "The exponential rise in security alerts across modern information systems presents a critical challenge for cybersecurity operations, frequently overwhelming analysts and hindering timely incident response. To address this, we propose an automated threat classification framework that leverages RetrievalAugmented Generation (RAG) in combination with Large Language Models (LLMs) to accurately interpret and categorize attack types. By combining real-time data ingestion with contextual retrieval and advanced natural language understanding, the system efficiently interprets alert data and categorizes threats to support incident response and decision-making. Experimental evaluations show that the proposed approach achieves a classification accuracy of 94%, demonstrating strong promise for realworld deployment in security operations centers. These results highlight the potential of integrating RAG-LLM based frameworks to significantly enhance the scalability and effectiveness of cybersecurity defenses.",
    "title_zh": "基于检索增强生成和大语言模型的攻击分类",
    "abstract_zh": "现代信息系统中安全告警的指数级增长给网络安全运营带来了严峻挑战，常常使分析人员不堪重负，影响及时的事件响应。为应对这一问题，我们提出了一种基于检索增强生成（RAG）与大语言模型（LLM）相结合的自动化威胁分类框架，能够准确解读并分类攻击类型。该系统通过实时数据接入、上下文检索以及先进的自然语言理解能力，高效解析告警信息，并对威胁进行分类，从而支持事件响应与决策制定。实验评估表明，所提方法的分类准确率达到94%，展现出在安全运营中心实际部署中的巨大潜力。这些结果凸显了将基于RAG-LLM的框架融入网络安全防御体系，可显著提升防御措施的可扩展性与有效性。"
  },
  {
    "date": "2026-1-5",
    "title": "WARD: Efficient Memory Protection for WebAssembly on Tiny Embedded Systems",
    "authors": "Chaewon Shin, Gowon Han, Suhyeon Song, Jinsun Park, Yoon-Ho Choi, Donghyun Kwon",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3650447",
    "source": "IEEE",
    "abstract": "WebAssembly (Wasm) has recently gained attention in embedded systems due to its lightweight runtime support and broad compatibility with various languages and platforms. However, Wasm’s linear memory model lacks internal protection mechanisms, making it vulnerable to a wide range of memory safety violations. To address this, prior work has proposed techniques for protecting memory within the linear memory space. While effective to some extent, these approaches either suffer from limited detection coverage or are unsuitable for resource-constrained embedded systems. In this paper, we present WARD, a redzone-based memory protection technique tailored for Wasm in tiny embedded systems. At its core, WARD employs a <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">randomized linear-to-physical address translation mechanism</i>, which enables effective detection of memory violations without requiring specific hardware extensions. Our evaluation shows that WARD provides improved memory detection coverage over prior techniques, with reasonable performance of 24.2% on benchmark and 13.03% on real-world applications, and memory overhead suitable for embedded systems.",
    "title_zh": "WARD：面向小型嵌入式系统的WebAssembly高效内存保护",
    "abstract_zh": "WebAssembly（Wasm）因其轻量级运行时支持以及对多种语言和平台的广泛兼容性，近年来在嵌入式系统领域受到广泛关注。然而，Wasm 的线性内存模型缺乏内部保护机制，容易遭受各类内存安全违规攻击。为应对这一问题，已有研究提出了一些针对线性内存空间的内存保护技术。尽管这些方法在一定程度上有效，但普遍存在检测覆盖范围有限或不适用于资源受限的嵌入式系统的缺陷。本文提出 WARD——一种专为小型嵌入式系统中的 WebAssembly 设计的基于红区（redzone）的内存保护技术。WARD 的核心在于采用一种**随机化的线性地址到物理地址映射机制**，能够在无需特定硬件扩展的情况下，实现对内存越界的高效检测。评估结果表明，与先前方法相比，WARD 在内存错误检测覆盖率方面有显著提升，在基准测试中性能开销仅为 24.2%，在真实应用中为 13.03%，同时其内存开销也适合嵌入式系统部署。"
  },
  {
    "date": "2026-1-5",
    "title": "Automatic programming via large language models with population self-evolution for dynamic fuzzy job shop scheduling problem",
    "authors": "Jin Huang, Qihao Liu, Xinyu Li, Liang Gao, Yue Teng",
    "publish": "IEEE Transactions on Fuzzy Systems",
    "url": "https://doi.org/10.1109/tfuzz.2025.3650586",
    "source": "IEEE",
    "abstract": "Heuristic dispatching rules (HDRs) are widely used for solving the dynamic fuzzy job shop scheduling problem (DFJSSP). However, their performance is highly sensitive to specific scenarios and often necessitates expert customization. To overcome this, automated design methods like genetic programming (GP) and gene expression programming (GEP) have been proposed. Despite their success, these methods face challenges, such as high randomness in the search process. Recently, the combination of large language models (LLMs) with evolutionary algorithms has opened new possibilities for prompt engineering and automated algorithm design. To improve the ability of LLMs in automatic HDR design, this paper introduces a novel population self-evolutionary (SeEvo) framework, which draws inspiration from the self-reflective design strategies employed by human experts. Notably, this framework employs a novel teacher-student learning mechanism, allowing the LLM (student) to generate robust HDRs. Guided by a teacher model with complete knowledge of actual processing times, the student learns to infer fuzzy uncertainties from historical deviations, enabling it to effectively anticipate and adapt to fuzzy impacts. Experimental results demonstrate that SeEvo significantly outperforms GP, GEP, deep reinforcement learning (DRL) methods, and more than ten commonly used HDRs from the literature, particularly in previously unseen and dynamic scenarios.",
    "title_zh": "基于种群自进化的大规模语言模型自动编程求解动态模糊作业车间调度问题",
    "abstract_zh": "启发式调度规则（HDRs）被广泛应用于求解动态模糊作业车间调度问题（DFJSSP）。然而，其性能对具体场景高度敏感，通常需要专家进行定制化调整。为克服这一局限，自动化设计方法如遗传编程（GP）和基因表达编程（GEP）已被提出。尽管这些方法取得了一定成功，但仍面临搜索过程随机性过高、难以稳定收敛等挑战。近年来，大型语言模型（LLMs）与进化算法的结合为提示工程和自动化算法设计开辟了新途径。为了提升LLMs在自动HDR设计中的能力，本文提出一种新颖的种群自演化（SeEvo）框架，该框架受到人类专家自我反思式设计策略的启发。特别地，该框架引入了一种创新的师生学习机制：由具备完整实际加工时间知识的教师模型指导，LLM（学生）能够从历史偏差中学习并推断模糊不确定性，从而有效预测和适应模糊影响。实验结果表明，SeEvo在以往未见过且动态变化的场景中显著优于GP、GEP、深度强化学习（DRL）方法以及文献中十余种常用HDRs，展现出卓越的泛化能力和鲁棒性。"
  },
  {
    "date": "2026-1-5",
    "title": "Towards a Driven Specific Modeling Language for Deep Neural Networks",
    "authors": "Angham Boukhari, Ahmed Hadj Kacem, Faiza Belala, Aïcha Choutri",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315261",
    "source": "IEEE",
    "abstract": "Deep Neural Networks (DNN) have recently driven a revolution in various fields of Artificial Intelligence (AI), especially in Deep Learning (DL). However, they remain difficult to design, implement, verify, adapt, and reuse. To address these challenges, this paper introduces DNN-SL, a Domain-Specific Modeling Language (DSML) designed specifically for modeling and verifying DNN architectures. Our approach combines Model-Driven Engineering (MDE) with formal methods to define a multi-layer framework consisting of a DSML definition layer, an MDE implementation layer, and a Maude-based formal semantics layer. This architecture enables DNN models to automatically be translated into formal Maude specifications, thus facilitating the verification of properties including robustness and interpretability. By combining intuitive modeling with formal rigor, our solution simplifies the development of reliable and accessible DL, even for non-experts.",
    "title_zh": "面向深度神经网络的驱动式特定领域建模语言",
    "abstract_zh": "深度神经网络（DNN）近年来在人工智能（AI）的各个领域，尤其是在深度学习（DL）中，推动了一场革命。然而，它们在设计、实现、验证、适应和重用方面仍然面临诸多挑战。为应对这些难题，本文提出了一种名为 DNN-SL 的领域特定建模语言（DSML），专门用于建模与验证 DNN 架构。我们的方法将模型驱动工程（MDE）与形式化方法相结合，构建了一个多层次框架，包括：DSML定义层、MDE实现层以及基于 Maude 的形式语义层。该架构能够将 DNN 模型自动转换为形式化的 Maude 规范，从而支持对鲁棒性、可解释性等关键属性的验证。通过将直观的建模方式与严格的形式化保障相结合，我们的解决方案显著简化了可靠且易于理解的深度学习系统的开发过程，即使对于非专家用户也具备良好的可访问性。"
  },
  {
    "date": "2026-1-5",
    "title": "Research on Generative Language Models Aiding Project Quantification Methods",
    "authors": "Fengxi Gao",
    "publish": "Proceedings of the 2025 3rd International Conference on Mathematics and Machine Learning",
    "url": "https://doi.org/10.1145/3783779.3783832",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "生成式语言模型辅助项目量化方法研究",
    "abstract_zh": "None"
  },
  {
    "date": "2026-1-5",
    "title": "A Data-driven Analysis of Code Optimizations",
    "authors": "Yacine Hakimi, Riyadh Baghdadi",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315210",
    "source": "IEEE",
    "abstract": "As the demand for computational power grows, optimizing code through compilers becomes increasingly crucial. In this context, we focus on fully automatic code optimization techniques that automate the process of selecting and applying code transformations for better performance without manual intervention. Understanding how these transformations behave and interact is key to designing more effective optimization strategies. Compiler developers must make numerous design choices when constructing these heuristics. For instance, they may decide whether to allow transformations to be explored in any arbitrary order or to enforce a fixed sequence. While the former may theoretically offer the best performance gains, it significantly increases the search space. This raises an important question: Can a predefined, fixed order of applying transformations speed up the search without severely compromising optimization potential? In this paper, we address this and other related questions that arise in the design of automatic code optimization algorithms. Using a data-driven approach, we generate a large dataset of random programs, apply random optimization sequences, and record their execution times. Through statistical analysis, we provide insights that guide the development of more efficient automatic code optimization algorithms.",
    "title_zh": "基于数据的代码优化分析",
    "abstract_zh": "随着计算需求的不断增长，通过编译器优化代码变得愈发重要。在此背景下，我们专注于完全自动化的代码优化技术，这些技术能够自动选择并应用代码变换以提升性能，而无需人工干预。理解这些变换的行为及其相互作用，是设计更高效优化策略的关键。编译器开发者在构建这些启发式规则时需做出诸多设计决策。例如，他们可能需要决定是否允许变换以任意顺序进行探索，还是强制采用固定的执行序列。虽然前者在理论上可能带来最佳的性能提升，但会显著增加搜索空间。这引出一个关键问题：预先设定的、固定的变换应用顺序是否能在不严重损害优化潜力的前提下加速搜索过程？本文针对这一问题以及自动代码优化算法设计中出现的其他相关问题展开研究。我们采用数据驱动的方法，生成大量随机程序，对它们应用随机的优化序列，并记录其执行时间。通过统计分析，我们提供了有助于开发更高效自动代码优化算法的深刻见解。"
  },
  {
    "date": "2026-1-5",
    "title": "Type Region Model and Its Application in Program Static Detection",
    "authors": "Xuejian Li, Linlin Wei",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3650540",
    "source": "IEEE",
    "abstract": "High-assurance software has become a key goal in modern software development, especially in the design of embedded kernel systems and other fields where high reliability and security are paramount. However, programming languages such as C, characterized by pointer usage, introduce complex variable relationships such as pointer aliasing. Furthermore, the support for arbitrary type conversions in C complicates program analysis, particularly in adequately characterizing the diverse relationships among variables and handling type conversion issues during program execution. To address these challenges, this paper introduces a type region model for the formal description of variable and value information. The model constructs pointer and heap memory object representations to characterize the program states of relevant variables. Based on predefined vulnerability detection rules, it subsequently detects potential defects arising from pointers, memory operations, and type conversions. The proposed static detection methodwas evaluated using the Juliet C/C++ 1.3 test suite. The experimental results demonstrate that this method achieves a favorable balance between precision and false positive rates, making it suitable for defect detection in programs of moderate complexity.",
    "title_zh": "类型区域模型及其在程序静态检测中的应用",
    "abstract_zh": "高保障软件已成为现代软件开发中的关键目标，尤其在嵌入式内核系统等对高可靠性与安全性要求极高的领域中更为重要。然而，C语言等编程语言由于广泛使用指针，引入了复杂的变量关系，如指针别名问题。此外，C语言中对任意类型转换的支持也增加了程序分析的复杂性，尤其是在准确刻画变量间的多种关系以及处理程序执行过程中的类型转换问题方面。为应对这些挑战，本文提出一种用于形式化描述变量与值信息的类型区域模型。该模型构建了指针和堆内存对象的表示形式，以刻画相关变量的程序状态。基于预定义的漏洞检测规则，该模型可进一步检测由指针、内存操作及类型转换引发的潜在缺陷。所提出的静态检测方法通过Juliet C/C++ 1.3测试套件进行了评估。实验结果表明，该方法在精度与误报率之间取得了良好的平衡，适用于中等复杂度程序的缺陷检测。"
  },
  {
    "date": "2026-1-5",
    "title": "Special Issue on Generative AI for Edge Computing",
    "authors": "Mi Zhang, Xia Hu",
    "publish": "IEEE Internet Computing",
    "url": "https://doi.org/10.1109/mic.2025.3630305",
    "source": "IEEE",
    "abstract": null,
    "title_zh": "边缘计算生成式人工智能专题",
    "abstract_zh": "None"
  },
  {
    "date": "2026-1-5",
    "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing",
    "authors": "Youssef Maklad, Fares Wael, Ali Hamdi, Wael Elsersy, Khaled Shaban",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315267",
    "source": "IEEE",
    "abstract": "Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications. This paper introduces MultiFuzz, a novel dense retrieval-based multi-agent system designed to overcome these limitations by integrating semantic-aware context retrieval, specialized agents, and structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of protocol documentation (RFC Documents) to build embeddings in a vector database for a retrieval-augmented generation (RAG) pipeline, enabling agents to generate more reliable and structured outputs, enhancing the fuzzer in mutating protocol messages with enhanced state coverage and adherence to syntactic constraints. The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynamically adapt fuzzing strategies based on the retrieved contextual knowledge. Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions over state-of-the-art (SOTA) fuzzers such as NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic coordination, and language model reasoning, MultiFuzz establishes a new paradigm in autonomous protocol fuzzing, offering a scalable and extensible foundation for future research in intelligent agentic-based fuzzing systems.",
    "title_zh": "MultiFuzz：一种基于密集检索的多智能体网络协议模糊测试系统",
    "abstract_zh": "传统的协议模糊测试技术，如基于AFL的系统所采用的方法，往往因对复杂协议语法结构的语义理解有限以及种子变异策略僵化而效果不佳。近期的研究工作（如ChatAFL）引入了大型语言模型（LLMs）来指导协议模糊测试，以应对上述挑战，推动模糊测试器在更广泛的协议状态空间中进行探索。然而，ChatAFL仍存在输出不可靠、LLM幻觉以及假设LLM具备协议规范知识等问题。本文提出MultiFuzz，一种基于密集检索的多智能体系统，旨在克服这些局限性。该系统通过融合语义感知的上下文检索、专用智能体以及结构化工具辅助推理，显著提升了模糊测试的可靠性与效率。MultiFuzz利用协议文档（RFC文档）中的智能体片段构建向量嵌入，并存入向量数据库，形成检索增强生成（RAG）流程，使智能体能够生成更加可靠且结构化的输出，从而在变异协议消息时实现更高的状态覆盖率和更强的语法约束遵循能力。该框架将模糊测试过程分解为多个模块化的智能体组，各智能体通过思维链（chain-of-thought）推理协同工作，根据检索到的上下文知识动态调整模糊测试策略。在实时流媒体协议（RTSP）上的实验评估表明，相较于当前最先进的模糊测试工具（如NSFuzz、AFLNet和ChatAFL），MultiFuzz显著提升了分支覆盖率，并能深入探索更复杂的协议状态及状态转换。通过结合密集检索、智能体协作与语言模型推理，MultiFuzz建立了一种自主协议模糊测试的新范式，为未来基于智能体的智能化模糊测试系统研究提供了可扩展、可拓展的坚实基础。"
  },
  {
    "date": "2026-1-5",
    "title": "A Comprehensive Comparison of LLaMA 3.1 and Traditional ML Approaches in Automated Vulnerability Detection",
    "authors": "Ahmed Sajwani, Fatima AlKaabi, Amr Tamer, Mohamed Yaqoob, Panos Liatsis",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315404",
    "source": "IEEE",
    "abstract": "Software vulnerability detection has traditionally relied on classical machine learning (ML) models with handcrafted features, which are lightweight but limited in performance. Transformer-based Large Language Models (LLMs) trained on source code provide deeper semantic understanding and stronger generalization. This work presents a direct comparison between ML- and LLM-based approaches across binary and multi-label tasks. We benchmark Random Forest, SVM, and LSTM against LoRA-finetuned LLaMA 3.1 models on both synthetic and real-world datasets. Our LLaMA 3.18 B model achieves $87 \\%$ macro $F 1$ in binary classification and $82 \\%$ in multi-label classification on real-world data-substantially outperforming ML and DL baselines. Despite higher computational demands, parameter-efficient fine-tuning with Unsloth and LoRA makes LLMs viable for practical deployment. These findings highlight the superiority of LLMs in security-critical code analysis and establish strong baselines for future research.",
    "title_zh": "LLaMA 3.1 与传统机器学习方法在自动化漏洞检测中的全面比较",
    "abstract_zh": "软件漏洞检测传统上依赖于基于手工特征的经典机器学习（ML）模型，这类模型轻量但性能有限。而基于Transformer的大型语言模型（LLM）在源代码数据上进行训练后，能够提供更深层次的语义理解与更强的泛化能力。本文首次对基于ML与基于LLM的方法在二分类和多标签任务中进行了直接对比。我们在合成数据集和真实世界数据集上，对随机森林、支持向量机（SVM）和LSTM模型与经过LoRA微调的LLaMA 3.1模型进行了基准测试。实验结果表明，我们的LLaMA 3.1 8B模型在真实世界数据上的二分类任务中达到了87%的宏平均F1分数，在多标签分类任务中达到82%，显著优于传统的ML与深度学习基线模型。尽管LLM具有更高的计算开销，但通过Unsloth与LoRA实现的参数高效微调，使得LLM在实际部署中具备可行性。这些发现凸显了LLM在安全关键型代码分析中的优越性，并为未来的研究建立了强有力的基准。"
  },
  {
    "date": "2026-1-5",
    "title": "Scalable Network-on-Chip Design for FPGA Implementation",
    "authors": "Mekala Bindu Bhargavi, Sai Siddharth Rokkam, Vattipelli Srinath, Sri Parameswaran, J Soumya",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2026.3650973",
    "source": "IEEE",
    "abstract": "Network-on-Chip (NoC) technology has become a fundamental solution to address communication bottlenecks and scalability challenges in modern System-on-Chip (SoC) architectures. This paper presents a fully automated and scalable NoC design framework capable of generating synthesizable Verilog code for diverse topologies, including Mesh, Torus, Butterfly Fat Tree (BFT), and Custom configurations. The framework, based on a wormhole router architecture, automatically configures interconnections, routing paths, and network parameters according to user specifications, eliminating manual intervention and reducing design time errors. The generated NoC architectures were synthesized and verified on a Xilinx Virtex-7 Field Programmable Gate Array (FPGA) using Vivado, demonstrating design adaptability and efficiency. Experimental results show up to ∼ 12, 500 speed-up for Mesh, ∼ 11, 500 for Torus, ∼ 10, 800 for BFT, and ∼ 10, 200 for Custom topologies compared with manual approaches. The proposed design was evaluated for throughput, latency, packet delivery ratio (PDR), and energy per packet. Results show that the BFT topology delivers the highest PDR (98.2%) and the lowest energy consumption (1.80 nJ), confirming the framework scalability. Overall, the AutoNoC framework offers a reconfigurable, topology-independent, and FPGA-compatible solution for scalable SoC communication.",
    "title_zh": "面向FPGA实现的可扩展片上网络设计",
    "abstract_zh": "片上网络（NoC）技术已成为解决现代片上系统（SoC）架构中通信瓶颈与可扩展性挑战的根本性解决方案。本文提出了一种完全自动化且可扩展的NoC设计框架，能够为多种拓扑结构（包括Mesh、Torus、蝴蝶胖树（BFT）以及自定义配置）生成可综合的Verilog代码。该框架基于虫洞路由器架构，可根据用户需求自动配置互连方式、路由路径及网络参数，无需人工干预，显著降低了设计时间并减少了出错概率。所生成的NoC架构在Xilinx Virtex-7现场可编程门阵列（FPGA）上通过Vivado工具完成综合与验证，充分展示了其设计的适应性与高效性。实验结果表明，与传统手动设计方法相比，Mesh拓扑最高可实现约12,500倍的加速，Torus拓扑达约11,500倍，BFT拓扑约10,800倍，自定义拓扑约10,200倍。对所提设计进行了吞吐量、延迟、分组投递率（PDR）及每分组能耗的评估。结果表明，BFT拓扑具有最高的分组投递率（98.2%）和最低的能耗（1.80 nJ），进一步验证了该框架的可扩展性。总体而言，AutoNoC框架提供了一种可重构、拓扑无关且适用于FPGA的高效解决方案，适用于大规模SoC通信系统的设计。"
  },
  {
    "date": "2026-1-5",
    "title": "Modeling and Mitigating Reentrancy Attacks: A Decision-Theoretic Framework for Smart Contract Security",
    "authors": "Arnab Mallick, Indraveni Chebolu",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3650603",
    "source": "IEEE",
    "abstract": "Reentrancy remains one of the most persistent and damaging vulnerabilities in Ethereum smart contracts, enabling adversaries to recursively drain funds despite the presence of static and runtime defenses. Existing studies mainly focus on detection or program analysis, but they do not explain why and under what conditions attackers decide to exploit. In this work, we introduce a decision-theoretic framework that models reentrancy as a profit-maximizing problem under gas, risk, and atomicity constraints. Our framework derives the conditions under which reentrancy attacks are economically viable and provides an algorithm for computing optimal exploit strategies. We further extend the analysis to multi-contract attacks, capturing sequential, parallel, and optimized execution strategies. A simulation environment evaluates profitability across varying balances and gas configurations, highlighting thresholds where attacks become infeasible. Finally, we translate attacker decision profiles into practical defense recommendations for developers, auditors, and DeFi system designers. This framework bridges the gap between exploit detection and adversarial economics, offering a rigorous basis for strengthening smart contract security. This framework establishes a theoretical baseline for adversarial economics in smart contract security, forming a foundation for future MEV-aware exploitability models and Layer-2 risk analysis.",
    "title_zh": "建模与缓解重入攻击：一种智能合约安全的决策理论框架",
    "abstract_zh": "重入攻击仍然是以太坊智能合约中最持久且危害最严重的漏洞之一，即使在存在静态和运行时防御机制的情况下，攻击者仍能通过递归调用手段持续抽干资金。现有研究主要集中在漏洞检测或程序分析上，但未能解释攻击者为何以及在何种条件下选择发动攻击。本文提出一种基于决策理论的框架，将重入攻击建模为在Gas消耗、风险和原子性约束下的利润最大化问题。该框架推导出重入攻击在经济上可行的条件，并提供了一种计算最优攻击策略的算法。我们进一步将分析扩展至多合约攻击场景，涵盖顺序执行、并行执行及优化执行等多种策略。通过仿真环境对不同余额和Gas配置下的攻击盈利能力进行评估，揭示了攻击变得不可行的关键阈值。最后，我们将攻击者的决策特征转化为开发人员、审计人员以及DeFi系统设计者可操作的防御建议。该框架弥合了漏洞检测与对抗性经济之间的鸿沟，为提升智能合约安全性提供了严谨的理论基础。同时，该框架为智能合约安全中的对抗性经济学建立了理论基准，为未来面向MEV（最大可提取价值）的可利用性模型及Layer-2风险分析奠定了坚实基础。"
  },
  {
    "date": "2026-1-5",
    "title": "Steer Your Model: Secure Code Generation with Contrastive Decoding",
    "authors": "Li Huang, Meng Yan, Tao Yin, Weifeng Sun, Zhongxin Liu, Hongyu Zhang, David Lo",
    "publish": "IEEE Transactions on Software Engineering",
    "url": "https://doi.org/10.1109/tse.2025.3650127",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) specialized in code have demonstrated impressive capabilities in various programming tasks such as code generation. However, these models often generate vulnerable code due to inherent flaws in training datasets derived from large-scale, unfiltered open-source repositories. Existing methods like SVEN (prefix tuning) and CoSec (supervised co-decoding) attempt to address these risks but face challenges with transferability or inflexible security constraints. To mitigate these issues, we propose SCoDE, a two-stage approach for secure and functionally correct code generation. After an initial functional tuning phase, we integrate a plug-andplay security steering matrix at the model’s output embedding layer. This matrix can be transferred across models without modifying their original weights. During inference, we introduce a novel contrastive decoding mechanism that adaptively balances the base model’s functional logits with positive and negative security steering signals. Extensive experiments on 60 security scenarios and two standard benchmarks (HumanEval, MBPP) using StarCoder, Qwen2.5-Coder, and CodeLlama demonstrate that SCoDE enhances security while maintaining functional correctness. On average, SCoDE improves security by 28.09% over the original models, 12.07% over CoSec, and 4.82% over SVEN. For functional correctness, it achieves average gains of 55.03% on HumanEval and 41.81% on MBPP over the original models.",
    "title_zh": "引导你的模型：通过对比解码实现安全代码生成",
    "abstract_zh": "专注于代码的大型语言模型（LLMs）在代码生成等各类编程任务中展现了令人瞩目的能力。然而，由于训练数据源自大规模、未经筛选的开源代码库，这些模型常常生成存在安全漏洞的代码。现有的方法如SVEN（前缀调优）和CoSec（监督联合解码）虽试图缓解此类风险，但在可迁移性或安全约束灵活性方面仍面临挑战。为解决这些问题，我们提出SCoDE——一种两阶段的安全且功能正确的代码生成方法。在初始的功能调优阶段之后，我们在模型输出嵌入层引入一个即插即用的安全引导矩阵，该矩阵可跨模型迁移而无需修改原始权重。推理阶段，我们设计了一种新颖的对比解码机制，能够自适应地平衡基础模型的功能得分与正负向安全引导信号。在60个安全场景以及两个标准基准（HumanEval、MBPP）上，基于StarCoder、Qwen2.5-Coder和CodeLlama的大量实验表明，SCoDE在保持功能正确性的同时显著提升了安全性。平均而言，相较于原始模型，SCoDE的安全性提升达28.09%；相比CoSec提升12.07%，相比SVEN提升4.82%。在功能正确性方面，SCoDE在HumanEval上平均提升55.03%，在MBPP上平均提升41.81%。"
  },
  {
    "date": "2026-1-5",
    "title": "Benchmarking Deep Learning Models for Detecting SSRF Vulnerabilities: A Comparative Study",
    "authors": "Jacqueline Mukamisha, Aline Iradukunda, Elyse Manzi, Jema David Ndibwile",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315477",
    "source": "IEEE",
    "abstract": "Server-Side Request Forgery (SSRF) has emerged as a critical security threat in cloud-native web applications. Existing static and rule-based defenses are often ineffective due to obfuscated payloads and rapid evolution of attack vectors. In this paper, we present a comparative evaluation of four deep learning models—LSTM, BiLSTM, GRU, and BERT—for SSRF detection. Using the CIC-URL2016 dataset, we benchmark model accuracy, training efficiency, and generalization via 5-fold cross-validation. The GRU model achieved the highest average accuracy of 99.83%. We analyze model performance, compare learning dynamics, and discuss implications for integration into lightweight security systems. Furthermore, we acknowledge that the CIC-URL2016 dataset does not fully capture modern SSRF attack vectors, and we propose evaluating our models on contemporary datasets and real-world exploit scenarios in future work.",
    "title_zh": "深度学习模型检测SSRF漏洞的基准测试：一项比较研究",
    "abstract_zh": "服务器端请求伪造（SSRF）已成为云原生Web应用中一项关键的安全威胁。现有的静态规则型防御机制往往因攻击载荷的混淆以及攻击向量的快速演变而效果不佳。本文对四种深度学习模型——LSTM、BiLSTM、GRU和BERT在SSRF检测中的表现进行了对比评估。基于CIC-URL2016数据集，我们通过五折交叉验证对各模型的准确率、训练效率及泛化能力进行了基准测试。结果显示，GRU模型取得了最高的平均准确率，达到99.83%。本文深入分析了各模型的性能表现，比较了其学习动态特性，并探讨了其在轻量级安全系统中集成的可行性。此外，我们指出CIC-URL2016数据集未能充分涵盖现代SSRF攻击向量，因此建议在未来工作中将所提出的模型在更先进的数据集及真实世界漏洞利用场景中进行评估。"
  },
  {
    "date": "2026-1-5",
    "title": "Bridging Language Models and Formal Methods for Intent-Driven Optical Network Design",
    "authors": "Anis Bekri, Amar Abane, Abdella Battou, Saddek Bensalem",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315283",
    "source": "IEEE",
    "abstract": "Intent-Based Networking (IBN) aims to simplify network management by enabling users to specify high-level goals that drive automated network design and configuration. However, translating informal natural-language intents into formally correct optical network topologies remains challenging due to inherent ambiguity and lack of rigor in Large Language Models (LLMs). To address this, we propose a novel hybrid pipeline that integrates LLM-based intent parsing, formal methods, and Optical Retrieval-Augmented Generation (RAG). By enriching design decisions with domain-specific optical standards and systematically incorporating symbolic reasoning and verification techniques, our pipeline generates explainable, verifiable, and trustworthy optical network designs. This approach significantly advances IBN by ensuring reliability and correctness, essential for mission-critical networking tasks.",
    "title_zh": "面向意图的光网络设计中语言模型与形式化方法的融合",
    "abstract_zh": "基于意图的网络（Intent-Based Networking, IBN）旨在通过允许用户指定高层次目标来简化网络管理，从而驱动网络设计与配置的自动化。然而，由于大型语言模型（LLMs）在表达上存在固有的模糊性与缺乏严谨性，将非正式的自然语言意图转化为形式正确的光网络拓扑结构仍面临挑战。为解决这一问题，我们提出一种创新的混合式流水线，该流水线融合了基于大语言模型的意图解析、形式化方法以及光学增强型检索生成（Optical Retrieval-Augmented Generation, RAG）。通过引入领域特定的光学标准来丰富设计决策，并系统性地结合符号推理与验证技术，我们的流水线能够生成可解释、可验证且可信的光网络设计方案。该方法显著提升了IBN的可靠性与正确性，使其在关键任务型网络应用中更具实用性与保障性。"
  },
  {
    "date": "2026-1-5",
    "title": "Deep Learning Framework Testing via Model Mutation: How Far Are We?",
    "authors": "Yanzhou Mu, Rong Wang, Juan Zhai, Chunrong Fang, Xiang Chen, Peiran Yang, Zhixiang Cao, Ruixiang Qian, Shaoyu Yang, Zhenyu Chen",
    "publish": "IEEE Transactions on Software Engineering",
    "url": "https://doi.org/10.1109/tse.2026.3651022",
    "source": "IEEE",
    "abstract": "Deep Learning (DL) frameworks are fundamental components of DL systems in their development, deployment, and execution, while defects in DL frameworks can cause severe consequences. Ensuring the quality of DL frameworks has therefore become a pressing challenge. Among the various testing techniques, model mutation has emerged as a widely adopted approach. Such methods generate mutants by applying mutation operators to DL models (e.g., structural changes or parameter edits) and then analyzing inconsistencies, crashes, or abnormal behaviors across different frameworks or hardware. Despite its effectiveness, existing methods suffer from the following limitations. First, they mainly reuse operators designed for model testing, raising doubts about their ability to expose framework-level defects. Besides, they insufficiently consider mutation constraints, such as mutation type, position, and order, which directly affect the defect detection ability of generated mutants. Finally, they rely on the limited detection range and narrow test oracles, focusing on functional correctness in model inference while overlooking defects in efficiency, resource usage, and other defects that developers care about in other stages, such as model training or deployment. These limitations result in a weak alignment with the critical defects that developers are most concerned about in practice. Motivated by these observations, this study conducts a comprehensive investigation into the effectiveness of existing mutation-based testing methods. We first collect and classify defect reports from PyTorch and MindSpore according to developers’ priority tags, building a taxonomy of seven categories and 19 sub-categories of HP defects. We then map the defects reported by five state-of-the-art methods into this taxonomy to evaluate their detection abilities. To explain these limitations, we further analyze how three key factors, mutation type, mutation position, and mutation order, affect the generated mutants. Based on the experiment results, we summarize ten findings ranging from revealing the priority of developers on fixing framework defects, evaluating the defect detection ability of existing methods, to how mutation factors affect the generated mutants. Furthermore, we reveal four limitations and their root causes of existing methods and propose four targeted optimization strategies. We further apply these strategies to COMET and successfully uncover six new defects spanning four types, including two previously unreported categories. Overall, our study identifies 38 unique framework defects, of which 30 are confirmed by developers and 12 have been fixed, demonstrating the practical value of our findings.",
    "title_zh": "基于模型变异的深度学习框架测试：我们走到了哪一步？",
    "abstract_zh": "深度学习（DL）框架是深度学习系统在开发、部署和执行过程中的核心组成部分，而DL框架中的缺陷可能引发严重后果。因此，确保DL框架的质量已成为一项紧迫挑战。在各类测试技术中，模型变异（model mutation）已发展为一种广泛应用的方法。该方法通过在深度学习模型上应用变异算子（如结构变更或参数修改）生成变异体，并分析不同框架或硬件环境下出现的不一致、崩溃或异常行为。尽管该方法具有一定的有效性，但现有方法仍存在以下局限性：首先，它们主要复用为模型测试设计的算子，难以有效暴露框架层面的缺陷；其次，对变异约束（如变异类型、位置和顺序）考虑不足，而这些因素直接影响生成变异体的缺陷检测能力；最后，现有方法依赖有限的检测范围和狭窄的测试断言，仅关注模型推理的功能正确性，忽视了开发者在模型训练、部署等其他阶段关心的效率、资源使用等方面的缺陷。\n\n上述局限性导致现有方法与开发者实际最关注的关键缺陷之间存在较弱的对齐。基于此，本研究对现有基于变异的测试方法的有效性进行了全面调查。我们首先收集并根据开发者的优先级标签对PyTorch和MindSpore的缺陷报告进行分类，构建了一个包含七类19个子类的高优先级（HP）缺陷分类体系。随后，我们将五种前沿方法报告的缺陷映射到该分类体系中，评估其缺陷发现能力。为进一步解释这些局限性，我们深入分析了三个关键因素——变异类型、变异位置和变异顺序——如何影响生成的变异体。基于实验结果，我们总结出十项重要发现，涵盖开发者修复框架缺陷的优先级、现有方法的缺陷检测能力，以及变异因素对变异体生成的影响。此外，我们揭示了现有方法存在的四项根本性局限及其成因，并提出了四项针对性优化策略。我们将这些策略应用于COMET工具，成功发现了六项新缺陷，覆盖四种类型，其中包括两个此前未被报告的类别。总体而言，本研究共识别出38个独特的框架缺陷，其中30个已由开发者确认，12个已被修复，充分体现了本研究发现的实际价值。"
  },
  {
    "date": "2026-1-5",
    "title": "CHOP: Clustered Hybrid Optimization for Logic Synthesis with Self-Supervised Prediction",
    "authors": "Rongliang Fu, Ran Zhang, Ziyang Zheng, Zhengyuan Shi, Yuan Pu, Junying Huang, Bei Yu, Qiang Xu, Tsung-Yi Ho",
    "publish": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "url": "https://doi.org/10.1109/tcad.2026.3650860",
    "source": "IEEE",
    "abstract": "Hybrid optimization is emerging as a promising approach for enhancing logic synthesis by combining multiple logic optimization methods. This paper introduces CHOP, a novel framework that leverages vertex-level feature extraction and optimization-aware clustering to achieve significant improvements in the efficiency and quality of logic synthesis. Unlike existing cluster-level approaches that follow a “partition-then-predict” paradigm, CHOP operates under a “predict-then-partition” approach where the optimization method most appropriate for a given circuit vertex is determined by analyzing its local structural and functional context. To capitalize on this insight, CHOP extracts a subgraph centered on each vertex and calculates its circuit cost, which serves as a vertex feature for the subsequent circuit partitioning process. Furthermore, we introduce a self-supervised prediction model to obtain these vertex features efficiently. Following circuit partitioning, CHOP determines the optimization method for each partition and then iteratively applies these methods to derive the optimized circuit. The experimental results from logic optimization and LUT mapping tasks on the EPFL benchmark demonstrate the effectiveness and efficiency of CHOP.",
    "title_zh": "CHOP：基于自监督预测的逻辑综合聚类混合优化方法",
    "abstract_zh": "混合优化正逐渐成为提升逻辑综合效率的一种有前景的方法，通过结合多种逻辑优化技术来实现。本文提出了一种名为CHOP的新框架，该框架利用顶点级特征提取与优化感知聚类，显著提升了逻辑综合的效率和质量。与现有基于簇级别的方法所采用的“先划分后预测”范式不同，CHOP采用“先预测后划分”的策略，通过分析电路中每个顶点的局部结构和功能上下文，确定最适合该顶点的优化方法。为充分利用这一洞察，CHOP以每个顶点为中心提取子图，并计算其电路代价，作为后续电路划分过程中的顶点特征。此外，我们引入了一种自监督预测模型，以高效获取这些顶点特征。在完成电路划分后，CHOP为每个划分区域确定相应的优化方法，并迭代应用这些方法以生成最终优化后的电路。在EPFL基准测试集上的逻辑优化与LUT映射实验结果表明，CHOP在性能和效率方面均表现出显著优势。"
  },
  {
    "date": "2026-1-5",
    "title": "Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study",
    "authors": "Ibuki Nakamura, Yutaro Kashiwa, Bin Lin, Hajimu Iida",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3786791",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "测试代码中自认技术债的理解：一项实证研究",
    "abstract_zh": "None"
  },
  {
    "date": "2026-1-5",
    "title": "CAKD: A Confidence-Aware Knowledge Distillation Approach for Building Compact and Efficient LLMs",
    "authors": "Mohammad Basheer Kotit, Omama Hamad, Khaled Shaban, Ali Hamdi",
    "publish": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
    "url": "https://doi.org/10.1109/aiccsa66935.2025.11315280",
    "source": "IEEE",
    "abstract": "High-quality models across various natural language processing tasks, such as summarization and chatbots, often rely on large architectures, making them computationally intensive and challenging to deploy in resource-constrained environments. While knowledge distillation enables smaller student models to approximate the performance of larger teacher models, existing methods frequently encounter significant trade-offs between accuracy and efficiency. Additionally, uncertain predictions from teacher models can negatively impact the student’s learning process. In this paper, we introduce CAKD, a novel approach that optimizes the training of student models by selectively emphasizing the teacher model’s most reliable predictions using confidence scores. By integrating entropybased confidence weighting into the distillation loss, CAKD effectively prioritizes high-confidence samples, resulting in improved performance and efficiency. Our experiments on text summarization (using a BART-based model on the CNN/DM dataset) and chatbot tasks (using Llamabased model on the DailyDialog and PersonaChat datasets) demonstrate that CAKD achieves significant performance gains over larger teacher models, with improvements of 10.53, 2.1 and 0.38 ROUGE-L points respectively.",
    "title_zh": "CAKD：一种自信感知的知识蒸馏方法，用于构建紧凑高效的大型语言模型",
    "abstract_zh": "在各种自然语言处理任务（如文本摘要和聊天机器人）中，高质量的模型通常依赖于庞大的架构，这导致其计算开销大，难以在资源受限的环境中部署。尽管知识蒸馏技术能够让小型学生模型逼近大型教师模型的性能，但现有方法往往在准确率与效率之间面临显著权衡。此外，教师模型产生的不确定预测会负面影响学生模型的学习过程。本文提出了一种名为CAKD的新方法，通过利用置信度分数有选择性地强调教师模型最可靠的预测，优化学生模型的训练过程。CAKD将基于熵的置信度加权机制融入蒸馏损失函数，有效优先处理高置信度样本，从而提升模型性能与效率。我们在文本摘要任务（基于BART模型在CNN/DM数据集上）和聊天机器人任务（基于Llama模型在DailyDialog和PersonaChat数据集上）上的实验表明，CAKD在性能上显著超越了更大的教师模型，分别实现了10.53、2.1和0.38的ROUGE-L指标提升。"
  }
]