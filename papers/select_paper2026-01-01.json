[
  {
    "date": "2026-1-1",
    "title": "Hardware Design and Side-Channel Security Analysis on the Key Computational Block for YOLOv11",
    "authors": "Runquan Shao, Liji Wu, Jing Hu, Le Wu, Xiangmin Zhang",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316014",
    "source": "IEEE",
    "abstract": "YOLO (You Only Look Once) algorithm plays a crucial role in autonomous vehicles and medical imaging, which require a large number of object detection and image classification tasks because of its advantages of speed and accuracy on edge device. However, due to the intrinsic high computational workload of YOLO model, it is still challenging when implementing large models on the embedded device, especially the latest version YOLOv11. In this paper, we propose the hardware design based on time-division multiplexing of the key computational block C3k2 which is most used in YOLOv11 on FPGA. Firstly, we extend a fast convolution (Conv) algorithm to our Conv layer. Secondly, the batch normalization (BN) layer has been significantly optimized, especially in the stage of calculating the mean and variance. Finally, a Sigmoid Linear Unit (SiLU) activation function in the form of a piecewise function will appear completely in the form of time-division multiplexing, which will greatly reduce the consumption of resources. Experimental results show that our design can achieve a 0.19 ms delay on an AMD artix-7 xc7a35tftg256 FPGA under the working frequency of 50 MHz, while the average relative error is 1.67%. Furthermore, we use the cw305 board, which can extract power information, to implement a simple power analysis (SPA) on C3k2 block and the SiLU. The information within the hardware design was successfully analyzed, such as the number of runs and input data information.",
    "title_zh": "YOLOv11关键计算模块的硬件设计与侧信道安全分析",
    "abstract_zh": "YOLO（You Only Look Once）算法在自动驾驶和医学影像等领域中发挥着至关重要的作用，这些领域需要大量进行目标检测与图像分类任务，而YOLO凭借其在边缘设备上兼具速度与精度的优势，成为理想选择。然而，由于YOLO模型本身具有较高的计算负载，将其大型版本部署在嵌入式设备上仍面临挑战，尤其是最新的YOLOv11版本。本文提出了一种基于FPGA的时间分复用硬件设计方法，针对YOLOv11中使用最频繁的核心计算模块C3k2进行优化。首先，我们对卷积（Conv）层引入并扩展了一种快速卷积算法；其次，对批归一化（BN）层进行了显著优化，特别是在均值与方差的计算阶段；最后，将Sigmoid线性单元（SiLU）激活函数以分段函数的形式完全通过时间分复用实现，大幅降低了资源消耗。实验结果表明，在50 MHz工作频率下，本设计在AMD Artix-7 XC7A35TFTG256 FPGA上可实现0.19 ms的延迟，平均相对误差仅为1.67%。此外，我们利用支持功耗信息提取的CW305开发板，对C3k2模块及SiLU部分实施了简单的功耗分析（SPA），成功获取了硬件设计中的关键信息，如运行次数和输入数据特征等。"
  },
  {
    "date": "2026-1-1",
    "title": "Low-Power MCU Architecture Optimization and Energy Efficiency Enhancement in Intelligent Pressure Sensor SoCs",
    "authors": "Yaoming Lv, Hong Yang, Feng Zou, Yuhua Cheng",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11315921",
    "source": "IEEE",
    "abstract": "In intelligent pressure sensor System-on-Chip (SoC) applications, the micro-controller unit (MCU) serves as a critical control and computational core, with its power consumption directly impacting system energy efficiency and battery life. These MCUs typically operate in a periodic wake-up and short-term operation mode within such systems. Reducing MCU active time by maximizing operating frequency is essential for power savings. This study proposes a dynamic adaptive calibration technique for maximum frequency, which adjusts system frequency in real time by monitoring MCU instruction fetch correctness. Unlike conventional voltage-tracking-based maximum frequency calibration, this approach eliminates voltage detection circuits and voltage-frequency mapping tables while mitigating aging effects. In low-power designs, clock control strategy plays an important role in optimizing power consumption. This paper proposes an instruction-decoding-driven fine-grained dynamic clock control technique. Compared to conventional approaches, this method enables instruction-level clock gating to achieve micro-operation-level power optimization. Experimental validation in intelligent sensor systems demonstrates that the proposed low-power technique enhances system energy efficiency by 16% under typical operating conditions.",
    "title_zh": "智能压力传感器SoC中低功耗MCU架构优化与能效提升",
    "abstract_zh": "在智能压力传感器系统级芯片（SoC）应用中，微控制器单元（MCU）作为关键的控制与计算核心，其功耗直接影响系统的能效和电池寿命。这类MCU通常在周期性唤醒与短时运行模式下工作。通过最大化工作频率来减少MCU的活跃时间，是实现节能的关键。本文提出一种动态自适应校准技术，用于确定最大工作频率，该技术通过实时监测MCU指令获取的正确性来动态调整系统频率。与传统的基于电压跟踪的最大频率校准方法不同，该方法无需电压检测电路和电压-频率映射表，同时有效缓解了器件老化带来的影响。在低功耗设计中，时钟控制策略对优化功耗起着至关重要的作用。本文进一步提出一种基于指令解码驱动的细粒度动态时钟控制技术，相比传统方法，该方案实现了指令级的时钟门控，从而达到微操作级别的功耗优化。在智能传感器系统中的实验验证表明，所提出的低功耗技术在典型工作条件下可使系统能效提升16%。"
  },
  {
    "date": "2026-1-1",
    "title": "A Node Merging Algorithm Using Fault-Detection for XOR-Majority Network",
    "authors": "Shijia Fan, Feifei Deng",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316049",
    "source": "IEEE",
    "abstract": "Recent advancements in emerging technology have spurred the development of novel electronic devices, such as Quantum-dot Cellular Automata (QCA) as a transistoralternative computing architecture, and Magnetic RandomAccess Memory (MRAM) as a next-generation storage medium. These technologies enable efficient implementations of XOR and majority logic, which offer superior expressiveness compared to traditional AND/OR logic. As a result, researchers have introduced new logical structures MIGs and XMGs, which pose more demands for digital logic synthesis. In this paper, we propose a node merging algorithm using fault detect model for XMGs logic synthesis, including building an optimal majority node and XOR node model using the path sensitivity method from single stuck-at fault model testing, activating and propagating faults among majority node and XOR nodes in the XMGs, and using ROBDD tree to compute and store the mandatory assignment sets for efficient storage. The algorithm is implemented in the C++ language under the framework of the EPFL logic synthesis tool. The experiment results show that the proposed algorithm can effectively reduce the depth and number of nodes of XMGs circuits when applied to the standard test circuit in EPFL, with an average optimization depth of 2.07% and an average optimization number of 0.91% in the 2-level verification, and can gain better optimization effect when the window sizes increase.",
    "title_zh": "一种基于故障检测的XOR-多数网络节点合并算法",
    "abstract_zh": "近年来，新兴技术的快速发展推动了新型电子器件的开发，例如量子点细胞自动机（QCA）作为一种晶体管替代型计算架构，以及磁性随机存取存储器（MRAM）作为下一代存储介质。这些技术能够高效实现XOR和多数逻辑，其表达能力优于传统的AND/OR逻辑。因此，研究人员提出了新的逻辑结构——MIGs和XMGs，这对数字逻辑综合提出了更高的要求。本文提出一种基于故障检测模型的节点合并算法，用于XMGs的逻辑综合，包括利用单 stuck-at 故障模型测试中的路径敏感方法构建最优的多数节点与XOR节点模型，激活并传播XMGs中多数节点与XOR节点之间的故障，并采用ROBDD树来计算和存储必要的赋值集合，以实现高效存储。该算法在EPFL逻辑综合工具框架下使用C++语言实现。实验结果表明，将该算法应用于EPFL标准测试电路时，能够有效降低XMGs电路的深度和节点数量，在两级验证中平均深度优化达2.07%，平均节点数优化达0.91%；且随着窗口尺寸的增大，优化效果更为显著。"
  },
  {
    "date": "2026-1-1",
    "title": "Replicated Partitioning for Hypergraphs with Multiple Constraints",
    "authors": "Kexin Zhang, Shunyang Bi, Jing Tang, Hailong You, Qiwang Chen",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316012",
    "source": "IEEE",
    "abstract": "Nowadays, with the explosive growth of circuit design complexity and the limited capacity of a single FPGA, the Multi-FPGA System (MFS) has become the primary solution to tackle the challenges brought by large-scale designs. Therefore, designing a suitable partition for MFS to reduce the total_hop has become a key research question affecting the overall system performance. This paper first combines replication and movement operations to optimizes the total hop by replicating appropriate gates. The experimental results show that our method can reduce the total hop by an average of 43%, effectively improving the partitioning performance.",
    "title_zh": "具有多重约束的超图复制分区",
    "abstract_zh": "如今，随着电路设计复杂度的爆炸式增长以及单个FPGA容量的有限性，多FPGA系统（MFS）已成为应对大规模设计挑战的主要解决方案。因此，设计一种合适的MFS划分方案以减少总跳数（total_hop），已成为影响整体系统性能的关键研究问题。本文首次结合复制与迁移操作，通过适当复制逻辑门来优化总跳数。实验结果表明，所提出的方法平均可将总跳数降低43%，显著提升了划分性能。"
  },
  {
    "date": "2026-1-1",
    "title": "GeNetE - Generative Network Environments: A Specialized Small Language Model",
    "authors": "Sebastian Burfeid, Paulo H. L. Rettore, Philipp Zißner, Peter Sevenich, Clayson Celes, Bruno P. Santos",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310543",
    "source": "IEEE",
    "abstract": "Network emulation is crucial for testing and validating network systems, especially in the military domain involving devices from multiple network vendors, interfaces, and components in a federated mission. However, constructing the network infrastructure generally requires specialized expertise and a time-consuming setup process. Although Large Language Models (LLMs) can assist in code generation, general-purpose models often lack the domain-specific precision required for network emulators. This article introduces GeNetE, a Generative Network Environments framework, for creating a Specialized Small Language Model (SSLM) by fine-tuning open-weight LLMs using synthetic, domain-specific datasets. GeNetE enables streamlined emulator use, runs efficiently on low-resource devices, and reduces user workload. GeNetE was extensively evaluated using the Mininet emulator, employing various model sizes and quantization techniques, achieving approximately 60% valid Software-Defined Network environment generation with a compact 2.4 GB model, underscoring its potential for accessible and efficient network testing.",
    "title_zh": "GeNetE - 生成式网络环境：一种专用的小型语言模型",
    "abstract_zh": "网络仿真对于测试和验证网络系统至关重要，尤其是在涉及多个网络供应商设备、接口及组件的军事联合任务领域。然而，构建网络基础设施通常需要专业知识，并且设置过程耗时较长。尽管大型语言模型（LLMs）能够辅助代码生成，但通用模型往往缺乏网络仿真所需的领域特定精确性。本文介绍了GeNetE——一种生成式网络环境框架，通过使用合成的领域特定数据集对开源权重的大型语言模型进行微调，创建专用的小型语言模型（SSLM）。GeNetE实现了仿真器使用的简化，能够在资源受限的设备上高效运行，并显著降低用户工作负担。通过对Mininet仿真器进行广泛评估，采用不同模型规模和量化技术，GeNetE在仅2.4 GB的紧凑模型下实现了约60%的有效软件定义网络环境生成率，充分展示了其在实现可访问且高效的网络测试方面的巨大潜力。"
  },
  {
    "date": "2026-1-1",
    "title": "Leveraging Formal Methods to Strengthen Cyber-Resilience in Authorization to Operate",
    "authors": "Alice Lee, Jack Cheng, Tim Braje, Noah Luther, Ian McQuoid, Gruia-Catalin Roman, Joseph Dan Trujillo, Richard Skowyra, Samuel Mergendahl",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310477",
    "source": "IEEE",
    "abstract": "Authorization to Operate (ATO) is a challenging but necessary process for systems to obtain approval for deployment within the Department of Defense (DoD). ATO includes requirement creation, selection, implementation, and satisfaction. Because each step of ATO involves multiple stakeholders, any disconnect between stakeholders can lead to approval difficulties and delay. When these miscommunications compound, they undermine the main goal of the ATO process: to ensure that only secure systems are deployed. In order to addresses these difficulties, this paper proposes to augment the ATO process with formal methods to strengthen the cybersecurity posture of deployed systems. In particular, we first introduce the traditional ATO process and its main stakeholders. We then introduce formal methods and their role to improve ATO. Finally, we apply our enhanced ATO architecture on the Cyber-Hardened Satellite Software (CHSS), satellite flight control software developed by AFRL/RV and MIT LL, to demonstrate how our work illuminates gaps in the traditional ATO process, aids in better communication between stakeholders, and provides formalized security guarantees.",
    "title_zh": "利用形式化方法加强授权运营中的网络弹性",
    "abstract_zh": "授权操作（ATO）是国防部（DoD）系统在部署前必须经历的一项具有挑战性但必不可少的流程。ATO包括需求创建、选择、实施和满足等环节。由于ATO的每个步骤都涉及多个利益相关方，各利益相关方之间的任何沟通不畅都可能导致审批困难和延迟。当这些沟通问题叠加时，会削弱ATO流程的核心目标：确保仅部署安全的系统。为应对这些挑战，本文提出通过引入形式化方法来增强ATO流程，以强化已部署系统的网络安全态势。具体而言，我们首先介绍传统的ATO流程及其主要利益相关方；然后阐述形式化方法及其在改进ATO中的作用；最后，我们将所提出的增强型ATO架构应用于AFRL/RV与MIT LL联合开发的“网络加固卫星软件”（CHSS）——一种卫星飞行控制软件，以展示我们的工作如何揭示传统ATO流程中的漏洞，促进利益相关方之间更有效的沟通，并提供形式化的安全保证。"
  },
  {
    "date": "2026-1-1",
    "title": "GraphRL-Core: Intelligent Logic Synthesis Optimization via Graph Transformer and Deep Reinforcement Learning",
    "authors": "Sujie Zhu, Guande Dong, Haoyang He, Chong Xia, Jianwang Zhai",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11315954",
    "source": "IEEE",
    "abstract": "With the rapid expansion of circuit design scales, traditional logic synthesis methods are encountering severe bottlenecks in optimizing complex circuit structures. This paper proposes an intelligent optimization approach, GraphRL-Core, which combines a graph transformer with deep reinforcement learning (DRL) and innovatively integrates the global modeling capability of the Gradformer model with the decision adaptability of reinforcement learning (RL). The system automatically generates optimization instructions for logic circuits by extracting graph structural features, utilizing graph transformer encoding, and employing a multi-objective reward-driven policy learning approach. Experiments conducted on several representative circuits demonstrate that the proposed method outperforms both traditional optimization flows and existing RL frameworks in terms of LUT minimization, optimization efficiency, and generalization capability, highlighting its significant practical value.",
    "title_zh": "GraphRL-Core：基于图Transformer与深度强化学习的智能逻辑综合优化",
    "abstract_zh": "随着电路设计规模的迅速扩展，传统的逻辑综合方法在优化复杂电路结构方面正面临严峻瓶颈。本文提出了一种智能优化方法——GraphRL-Core，该方法将图Transformer与深度强化学习（DRL）相结合，并创新性地融合了Gradformer模型的全局建模能力与强化学习（RL）的决策适应性。系统通过提取电路的图结构特征，利用图Transformer进行编码，并采用多目标奖励驱动的策略学习方法，自动为逻辑电路生成优化指令。在多个代表性电路上的实验结果表明，所提方法在LUT最小化、优化效率及泛化能力方面均优于传统优化流程以及现有的强化学习框架，展现出显著的实际应用价值。"
  },
  {
    "date": "2026-1-1",
    "title": "Design and Implementation of an AI Agent for Natural Language-Driven Database Operations",
    "authors": "Aparup Roychowdhury, Anu Shit, Aritra Mukherjee, Subhabrata Banerjee",
    "publish": "2025 6th International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)",
    "url": "https://doi.org/10.1109/icicnis66685.2025.11315754",
    "source": "IEEE",
    "abstract": "Natural Language Interfaces to Database (NLIDB) plays a significant role in simplifying data access for users by providing access through natural language instead of complex SQL commands. This work presents an intelligent, agent-driven NLIDB system powered by a Large Language Model (LLM) and a ReAct agent architecture. This system generates accurate SQL queries by interpreting user inputs and performs the task by selecting appropriate tools. The defining feature of this work is the incorporation of specialized tools – Read, Write and Commit. The memory module stores previous interactions, which allows the agent to use it as a reference to improve subsequent responses. Multi-step database tasks can be executed easily by the agent by using the available tools iteratively. This ReAct agent separates write and commit actions, which supports iterative SQL execution, dynamic error correction, and safe transactional behaviour. This agent provides reliability by allowing users to review intermediate database changes before committing. Overall, the proposed system is capable of performing complex multi-query operations while maintaining accuracy, safety and transparency.",
    "title_zh": "基于自然语言驱动的数据库操作的人工智能代理的设计与实现",
    "abstract_zh": "自然语言数据库接口（NLIDB）在简化用户数据访问方面发挥着重要作用，它通过自然语言而非复杂的SQL命令为用户提供数据访问。本文提出了一种由大型语言模型（LLM）和ReAct智能体架构驱动的智能化NLIDB系统。该系统能够通过解析用户输入生成准确的SQL查询，并通过选择合适的工具来完成任务。本研究的突出特点是引入了专用工具——读取（Read）、写入（Write）和提交（Commit）。记忆模块用于存储之前的交互记录，使智能体能够将其作为参考以改进后续响应。通过迭代使用这些可用工具，智能体可轻松执行多步骤数据库操作。该ReAct智能体将写入与提交操作分离，支持SQL的迭代执行、动态错误修正以及安全的事务行为。通过允许用户在提交前审查中间数据库变更，该智能体提升了系统的可靠性。总体而言，所提出的系统能够在保证准确性、安全性与透明性的前提下，完成复杂的多查询操作。"
  },
  {
    "date": "2026-1-1",
    "title": "An End-to-End Compact Shape-Aware Macro Placer Using Reinforcement Learning",
    "authors": "Hailiang Li, Xiao Wang, Xu Yang, Miaohui Hao, Yan Huo, Beiping Yan",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316075",
    "source": "IEEE",
    "abstract": "In modern chip design, the placement of millions of circuit modules poses a significant challenge. However, macro placement can significantly enhance this process by enabling greater modularization of the chip design. In many cases, macros can occupy more than half of the die area, making their placement a critical factor in defining the die’s overall shape. Despite its importance, existing algorithms often either neglect this aspect or fail to address it adequately. This paper presents a novel macro placer that enhances reinforcement learning architectures to produce compact and shape-optimized placements. Our research demonstrates that strategically decoupling standard cells during macro placement can significantly reduce network complexity, enabling an efficient relinking solution. Additionally, we introduce a shape force mask into the reinforcement learning-based visual representation learning framework. The approach proposed in this paper enables the creation of compact and shape-optimized chip designs. Experimental results validate that our algorithm can produce more compact and regular macro placements while consistently maintaining other key performance metrics.",
    "title_zh": "基于强化学习的端到端紧凑型形状感知宏单元布局方法",
    "abstract_zh": "在现代芯片设计中，数百万个电路模块的布局面临着巨大挑战。然而，宏单元（macro）布局能够显著优化这一过程，通过实现更深层次的模块化设计。在许多情况下，宏单元占据芯片面积的一半以上，其布局成为决定芯片整体形状的关键因素。尽管如此，现有算法往往忽视或未能充分解决这一问题。本文提出了一种新型宏单元布局方法，通过增强强化学习架构，实现紧凑且形状优化的布局结果。我们的研究证明，在宏单元布局过程中有策略地解耦标准单元，可显著降低网络复杂度，从而实现高效的重新连接方案。此外，我们还引入了一种形状力掩码（shape force mask），融入基于强化学习的视觉表征学习框架中。本文所提出的方案能够生成紧凑且形状优化的芯片设计。实验结果表明，该算法不仅能够产生更加紧凑、规则的宏单元布局，同时在保持其他关键性能指标方面也表现出持续稳定的优异表现。"
  },
  {
    "date": "2026-1-1",
    "title": "HybridSYN: An Efficient Logic Synthesis Methodology",
    "authors": "Ke Wang, Yue Wu, Xiaoyan Yang",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11315937",
    "source": "IEEE",
    "abstract": "As the complexity of modern circuits increases, logic synthesis faces growing challenges. Reinforcement learning (RL) shows promise for efficient logic optimization. However, current RL methods rely solely on a single Boolean circuit represen-tation-typically And-Inverter Graphs (AIG). This constraint hinders the exploration of larger optimization spaces, leading to suboptimal outcomes. This study introduces HybridSYN, a hybrid RL framework combining optimization algorithms from AIG and Majority-Inverter Graphs (MIG) to address the limitations of synthesis methods relying on a single logic graph representation. By expanding the action space, HybridSYN enables the exploration of better LUT-level solutions. To improve state representation, HybridSYN processes topological features from both AIG and MIG using independent Graph Convolutional Network (GCN) encoders. The resulting features are fused and passed through a fully connected network, where the final layer jointly predicts the type of operator and its parameters. Experimental results on the EPFL benchmark suite show that HybridSYN achieves a 9.7% reduction in LUT count compared to existing RL methods and a $\\mathbf{1 7. 1 \\%}$ improvement over baseline approaches.",
    "title_zh": "HybridSYN：一种高效的逻辑综合方法",
    "abstract_zh": "随着现代电路复杂度的不断提升，逻辑综合面临着日益严峻的挑战。强化学习（RL）在高效逻辑优化方面展现出巨大潜力。然而，现有的RL方法仅依赖于单一的布尔电路表示形式——通常是与非门图（AIG）。这种限制阻碍了对更大优化空间的探索，导致优化结果不够理想。本研究提出了一种名为HybridSYN的混合强化学习框架，该框架融合了AIG和多数非门图（MIG）中的优化算法，以克服仅依赖单一逻辑图表示的合成方法的局限性。通过扩展动作空间，HybridSYN能够探索更优的查找表（LUT）级解决方案。为改善状态表示，HybridSYN采用独立的图卷积网络（GCN）编码器分别处理来自AIG和MIG的拓扑特征，随后将提取的特征进行融合，并输入全连接网络，最终层联合预测操作符类型及其参数。在EPFL基准套件上的实验结果表明，与现有RL方法相比，HybridSYN在LUT数量上减少了9.7%；相较于基线方法，更是实现了17.1%的显著提升。"
  },
  {
    "date": "2026-1-1",
    "title": "Evidence-based Hiring: A GitHub-Centric Approach to Candidate Assessment",
    "authors": "G. Kiran Kumar, Bokka Daivik Reddy, Sheelam Vihas Reddy, Anila Macharla, S. China Ramu",
    "publish": "2025 6th International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)",
    "url": "https://doi.org/10.1109/icicnis66685.2025.11315719",
    "source": "IEEE",
    "abstract": "Resume shortlisting is the first round of candidate evaluation in the recruitment process. The current method Automated Resume Parsers (ARPs) that use keyword matching to assess the claimed skills of the candidate. This creates a gap between qualifications and actual abilities, more specifically in terms of projects, code quality, etc. The current approach doesn’t check if the candidate actually used their skills, can’t find copied work, and doesn’t access the progress over time. Tool like CVExplorer and SkillsMap try to use GitHub data and evaluate the candidate’s skills, but they don’t support many languages, they’re not very accurate, can’t access the quality of their work. The GitMeter system, which was introduced in this paper, is a better option because it uses a smart analysis method to determine both hard and soft skills from publicly available projects on their GitHub. But it still has some issues: it only looks at public GitHub activity, works with just Python and Java, can’t catch copied or AI-generated code, and doesn’t assess important soft skills like teamwork or empathy.To fix these problems, our improved system adds support for more languages using tools like CodeQL, checks if code is original with the help of AI models and plagiarism tools,and academic profiles to build a more complete and reliable way to hire.",
    "title_zh": "基于证据的招聘：以GitHub为中心的候选人评估方法",
    "abstract_zh": "简历筛选是招聘流程中的第一轮候选人评估环节。目前普遍采用的自动化简历解析工具（ARPs）通过关键词匹配来评估候选人声称的技能，这导致了资质与实际能力之间的差距，尤其是在项目经验、代码质量等方面。现有方法无法判断候选人是否真正运用过其技能，难以发现抄袭内容，也无法追踪其能力的发展历程。尽管像CVExplorer和SkillsMap这样的工具尝试利用GitHub数据来评估候选人的技能，但它们支持的语言种类有限，准确率不高，且无法评估工作质量。本文提出的GitMeter系统则提供了一种更优的解决方案，它通过智能分析方法，从候选人在GitHub上的公开项目中识别出硬技能和软技能。然而，该系统仍存在一些问题：仅分析公开的GitHub活动，仅支持Python和Java两种语言，无法检测抄袭或AI生成的代码，也难以评估团队协作、同理心等重要软技能。为解决这些问题，我们改进的系统引入了CodeQL等工具以支持更多编程语言，借助AI模型和查重工具检测代码原创性，并结合学术履历信息，构建了一个更全面、更可靠的招聘评估体系。"
  },
  {
    "date": "2026-1-1",
    "title": "Agentic AI for Cyber Defense: LLM-Guided Hierarchical Multi-Agent Reinforcement Learning",
    "authors": "Guangyu Jiang, Mahdi Imani, Nathaniel D. Bastian, Tian Lan",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310253",
    "source": "IEEE",
    "abstract": "In cyber defense, agentic AI in the form of distributed multi-agent teams must rapidly adapt to evolving threats across dynamic network environments. Hierarchical Reinforcement Learning (HRL) mitigates sample complexity by decomposing complex objectives into simpler subskills, yet handcrafting these skills and their intrinsic rewards remains expertise-dependent and resource-intensive. We introduce a novel agentic AI framework that harnesses the task decomposition and planning capabilities of Large Language Models (LLMs) to automate both skill discovery and reward shaping within HRL. By prompting an LLM to identify semantically meaningful skills and craft corresponding intrinsic reward functions, our method significantly reduces manual engineering effort. We validate our approach in CAGE Challenge 4, a Multi-Agent Reinforcement Learning (MARL) cybersecurity environment, showing that our LLM-guided hierarchical policy achieves faster convergence and improved performance in cyber defense relative to flat MARL baselines. These results underscore the potential of integrating agentic AI into multi-agent HRL for scalable, interpretable, and efficient cyber-defense.",
    "title_zh": "用于网络防御的代理型人工智能：大语言模型引导的分层多智能体强化学习",
    "abstract_zh": "在网络安全防御中，以分布式多智能体团队形式存在的代理型人工智能必须能够快速适应动态网络环境中不断演变的威胁。层次化强化学习（HRL）通过将复杂目标分解为更简单的子技能，有效降低了样本复杂度，但手工设计这些技能及其内在奖励函数仍高度依赖专家经验且资源消耗巨大。我们提出一种新型代理型人工智能框架，利用大语言模型（LLM）的任务分解与规划能力，自动实现HRL中的技能发现与奖励设计。通过提示LLM识别语义上具有意义的技能并构建相应的内在奖励函数，我们的方法显著减少了人工工程的工作量。我们在CAGE挑战赛第4项——一个基于多智能体强化学习（MARL）的网络安全环境——中验证了该方法的有效性，结果表明，相较于传统的扁平式MARL基线，由LLM引导的分层策略在网络安全防御任务中实现了更快的收敛速度和更优的性能表现。这些成果凸显了将代理型人工智能融入多智能体HRL，在实现可扩展、可解释且高效的网络防御方面所蕴含的巨大潜力。"
  },
  {
    "date": "2026-1-1",
    "title": "A DDR Controller Circuit Gate-level Post-simulation Method in SoC Design",
    "authors": "Yande Jiang, Jingbo Ma, Guangda Zhang, Huiquan Wang, Bingxi Pei, Na Chen, Jian Fang",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316058",
    "source": "IEEE",
    "abstract": "As the complexity of System-on-Chip (SoC) designs increases, the gate-level post-simulation functional verification of SoCs has become increasingly critical. In addition, to enhance bandwidth, modern SoCs integrate high-performance DDR controller circuits. However, the gate-level post-simulation verification of DDR controllers in SoC designs demands substantial time and human resources. To address this challenge, we propose an efficient multi-level gate-level post-simulation methodology. To validate the proposed method, we design a SoC chip circuit which integrates 2 DDR4 controllers, 4 CPU cores, and employ the commercial Electronic Design Automatic (EDA) tools to obtain the DDR controller circuit netlist, and Standard Delay Format (SDF) files as the post-simulation input files. The experimental results demonstrate that our proposed multi-level gate-level post-simulation method achieves a $4 \\times$ speedup in functional post-simulation for the DDR controller circuit in SoC design, while also validating the effectiveness of the approach.",
    "title_zh": "SoC设计中DDR控制器电路门级后仿真方法",
    "abstract_zh": "随着系统级芯片（SoC）设计复杂度的不断提升，门级后仿真功能验证在SoC设计中变得日益关键。此外，为提升带宽性能，现代SoC集成了高性能的DDR控制器电路。然而，SoC设计中DDR控制器的门级后仿真验证需要耗费大量时间和人力。针对这一挑战，本文提出了一种高效的多层次门级后仿真方法。为验证所提方法的有效性，我们设计了一款集成2个DDR4控制器和4个CPU核的SoC芯片电路，并利用商用电子设计自动化（EDA）工具获取DDR控制器电路的网表文件以及标准延迟格式（SDF）文件，作为后仿真的输入数据。实验结果表明，所提出的多层次门级后仿真方法在SoC设计中对DDR控制器电路的功能后仿真实现了约4倍的加速，同时验证了该方法的有效性。"
  },
  {
    "date": "2026-1-1",
    "title": "Next-Generation AI-Enabled Incident Responses for Military Cloud Networks",
    "authors": "Vasil Boshnakov, Konrad Wrona, Caglar Ege Baser",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310692",
    "source": "IEEE",
    "abstract": "We explore the feasibility of a next-generation AI-enabled automated incident response approach for military networks hosted on cloud infrastructure, utilizing infrastructure-as-code principles to enable rapid and scalable recovery mechanisms. The methodology includes isolating compromised components, preserving evidence for forensic analysis, and rebuilding affected resources to ensure the continuity of critical military services. The proposed approach provides a resilient, automated, and scalable solution to maintain critical service availability during cyber incidents. The experimental results are analyzed to validate the effectiveness of the methodology in real-world military network scenarios. We describe the challenges faced while executing incident response in the cloud. We also discuss the integration of autonomous cyber defense agents for automating incident detection and response. The paper outlines the possible further implementations of decoys, transforming the environment into a continuous training ground for the red team and eventual AI vs. AI playground.",
    "title_zh": "下一代人工智能赋能的军事云网络事件响应",
    "abstract_zh": "我们探讨了一种下一代AI驱动的自动化事件响应方法在军事网络中的可行性，该网络部署于云基础设施之上，并采用“基础设施即代码”（Infrastructure-as-Code）原则，以实现快速且可扩展的恢复机制。该方法包括隔离受损组件、保留证据以供取证分析，以及重建受影响资源，从而确保关键军事服务的持续运行。所提出的方案提供了一种具备韧性、自动化和可扩展性的解决方案，能够在网络攻击事件期间维持关键服务的可用性。通过实验结果分析，验证了该方法在真实军事网络场景下的有效性。本文还描述了在云环境中执行事件响应所面临的挑战，并讨论了自主网络安全防御代理的集成，以实现事件检测与响应的自动化。此外，论文还展望了诱饵系统等进一步实施的可能性，使环境转变为红队持续训练的平台，最终发展为人工智能之间的对抗演练场。"
  },
  {
    "date": "2026-1-1",
    "title": "Daisy Chain Transmitter Circuit Design with AntiElectromagnetic Interference for BMS Chip in EV",
    "authors": "Chen Lin, Liji Wu, Jing Hu, Zonghuan Wu, Xiangmin Zhang",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316021",
    "source": "IEEE",
    "abstract": "New energy vehicles have three core systems, namely batteries, motors and electronic control systems. The difficulties in the research and development of power batteries lie in the safety issues, service life and manufacturing costs of lithium batteries. Only by accurately collecting and monitoring parameters such as voltage, current and temperature in power batteries can the safety and reliability of electric vehicles be guaranteed. The battery management system (BMS) in electric vehicles (EVs) requires reliable communication interfaces to accurately monitor and control lithium-ion battery cells. This paper proposes an anti-electromagnetic interference Daisy link port circuit that uses capacitors or transformers as isolators. The system includes the transmitter circuit, the receiver circuit and the isolator circuit. This article centers on the high-reliability communication needs of the power battery management system (BMS) chip applied to new energy vehicles. It lays stress on the design of the transmitter circuit for the Daisy chain communication interface, which adopts a differential signal processing framework. The high-pressure BCD process using the technology from eastern South Korea, By means of suppression, it controls high-voltage common-mode interference to guarantee signal integrity. The effective filtered pulse width is below 250 ns, Digital circuits offer two modes which cuts down noise and improves the capability to withstand electromagnetic interference.",
    "title_zh": "电动汽车BMS芯片用抗电磁干扰的菊花链发射电路设计",
    "abstract_zh": "新能源汽车具有三大核心系统，即电池、电机和电子控制系统。动力电池研发的难点在于锂离子电池的安全性、使用寿命以及制造成本问题。只有准确采集并监测动力电池中的电压、电流和温度等参数，才能保障电动汽车的安全性和可靠性。电动汽车（EV）中的电池管理系统（BMS）需要可靠的通信接口，以实现对锂离子电池单体的精确监控与控制。本文提出一种抗电磁干扰的Daisy链路端口电路，采用电容或变压器作为隔离元件。该系统包括发送电路、接收电路和隔离电路三部分。本文聚焦于应用于新能源汽车的动力电池管理系统（BMS）芯片所面临的高可靠性通信需求，重点阐述了Daisy链式通信接口的发送电路设计，采用差分信号处理架构。通过采用来自韩国东部的高压BCD工艺技术，结合有效的抑制手段，有效控制高压共模干扰，确保信号完整性。经过滤后的有效脉冲宽度低于250纳秒，数字电路提供两种工作模式，可有效降低噪声，提升抗电磁干扰能力。"
  },
  {
    "date": "2026-1-1",
    "title": "Graph-Based Representation of Verilog HDL: Python-Based Control and Data Flow Graph Generation",
    "authors": "Yipeng Wang, Zhiqiang He, Lingwei Yan, Gang Chen",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11315933",
    "source": "IEEE",
    "abstract": "This paper proposes an automatic framework for controlled data flow graph (CDFG) generation from verilog designs, where the generated CDFGs can be applied to visualization, formal verification, logic optimization, and serve as structured representations for large-scale machine learning models. The framework leverages Yosys to flatten the RTL code, eliminating high-level control structures, and then constructs an abstract syntax tree (AST) to dynamically extract control and data dependencies. A unified graph representation is generated, enabling accurate modeling of both control-flow and data-flow semantics. The proposed RTL-to-CDFG conversion tool demonstrates high scalability and precision, successfully handling million-line industrial Verilog designs. Experimental results confirm that extracted CDFGs capture essential structural and semantic features of verilog code, providing a solid foundation for downstream tasks that include dataflow analysis, logic simplification, and resource sharing.",
    "title_zh": "基于图的Verilog HDL表示：基于Python的控制流与数据流图生成",
    "abstract_zh": "本文提出了一种从Verilog设计中自动生成受控数据流图（CDFG）的框架。所生成的CDFG可应用于可视化、形式化验证、逻辑优化，并作为大规模机器学习模型的结构化表示。该框架利用Yosys对RTL代码进行扁平化处理，消除高层控制结构，随后构建抽象语法树（AST），动态提取控制依赖和数据依赖关系。通过生成统一的图表示，能够准确建模控制流与数据流语义。所提出的RTL到CDFG转换工具展现出高度的可扩展性和精确性，成功处理了包含数百万行代码的工业级Verilog设计。实验结果表明，提取出的CDFG能够有效捕捉Verilog代码的关键结构与语义特征，为下游任务（如数据流分析、逻辑简化和资源共享）提供了坚实基础。"
  },
  {
    "date": "2026-1-1",
    "title": "A Novel Region-Wise Automatic Routing Algorithm for Analog Circuit",
    "authors": "Hao Xie, Wenxue Chen, Wei Zhang, Bijian Lan, Jing Wan",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316056",
    "source": "IEEE",
    "abstract": "Routing is a key step of circuit design, directly determining the performance of the circuit. Existing research on analog circuit routing is mostly applicable only to specific circuits and follows a net-wise routing approach based on a particular net order. These methods have poor generalization and cannot guarantee full routability and global optimization. This paper proposes a region-wise routing algorithm for circuits with macro-cells of arbitrary numbers, sizes, and shapes. The algorithm can determine whether the circuit is routable during the global routing stage. A special step is inserted between global and detailed routing to classify and positioning the pins of each net based on region sorting result. During detailed routing step, optimized routing strategies is selected based on the net conditions of the regions. Therefore, the algorithm in this paper can effectively improve routing efficiency, routability and has excellent generalization.",
    "title_zh": "一种用于模拟电路的新型区域化自动布线算法",
    "abstract_zh": "路由是电路设计中的关键步骤，直接决定了电路的性能。现有的模拟电路布线研究大多仅适用于特定电路，且基于特定网络顺序采用逐网布线的方法，泛化能力差，无法保证完全可布线性与全局优化。本文提出一种针对具有任意数量、尺寸和形状宏单元的电路的区域化布线算法。该算法可在全局布线阶段判断电路是否可布线。在全局布线与详细布线之间插入一个特殊步骤，根据区域排序结果对每个网络的引脚进行分类与定位。在详细布线阶段，依据各区域的网络状况选择最优的布线策略。因此，本文提出的算法能够有效提升布线效率与可布线性，并具备优异的泛化能力。"
  },
  {
    "date": "2026-1-1",
    "title": "Residual Feature Enhancement for Large Language Models: Methodology and Applications",
    "authors": "Qidong Yan, Chenglin Jiang, Yingjie Li, Ning Ma",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3650213",
    "source": "IEEE",
    "abstract": "Large language models (LLMs) have achieved remarkable progress in natural language processing, yet their ability to perform complex logical reasoning remains limited. Existing approaches such as prompting, retrieval-augmented generation, and parameter-efficient fine-tuning (PEFT) provide partial improvements but often suffer from prompt sensitivity, semantic compression, or additional computational cost. In this work, we propose a Residual Feature Enhancement (RFE) module, a lightweight architectural component designed to strengthen reasoning ability while maintaining computational efficiency. RFE integrates a dimension-preserving linear transformation, SwiGLU nonlinear activation, and residual connections to enrich attention outputs without altering the backbone structure. We conducted comprehensive experiments across six reasoning and comprehension benchmarks—LogiQA, ReClor, LogiQA2.0, GSM8K, HellaSwag, and MBPP—covering deductive reasoning, standardized test comprehension, commonsense inference, and program synthesis. Results demonstrate that ChatGLM4-9B augmented with RFE consistently achieves superior performance compared with both adapter-based methods and larger-scale baselines. Specifically, ChatGLM4-9B+RFE attains 68.20% on LogiQA, 82.00% on ReClor, 79.74% on LogiQA 2.0, 95.68% on GSM8K, 72.42% on HellaSwag, and 56.82% on MBPP, all of which surpass the Adapter mechanism (67.68%, 81.15%, 78.52%, 94.47%, 66.85%, 55.02%) and show clear advantages over open-source baselines such as Qwen1.5-MoE-A2.7B, Llama3.1-8B, and DeepSeek distilled models. Ablation studies further confirm that removing RFE leads to performance degradation of up to 3.84 percentage points, and convergence analysis shows improved stability and faster training.",
    "title_zh": "大语言模型的残差特征增强：方法与应用",
    "abstract_zh": "大型语言模型（LLMs）在自然语言处理领域取得了显著进展，但其在复杂逻辑推理方面的能力仍然有限。现有的方法如提示工程（prompting）、检索增强生成（retrieval-augmented generation）以及参数高效微调（PEFT）虽在一定程度上提升了性能，却普遍存在对提示敏感、语义压缩或额外计算开销等问题。本文提出一种轻量级的架构组件——残差特征增强模块（Residual Feature Enhancement, RFE），旨在不牺牲计算效率的前提下，有效提升模型的推理能力。RFE通过引入保持维度的线性变换、SwiGLU非线性激活函数以及残差连接，丰富注意力输出，同时无需改变原始模型结构。\n\n我们在六个涵盖演绎推理、标准化测试理解、常识推理与程序合成任务的基准测试上进行了全面实验：LogiQA、ReClor、LogiQA2.0、GSM8K、HellaSwag 和 MBPP。结果表明，采用 RFE 增强后的 ChatGLM4-9B 模型在各项任务中均表现优异，显著优于基于适配器的方法及更大规模的基线模型。具体而言，ChatGLM4-9B+RFE 在 LogiQA 上达到 68.20%，ReClor 达到 82.00%，LogiQA2.0 达到 79.74%，GSM8K 达到 95.68%，HellaSwag 达到 72.42%，MBPP 达到 56.82%。这些结果不仅超越了适配器机制（分别为 67.68%、81.15%、78.52%、94.47%、66.85%、55.02%），也明显优于 Qwen1.5-MoE-A2.7B、Llama3.1-8B 和 DeepSeek 优化蒸馏模型等开源基线。\n\n消融实验进一步验证了 RFE 的有效性：移除 RFE 后性能下降最高达 3.84 个百分点；收敛性分析显示，加入 RFE 的模型具有更优的训练稳定性与更快的收敛速度。"
  },
  {
    "date": "2026-1-1",
    "title": "Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things",
    "authors": "Christopher D. Molek, Roberto Fronteddu, K. Brent Venable, Niranjan Suri",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310311",
    "source": "IEEE",
    "abstract": "The expansion of the Internet of Things (IoT) in the battlefield, Internet of Battlefield Things (IoBT), gives rise to new opportunities for enhancing situational awareness. To increase the potential of IoBT for situational awareness in critical decision making, the data from these devices must be processed into consumer-ready information objects and made available to consumers on demand. To address this challenge we propose a workflow that makes use of natural language processing (NLP) to query a database and return a response in natural language. Our solution utilizes Large Language Models (LLMs) sized for edge devices to perform NLP, as well as a graph database. These types of databases are well suited for the dynamic, connected networks pervasive in the IoBT. Our architecture employs LLMs for both mapping questions in natural language to Cypher database queries as well as to summarize the database output back to the user in natural language. We evaluated several medium-sized LLMs for both of these tasks on a database representing publicly available data from the US Army’s Multi-purpose Sensing Area Multi-Purpose Sensing Area (MSA) at the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion parameters) outperforms the other models across all considered metrics. Most importantly, we note that, unlike current methods, our two step approach allows the relaxation of the Exact Match (EM) requirement of the produced Cypher queries with ground truth code and, in this way, it achieves a 19.4% increase in accuracy. Our workflow lays the groundwork for deploying LLMs on edge devices to enable natural language interactions with databases containing information objects for critical decision making.",
    "title_zh": "战场物联网中边缘设备的数据库自然语言交互",
    "abstract_zh": "物联网（IoT）在战场环境中的扩展，即“战场物联网”（IoBT），为提升态势感知能力带来了新的机遇。为了充分发挥IoBT在关键决策中增强态势感知的潜力，这些设备产生的数据必须被处理成可供消费者使用的、易于理解的信息对象，并按需提供给使用者。为应对这一挑战，我们提出了一种工作流程，利用自然语言处理（NLP）技术查询数据库，并以自然语言形式返回响应。我们的解决方案采用专为边缘设备设计的大型语言模型（LLMs）来执行NLP任务，同时结合图数据库。这类数据库非常适合处理IoBT中普遍存在的动态、高度互联的网络结构。我们的架构利用LLM完成两个关键任务：一是将用户用自然语言提出的问题映射为Cypher数据库查询语句；二是将数据库返回的结果重新总结为自然语言反馈给用户。我们在一个代表美国陆军位于新墨西哥州拉斯克鲁塞斯市朱纳达草原（Jornada Range）多用途传感区（MSA）公开数据的数据库上，对多个中等规模的LLM在上述两项任务中的表现进行了评估。结果表明，Llama 3.1（80亿参数）在所有评估指标上均优于其他模型。尤为重要的是，与现有方法不同，我们的两步式方法能够放宽对生成的Cypher查询与真实代码之间“精确匹配”（EM）的要求，从而在准确率上实现了19.4%的提升。该工作流程为在边缘设备上部署LLM以实现与包含关键决策信息对象的数据库进行自然语言交互奠定了基础。"
  },
  {
    "date": "2026-1-1",
    "title": "RLMBFF: Multi-bit Flip-Flops Merging using Deep Reinforcement Learning",
    "authors": "Zeqi Chen, Zhaori Bi, Changhao Yan, Ming Zhu, Xiulong Wu, Xuan Zeng",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316072",
    "source": "IEEE",
    "abstract": "Multi-bit flip-flop (MBFF) merging has been proven to be an effective way to save dynamic power and reduce the utilization rate of the chip area. Most of the existing work uses either a clustering method or an analytical way to merge multiple single-bit flip-flops (SBFFs) into MBFFs. In this paper, inspired by the recent success in deep reinforcement learning, we propose a novel MBFF merging method leveraging neural network. The approach can be divided into two steps: (i) placing MBFFs by a pretrained RL agent, and (ii) mapping the pin pairs of the original SBFF to the MBFF cell. Experimental results show that our method can significantly save power and area, without large perturbations to the timing.",
    "title_zh": "基于深度强化学习的多比特触发器合并技术",
    "abstract_zh": "多比特触发器（MBFF）合并已被证明是节省动态功耗并降低芯片面积利用率的有效方法。目前大多数现有工作采用聚类方法或解析方法，将多个单比特触发器（SBFF）合并为MBFF。本文受深度强化学习近期成功应用的启发，提出了一种基于神经网络的新型MBFF合并方法。该方法可分为两个步骤：(i) 由预训练的强化学习代理进行MBFF的布局；(ii) 将原始SBFF的引脚对映射到MBFF单元。实验结果表明，我们的方法能够在不显著影响时序的情况下，显著节省功耗和面积。"
  },
  {
    "date": "2026-1-1",
    "title": "Dynamic Reward Weighting Based Deep Q-Learning for Routing in Advanced Packaging",
    "authors": "Shubin Chen, Qinghai Liu, Xiaowei Wu, Jun Xu, Xiaolin Xu",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316038",
    "source": "IEEE",
    "abstract": "In the contemporary electronics industry, advanced packaging technology has emerged as a critical avenue for overcoming performance bottlenecks in the semiconductor sector. Among these, redistribution layer (RDL)-a pivotal component of electronic design automation (EDA)-face significant challenges, including low routing efficiency and suboptimal outcomes. To address these limitations, this study introduces a deep Q-learning (DQN)-based approach for the dynamic adjustment of reward weights in the context of RDL global routing for multi-chip packaging (MCP). By training a set of weight vectors to adaptively regulate the relative importance of sub-objectives within the overall reward function in real time, the proposed method enhances both flexibility and efficiency during training and inference. Experimental results demonstrate that the approach achieves a $\\mathbf{1 0. 7 \\%}$ reduction in overall routing length compared with conventional strategies employing fixed reward weights.",
    "title_zh": "基于动态奖励加权的深度Q学习在先进封装路由中的应用",
    "abstract_zh": "在当代电子产业中，先进封装技术已成为突破半导体领域性能瓶颈的关键途径。其中，重分布层（RDL）作为电子设计自动化（EDA）中的核心组件，面临着布线效率低、优化效果不理想等重大挑战。为解决这些问题，本文提出一种基于深度Q学习（DQN）的方法，用于多芯片封装（MCP）场景下RDL全局布线过程中奖励权重的动态调整。通过训练一组权重向量，实时自适应地调节整体奖励函数中各子目标的相对重要性，所提方法在训练与推理阶段均显著提升了灵活性与效率。实验结果表明，相较于采用固定奖励权重的传统策略，该方法可使整体布线长度减少10.7%。"
  },
  {
    "date": "2026-1-1",
    "title": "Generative Model-Driven Code Creation and Vulnerability Analysis for Developmental Applications",
    "authors": "Noah Reneau, Kyle Stein, Brian Walker, Seth Vinson, Lanier Watkins",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310199",
    "source": "IEEE",
    "abstract": "With the recent societal implications of generative artificial intelligence (AI), particularly in the realm of software development, concerns about the reliability and security of AI-generated code becomes paramount in the minds of researchers and organizations alike. This paper examines whether large language models (LLMs) can develop effective and secure tools for auditing AI-developed autonomous agents in a competitive game environment. To model this scenario, we focus on a modified version of weighted Rock-Paper-Scissors, specifically Glico, as a testbed for adversarial AI competition. We then prompted the LLM to generate various strategy bots to play the game, including: a pattern-based bot, a cyclic-strategy bot, a random-move bot, and a reinforcement learning bot using Q-learning. The bot development process was iterative, with the LLM refining code in response to functional errors identified through custom dynamic testing strategies. We evaluated the performance of each bot (win/loss rates, time to win), as well as analyzed the security of the AI-generated code using static analysis tools. Our results demonstrate that while LLMs can generate functional game-playing agents, doing so requires multiple prompt iterations and human guidance. Even then, security vulnerabilities remain when deploying AI-generated code in user-created environments like the tested gaming platform. The results of this project aim to provide insight into the practical potential of LLMs to develop AI within restricted, goal-driven environments.",
    "title_zh": "生成式模型驱动的开发应用代码生成与漏洞分析",
    "abstract_zh": "随着生成式人工智能（AI）在社会层面的影响日益显著，尤其是在软件开发领域，研究人员和组织普遍关注AI生成代码的可靠性与安全性。本文探讨大型语言模型（LLM）是否能够开发出有效且安全的工具，用于审计在竞争性游戏环境中由AI自主生成的智能体。为模拟这一场景，我们选取经过修改的加权石头剪刀布游戏——Glico作为对抗性AI竞赛的测试平台。随后，我们向LLM提出指令，要求其生成多种策略型机器人参与游戏，包括基于模式的机器人、循环策略机器人、随机走步机器人，以及采用Q学习算法的强化学习机器人。机器人开发过程采用迭代方式，LLM根据通过自定义动态测试策略发现的功能性错误不断优化代码。我们评估了各机器人的表现（胜率/败率、获胜时间），并利用静态分析工具对AI生成代码的安全性进行了分析。研究结果表明，尽管LLM能够生成具备功能性的游戏参与者，但这一过程需要多次提示迭代及人工指导。即便如此，在将AI生成代码部署到用户自定义环境（如本研究所测试的游戏平台）时，仍存在安全漏洞。本项目的研究成果旨在揭示LLM在受限、目标驱动环境内开发AI的实际潜力。"
  },
  {
    "date": "2026-1-1",
    "title": "VCIL: An Open-Source Pipeline-Tight HIL Framework for Cycle-Accurate RISC-V Coprocessor Verification",
    "authors": "Xian Lin, Xin Zheng, Zhixin Fan, Huaien Gao, Shuting Cai, Xiaoming Xiong",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11315960",
    "source": "IEEE",
    "abstract": "The open-source RISC-V instruction set architecture (ISA) enables custom instructions to implement coprocessors, which requires higher verification speed and timing accuracy. However, most existing hardware-in-the-loop (HIL) approaches fail to meet these. To this end, this brief proposes Virtual-Coprocessor-in-the-Loop (VCIL), a pipeline-tight HIL framework for cycle-accurate RISC-V coprocessor verification. It bypasses the TLM2.0 bus and tightly couples the coprocessor to the pipeline via UART, which reduces transmissions and improves simulation speed. Additionally, a timing dynamic calibration method is introduced to enhance timing accuracy and enable early detection of timing violations. Compared to the state-of-the-art Virtual-Peripheral-in-the-Loop (VPIL) framework, VCIL improves simulation speed by $4.8 \\times$ and reduces timing errors by 35.9%. Experiments conducted on the SM3/SM4 coprocessor demonstrate that VCIL reduces verification time by 53.4%, and the timing error is only 3.5% compared to the RTL full-system. Our code will be available at https://github.com/LX-IC/VCIL.",
    "title_zh": "VCIL：一个开源的、面向周期精确的RISC-V协处理器验证的管道紧密型HIL框架",
    "abstract_zh": "开源的RISC-V指令集架构（ISA）支持通过自定义指令实现协处理器，这要求具备更高的验证速度和时序精度。然而，现有的大多数硬件在环（HIL）方法难以满足这些需求。为此，本文提出了一种名为“虚拟协处理器在环”（Virtual-Coprocessor-in-the-Loop, VCIL）的流水线紧密型HIL框架，用于实现周期精确的RISC-V协处理器验证。VCIL绕过了TLM2.0总线，通过UART将协处理器与流水线紧密耦合，减少了通信开销，从而提升了仿真速度。此外，本文还引入了一种动态时序校准方法，显著提高了时序精度，并能够提前检测出时序违规问题。与当前最先进的“虚拟外设在环”（VPIL）框架相比，VCIL的仿真速度提升了4.8倍，时序误差降低了35.9%。在SM3/SM4协处理器上的实验表明，VCIL将验证时间缩短了53.4%，且时序误差仅占RTL全系统模型的3.5%。相关代码将公开于 https://github.com/LX-IC/VCIL。"
  },
  {
    "date": "2026-1-1",
    "title": "Hardware Implementation and Side-Channel Security Analysis for High-Precision AI Tansformer Encoder",
    "authors": "Wentao Wang, Liji Wu, Jing Hu, Le Wu, Xiangmin Zhang",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316085",
    "source": "IEEE",
    "abstract": "This paper provides a high-precision transformer encoder hardware by using fixed-point numbers with variable decimal places. The original power consumption trajectory of the feedforward neural network module was collected. It was found that there was an obvious power consumption peak in the output of this module. Therefore, we implemented the output obfuscation technology for this module and collected the protected power consumption trajectory. The verification is based on an FPGA development board xc7k160tfbg676-1 to design and verify the transformer encoder with a token of 64 and a word embedding dimension of 512. The verification results show that the usage of digital signal processing (DSP) is 552, and logic Look-Up Tables (LUT) is 96,043. In this experiment, the model’s data is quantized to 16-bit signed fixed-point numbers, with the number of decimal places varying according to different modules. A compensation coefficient is added to the matrix multiplication and a module design method of divide-and-add is adopted. The average error of the hardware implementation compared to software calculation is 0.000867, and the execution time at 200 MHz is 8.6837 ms, which is 1.657 times faster, saving approximately 39.7% of the execution time.",
    "title_zh": "高精度AI Transformer编码器的硬件实现与侧信道安全分析",
    "abstract_zh": "本文提出了一种基于可变小数位数的定点数实现的高精度Transformer编码器硬件。首先，采集了前馈神经网络模块的原始功耗轨迹，发现该模块输出存在明显的功耗峰值。为此，我们对该模块实施了输出混淆技术，并采集了防护后的功耗轨迹。验证基于FPGA开发板xc7k160tfbg676-1进行，设计并验证了输入token长度为64、词嵌入维度为512的Transformer编码器。验证结果表明，该设计使用了552个数字信号处理单元（DSP），逻辑查找表（LUT）数量为96,043。在本实验中，模型数据被量化为16位有符号定点数，不同模块的小数位数根据实际需求动态调整。在矩阵乘法中引入补偿系数，并采用分治加法的模块化设计方法。与软件计算相比，硬件实现的平均误差为0.000867，工作频率为200 MHz时的执行时间为8.6837毫秒，速度提升1.657倍，约节省39.7%的执行时间。"
  },
  {
    "date": "2026-1-1",
    "title": "Hybrid Knowledge Graph Integration in Large Language Models for Explainable Contextual Understanding and Improved Accuracy",
    "authors": "G Venkata Krishna, D Arthur Jebastine Sunderraj, Finito Deagesh D, Udaya Kiran M, V T Ram Pavan Kumar M, V MNSSVKR Gupta",
    "publish": "2025 6th International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)",
    "url": "https://doi.org/10.1109/icicnis66685.2025.11315825",
    "source": "IEEE",
    "abstract": "The study investigated the application of Hybrid Knowledge Graphs (HKGs) to Large Language Models (LLMs) to improve contextual understanding, semantic accuracy, and interpretability. Although LLMs such as GPT and BERT can be quite strong language generators, they tend to fail and have problems in factual reasoning, entity disambiguation, and multi-hop inference because they have very weak structured knowledge. To overcome these shortcomings, a hybrid architecture was developed combining structured knowledge of Knowledge Graphs with transformer based Embeddings of LLM using embedding alignment, contextual fusion and attention-based reasoning. Several benchmark datasets, quantitative metrics, such as, perplexity, contextual coherence, semantic similarity, and reasoning accuracy, and visual analyses, such as, accuracy trends, confusion matrices, and embedding projections, were used to evaluate the model. The findings showed that the hybrid model was much more effective at contextual understanding, semantic ambiguity, and reasoning interpretability than the base LLMs. The results show that Knowledge Graph integration is semantically grounded, can enhance representational consistency, and can support human-like reasoning, which can be used to present a strong framework of explainable and context-aware AI systems.",
    "title_zh": "大型语言模型中混合知识图谱的集成，以实现可解释的上下文理解与更高的准确性",
    "abstract_zh": "该研究探讨了混合知识图谱（HKGs）在大型语言模型（LLMs）中的应用，旨在提升上下文理解能力、语义准确性以及可解释性。尽管GPT、BERT等大型语言模型在语言生成方面表现强劲，但由于其结构化知识极为薄弱，在事实推理、实体消歧和多跳推理等方面常出现失败或问题。为克服这些缺陷，研究人员提出了一种混合架构，将知识图谱的结构化知识与基于Transformer的LLM嵌入表示相结合，通过嵌入对齐、上下文融合以及基于注意力的推理机制实现协同优化。研究采用多个基准数据集，并利用多种定量指标（如困惑度、上下文连贯性、语义相似度和推理准确率）以及可视化分析方法（如准确率趋势图、混淆矩阵和嵌入投影图）对模型进行评估。结果表明，相较于基础LLM，该混合模型在上下文理解、语义模糊处理及推理可解释性方面均有显著提升。研究结果证明，知识图谱的融合具有坚实的语义基础，能够增强表征的一致性，并支持类人化的推理过程，从而为构建可解释、上下文感知的人工智能系统提供了强有力的方法框架。"
  },
  {
    "date": "2026-1-1",
    "title": "Enabling Trustworthy Federated Learning via Remote Attestation for Mitigating Byzantine Threats",
    "authors": "Chaoyu Zhang, Heng Jin, Shanghao Shi, Hexuan Yu, Sydney Johns, Y. Thomas Hou, Wenjing Lou",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310549",
    "source": "IEEE",
    "abstract": "Federated Learning (FL) has gained significant attention for its privacy-preserving capabilities, enabling distributed devices to collaboratively train a global model without sharing raw data. However, its distributed nature forces the central server to blindly trust the local training process and aggregate uncertain model updates, making it susceptible to Byzantine attacks from malicious participants, especially in mission-critical scenarios. Detecting such attacks is challenging due to the diverse knowledge across clients, where variations in model updates may stem from benign factors, such as non-IID data, rather than adversarial behavior. Existing data-driven defenses struggle to distinguish malicious updates from natural variations, leading to high false positive rates and poor filtering performance.To address this challenge, we propose Sentinel, a remote attestation (RA)-based scheme for FL systems that regains client-side transparency and mitigates Byzantine attacks from a system security perspective. Our system employs code instrumentation to track control-flow and monitor critical variables in the local training process. Additionally, we utilize a trusted training recorder within a Trusted Execution Environment (TEE) to generate an attestation report, which is cryptographically signed and securely transmitted to the server. Upon verification, the server ensures that legitimate client training processes remain free from program behavior violation or data manipulation, allowing only trusted model updates to be aggregated into the global model. Experimental results on IoT devices demonstrate that Sentinel ensures the trustworthiness of the local training integrity with low runtime and memory overhead.",
    "title_zh": "通过远程证明实现可信的联邦学习以缓解拜占庭威胁",
    "abstract_zh": "联邦学习（Federated Learning, FL）因其出色的隐私保护能力而受到广泛关注，使分布式设备能够在不共享原始数据的情况下协同训练全局模型。然而，其分布式特性导致中心服务器必须盲目信任本地训练过程，并聚合存在不确定性的模型更新，从而容易受到恶意参与方发起的拜占庭攻击，尤其在关键任务场景下风险更高。由于客户端间知识差异显著，模型更新的差异可能源于非独立同分布（non-IID）数据等良性因素，而非恶意行为，这使得攻击检测极具挑战性。现有的数据驱动型防御方法难以区分恶意更新与自然变化，常导致误报率高且过滤效果不佳。\n\n为应对这一挑战，我们提出Sentinel——一种基于远程证明（Remote Attestation, RA）的联邦学习安全机制，从系统安全角度恢复客户端侧的透明性，有效抵御拜占庭攻击。该系统通过代码插桩技术追踪控制流并监控本地训练过程中的关键变量。同时，我们利用可信执行环境（Trusted Execution Environment, TEE）内的可信训练记录器生成经密码学签名的证明报告，并安全地传送到服务器端。服务器在验证后可确保合法客户端的训练过程未发生程序行为异常或数据篡改，仅允许可信的模型更新被纳入全局模型的聚合。在物联网设备上的实验结果表明，Sentinel在保证本地训练完整性可信的同时，具有极低的运行时开销和内存占用。"
  },
  {
    "date": "2026-1-1",
    "title": "PAVE-MAVLink: Formal Verification of MAVLink 2 for Secure UAV Communications",
    "authors": "Tom Wray, Ying Wang",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310743",
    "source": "IEEE",
    "abstract": "This work presents a multi-stage, Large Language Model(LLM)-accelerated framework for formal verification and security validation of UAV communication protocols, focusing on MAVLink 2. We introduce a prompt-driven approach to automatically generate symbolic models for nuXmv and ProVerif, allowing rapid identification and mitigation of protocol vulnerabilities. Experimental results demonstrate that, without cryptographic protections, MAVLink 2 is susceptible to critical command injection attacks, validated through symbolic model checking, software-defined drone simulation, and over-the-air (OTA) testing on physical UAVs. Incorporating ChaCha20 based encryption eliminates these vulnerabilities, as confirmed by formal analysis and empirical validation. These findings illustrate the effectiveness and flexibility of LLM assisted workflows like Prompt Aided Formal Verification (PAVE) while providing a platform for robust cryptographic extensions in advancing UAV protocol security.",
    "title_zh": "PAVE-MAVLink：面向安全无人机通信的MAVLink 2形式化验证",
    "abstract_zh": "本文提出了一种多阶段、基于大语言模型（LLM）加速的框架，用于无人机（UAV）通信协议的形式化验证与安全评估，重点针对MAVLink 2协议。我们引入了一种提示驱动的方法，可自动为nuXmv和ProVerif生成符号模型，从而实现对协议漏洞的快速识别与修复。实验结果表明，在缺乏密码保护的情况下，MAVLink 2协议极易遭受严重的命令注入攻击，该结论通过符号模型检测、软件定义的无人机仿真以及在真实无人机上的无线空中（OTA）测试得到了验证。引入基于ChaCha20的加密机制后，这些安全漏洞被有效消除，这一结论也得到了形式化分析与实证验证的支持。研究结果不仅展示了LLM辅助工作流（如提示引导的形式化验证，PAVE）的有效性与灵活性，也为推进无人机协议安全性的增强提供了可靠的密码学扩展平台。"
  },
  {
    "date": "2026-1-1",
    "title": "Automatic Generation Method of Test Vectors for FPGA Interconnect Resources Based on Self-adaptive Ford-Fulkerson Algorithm",
    "authors": "Liang Zhou, Wei Zhou, Mingzhe Li, Kun Guo, Ding Zhang, Yan Huang",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11315918",
    "source": "IEEE",
    "abstract": "SRAM-based FPGA is widely used in military and aerospace applications because of its flexibility and programmability. Therefore, the reliability of FPGA is particularly important. The interconnect resources (IR) of FPGA pose a critical challenge in testing due to their large scale and complex structure. This article first analyzes the structure characteristics of interconnect resources and constructs a transfer model in ultra-large-scale FPGA interconnect. Based on the maximum flow problem in graph theory, an adaptive Ford-Fulkerson algorithm is proposed, which solves the technical difficulty of constructing effective test graphics of interconnect resources. Finally, this article implements an automatic generation method of IR test vectors via the XDL development tool, and forms a high-coverage test vector set for $\\mathbf{7 0}$-million-gate FPGA’s complex IR, ensuring the high reliability and security of FPGA applications.",
    "title_zh": "基于自适应Ford-Fulkerson算法的FPGA互连资源测试向量自动生成方法",
    "abstract_zh": "基于SRAM的FPGA因其灵活性和可编程性，被广泛应用于军事和航空航天领域，因此FPGA的可靠性尤为重要。FPGA中的互连资源（Interconnect Resources, IR）由于规模庞大且结构复杂，在测试方面面临严峻挑战。本文首先分析了互连资源的结构特性，并构建了超大规模FPGA互连资源的传输模型。基于图论中的最大流问题，提出了一种自适应的Ford-Fulkerson算法，有效解决了互连资源高效测试图形构造的技术难题。最后，本文利用XDL开发工具实现了一种互连资源测试向量的自动生成方法，为拥有7000万门级复杂互连资源的FPGA生成了高覆盖率的测试向量集，从而保障了FPGA应用的高可靠性和安全性。"
  },
  {
    "date": "2026-1-1",
    "title": "Hardware Acceleration for Massive-MIMO Hybrid Beamforming on RISC-V Vector DSPs",
    "authors": "Javier Acevedo, Patrick Seeling, Frank H. P. Fitzek",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310306",
    "source": "IEEE",
    "abstract": "Vector Digital Signal Processors (DSPs) have been employed in every generation of wireless communication systems due to their ability to compute complex algorithms, efficiently meeting the growing demand for higher data rates and user capacity. Compute-intensive wireless kernels, such as Beamforming (BF), equalization, and Channel Estimation (CE), require data parallelization to perform multiple matrix multiplications, inversions, and vector decomposition operations in real time. In this paper, we present the software implementation of hybrid beamforming for massive Multiple Input Multiple Output (mMIMO) 5th Generation Cellular Networks (5G) New Radio (NR). Specifically, we implement the linear Zero-Forcing (ZF) algorithm in a Reduced Instruction Set Computer (RISC)-V-based single core processor, computing the precoding matrix for a system composed of M transmit antennas at the eNodeB and K receivers, each with N antennas.Our single-core implementation demonstrates that optimizing the the algorithm with the Cholesky decomposition for computing the precoding matrix results in throughout and latency gains, when comparing the core’s scalar and vector processors. The vector processor achieves a speedup up to 107.73 and 108.36 in throughput and latency, respectively, when computing the ZF kernel.",
    "title_zh": "基于RISC-V向量DSP的大型MIMO混合波束成形硬件加速",
    "abstract_zh": "向量数字信号处理器（DSP）因其能够高效计算复杂算法，已被应用于每一代无线通信系统中，以满足日益增长的高数据速率和用户容量需求。计算密集型的无线核心算法，如波束成形（BF）、均衡和信道估计（CE），需要进行数据并行化处理，以实现实时完成多项矩阵乘法、求逆及向量分解操作。本文提出了一种针对大规模多输入多输出（mMIMO）第五代蜂窝网络（5G）新空口（NR）的混合波束成形的软件实现方案。具体而言，我们在基于精简指令集计算机（RISC-V）的单核处理器上实现了线性零强迫（ZF）算法，用于计算由eNodeB端M个发射天线和K个接收端（每个接收端配备N个天线）构成系统的预编码矩阵。我们的单核实现表明，通过采用Cholesky分解优化算法来计算预编码矩阵，在对比核心的标量处理器与向量处理器时，显著提升了吞吐量和降低了延迟。在计算ZF核心时，向量处理器分别实现了高达107.73倍和108.36倍的吞吐量与延迟加速。"
  },
  {
    "date": "2026-1-1",
    "title": "PAVE++ Demo: Cross-Layer Formal Verification and OTA Validation for UAV Communications",
    "authors": "Tom Wray, Ying Wang",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310334",
    "source": "IEEE",
    "abstract": "This demonstration presents an interactive showcase of the Prompt Aided Formal Verification (PAVE) framework applied to MAVLink 2, emphasizing practical and visual verification processes. Attendees will experience real-time demonstrations including: automatic translation of natural language protocol specifications into symbolic verification models using Large Language Models (LLMs); visual state-machine verification via nuXmv and ProVerif; simulated cyber-attacks on the Damn Vulnerable Drone (DVD) platform; and over-the-air (OTA) security validation using a physical PX4-based drone. Participants will directly observe MAVLink 2 vulnerabilities being exploited and immediately mitigated through ChaCha20 encryption. The demonstration underscores the effectiveness of combining symbolic verification with practical cryptographic protections, providing a clear and compelling methodology for ensuring secure UAV communication in contested operational scenarios.",
    "title_zh": "PAVE++ 演示：无人机通信的跨层形式化验证与OTA验证",
    "abstract_zh": "本演示展示了一个交互式案例，介绍了将提示辅助形式化验证（PAVE）框架应用于MAVLink 2的实践过程，重点突出可视化与实际验证流程。参会者将亲身体验以下实时演示内容：利用大型语言模型（LLMs）自动将自然语言协议规范转换为符号化验证模型；通过nuXmv和ProVerif进行可视化状态机验证；在“脆弱无人机”（DVD）平台上演示模拟网络攻击；以及使用基于PX4的实体无人机进行空中无线（OTA）安全验证。参与者将直接观察MAVLink 2协议中的漏洞如何被利用，并立即通过ChaCha20加密技术实现有效缓解。该演示凸显了将符号化验证与实际密码学防护相结合的有效性，为在对抗性操作环境中保障无人飞行器通信安全，提供了一种清晰且极具说服力的方法论。"
  },
  {
    "date": "2026-1-1",
    "title": "Design of a Reusable UVM Verification Framework for Pixel Readout Chips",
    "authors": "Yu Zhao, Zexuan Zhao, Yuanhong Jiao, Xiaomin Wei, Heng Yang, Long Liu, Yongcai Hu",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11315974",
    "source": "IEEE",
    "abstract": "The escalating demands of High-Energy Physics (HEP) experiments impose significant challenges on pixel readout chips, including growing functional complexity, compressed design cycles, and high fabrication costs. Traditional verification approaches, relying heavily on static stimuli and manual inspection, suffer from insufficient coverage, low automation, and poor reusability, thus failing to guarantee the functional integrity of such complex designs. To address these challenges, this work proposes and implements a reusable verification framework based on the Universal Verification Methodology (UVM), specifically tailored for advanced pixel readout chips. The key contributions of this framework include: (1) a physics-aware stimulus generator for emulating realistic particle-hit events; (2) a generic serial bus configuration component leveraging the Register ion Layer (RAL) to enable high-level automated configuration; and (3) an intelligent scoreboard capable of out-oforder data matching to ensure robust data validation. The framework’s effectiveness was validated on a pixel readout chip, where its metric-driven verification (MDV) methodology successfully exercised complex functionalities and corner cases of the design. By adopting a modular architecture, the framework significantly enhances reusability and efficiency, offering a systematic and scalable solution for the verification of similar mixed-signal chips.",
    "title_zh": "可重用的UVM验证框架设计用于像素读出芯片",
    "abstract_zh": "高能物理（HEP）实验日益增长的需求对像素读出芯片带来了巨大挑战，包括功能复杂度不断提升、设计周期不断压缩以及制造成本高昂等问题。传统的验证方法主要依赖静态激励和人工检查，存在覆盖不全、自动化程度低、可重用性差等缺陷，难以确保此类复杂设计的功能完整性。为应对这些挑战，本文提出并实现了一种基于通用验证方法学（UVM）的可复用验证框架，专门针对先进的像素读出芯片进行优化。该框架的主要贡献包括：（1）一种考虑物理特性的激励生成器，用于模拟真实的粒子撞击事件；（2）一个通用的串行总线配置组件，利用寄存器抽象层（RAL）实现高层级的自动化配置；（3）一种具备乱序数据匹配能力的智能记分板，以确保数据验证的可靠性。该框架在一款像素读出芯片上进行了验证，其基于度量的验证（MDV）方法成功覆盖了设计中的复杂功能及边界情况。通过采用模块化架构，该框架显著提升了可重用性和验证效率，为类似混合信号芯片的验证提供了一套系统化、可扩展的解决方案。"
  },
  {
    "date": "2026-1-1",
    "title": "An Adaptive dynamic R-tree indexing algorithm for HDI PCB layout",
    "authors": "Zhang Yang, Liang Xiaoyu, Ning Xu",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11315936",
    "source": "IEEE",
    "abstract": "The layout data intuitively conveys the fundamental spatial information of a PCB, and efficient querying of which is crucial for IC industrial manufacturing and EDA software algorithm development. In high-density interconnect (HDI) PCBs, traditional layout indexing algorithms suffer from low query efficiency due to massive data volumes, primarily caused by the large number of components and their compact spatial distribution. To address these challenges, this paper proposes an optimized approach to the classic R-tree algorithm. It adjusts the data storage capacity of nodes based on the scale of layout data adaptively and further optimizes the node splitting strategy using a heuristic splitting factor. This method reduces the overlap rate of spatial division, constructing an efficient indexing structure tailored for HDI PCBs. Experimental results show that compared with the traditional R -tree indexing algorithm, the proposed method achieves superior spatial index planning while ensuring query accuracy, with significantly improved indexing efficiency.",
    "title_zh": "一种用于HDI PCB布局的自适应动态R树索引算法",
    "abstract_zh": "布局数据直观地传达了PCB的基本空间信息，高效查询这些信息对于集成电路工业制造以及EDA软件算法开发至关重要。在高密度互连（HDI）PCB中，由于组件数量众多且空间分布紧凑，传统布局索引算法面临数据量巨大导致的查询效率低下问题。为应对这一挑战，本文提出了一种优化的经典R树算法。该方法根据布局数据规模自适应调整节点的数据存储容量，并进一步通过引入启发式分割因子优化节点分裂策略，有效降低了空间划分的重叠率，构建出适用于HDI PCB的高效索引结构。实验结果表明，与传统的R树索引算法相比，所提方法在保证查询准确性的前提下，实现了更优的空间索引规划，显著提升了索引效率。"
  },
  {
    "date": "2026-1-1",
    "title": "Model-Agnostic Unsupervised Detection of Prompt Injection with Multiscale Perplexity Signatures",
    "authors": "Jielun Zhang, Fuhao Li",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310042",
    "source": "IEEE",
    "abstract": "Prompt injection represents a critical and increasingly prevalent threat to the safety and reliability of large language models (LLMs). These attacks enable adversaries to override intended instructions using only natural language, often without access to model parameters or system internals. In this work, we propose a model-agnostic, unsupervised framework for prompt injection detection based on token-level uncertainty dynamics. Specifically, we introduce multi-scale perplexity signatures, i.e., structured representations of model perplexity computed across multiple sliding window sizes, to capture local and global fluctuations in predictive confidence. These signatures serve as input to lightweight statistical classifiers, such as OC-SVMs, which are trained exclusively on clean prompts to detect anomalous inputs. Extensive experiments are conducted on an open dataset, and the results demonstrate that the multiscale perplexity signatures captured by the proposed approach are effective in distinguishing injected prompts from clean ones across multiple detection models.",
    "title_zh": "无需依赖模型的无监督提示注入检测方法：基于多尺度困惑度特征",
    "abstract_zh": "提示注入是大型语言模型（LLM）安全性和可靠性面临的一项关键且日益严重的威胁。此类攻击使攻击者仅通过自然语言即可绕过模型的预定指令，而无需访问模型参数或系统内部结构。本文提出了一种模型无关、无监督的提示注入检测框架，其核心基于标记级别的不确定性动态分析。具体而言，我们引入了多尺度困惑度特征——即在多个滑动窗口尺寸下计算的模型困惑度的结构化表示，以捕捉预测置信度在局部与全局层面的波动变化。这些特征作为输入，被用于轻量级统计分类器（如OC-SVM）进行异常检测，该分类器仅在干净提示样本上训练，从而识别异常输入。我们在一个公开数据集上进行了大量实验，结果表明，所提出的多尺度困惑度特征能够有效区分注入提示与正常提示，适用于多种检测模型，展现出良好的泛化能力。"
  },
  {
    "date": "2026-1-1",
    "title": "Design and FPGA Implementation of a NTT Hardware Accelerator for PQC ML-DSA",
    "authors": "Shuhang Zhu, Liji Wu, Lei Li, Yifan Yang, Xiangmin Zhang",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11315992",
    "source": "IEEE",
    "abstract": "In this paper, we present a high - performance, resource - efficient hardware architecture for the 256-point Number Theoretic Transform (NTT) core tailored to the CRYSTALS-Dilithium post-quantum signature scheme. Our design introduces three key innovations: (1) a Barrett modular reduction unit that replaces conventional subtraction-based reduction with precomputed constants, achieving a 42% reduction in critical-path delay; (2) a dynamic, algorithmic address resolver that eliminates lookup tables and reduces LUT usage by $\\mathbf{1 5. 2 \\%}$; and (3) a quad-butterfly pipeline comprising four parallel configurable butterfly units that support both Cooley-Tukey and Gentleman-Sande modes, enabling a fourfold increase in per-cycle throughput. Implemented on FPGA with modulus $q=8,380,417$, the proposed NTT core consumes only $\\mathbf{1, 0 3 3}$ slices ($\\mathbf{3 9. 3 \\%}$ fewer) and 3,189 LUTs (44.1% fewer) compared to state-of-the-art designs, at a modest cost of 28 DSPs and 8 BRAMs. Functional simulation over 512 test points confirms 100% correctness ($\\mathbf{R}^{\\mathbf{2}}=\\mathbf{1. 0}$) across the full coefficient range. The architecture thus offers an attractive balance between throughput, resource utilization, and implementation simplicity, providing a practical foundation for lattice-based cryptosystems on resource constrained platforms.",
    "title_zh": "PQC ML-DSA 用 NTT 硬件加速器的设计与 FPGA 实现",
    "abstract_zh": "本文提出了一种高性能、资源高效的256点数论变换（NTT）核心硬件架构，专为CRYSTALS-Dilithium后量子签名方案量身定制。我们的设计引入了三项关键技术革新：（1）采用Barrett模约减单元，以预计算常数替代传统的基于减法的约减方法，使关键路径延迟降低42%；（2）设计了一种动态算法地址解析器，无需查找表，LUT使用量减少15.2%；（3）采用四蝶形流水线结构，包含四个并行可配置的蝶形单元，支持Cooley-Tukey与Gentleman-Sande两种模式，实现每周期吞吐量提升四倍。在FPGA上实现时，采用模数$q=8,380,417$，所提出的NTT核心仅消耗1,033个逻辑单元（比现有先进设计减少39.3%），以及3,189个LUT（减少44.1%），仅额外消耗28个DSP和8个BRAM。对512个测试点的功能仿真验证表明，在整个系数范围内均达到100%正确性（$\\mathbf{R}^{\\mathbf{2}}=\\mathbf{1.0}$）。该架构在吞吐量、资源利用率与实现复杂度之间取得了良好平衡，为资源受限平台上的格密码系统提供了实用可靠的基础。"
  },
  {
    "date": "2026-1-1",
    "title": "Ma’at: FPGA-Assisted Dual-Core Architecture for Deterministic IoT Telemetry Integrity",
    "authors": "Mohamed El-Hadedy, Wen-Mei Hwu, Benny Cheng",
    "publish": "MILCOM 2025 - 2025 IEEE Military Communications Conference (MILCOM)",
    "url": "https://doi.org/10.1109/milcom64451.2025.11310771",
    "source": "IEEE",
    "abstract": "Mission-critical IoT nodes must protect every telemetry record without blowing tight power and latency budgets, even under I/O and memory stress. We present Ma'at, a dual-core design on Zynq-7000 (PYNQ-Z1) that separates GPS ingest (Core-0) from hashing (Core-1) and adds two small FPGA blocks: an in-line NMEA 0183 checksum + timestamp guard and an Ascon-Hash accelerator reached via simple-mode AXI-DMA.On live GPS, Ma'at sustains about 1.0–1.1 hashes/s in software and 1.48 hashes/s with programmable-logic offload; the accelerator alone reaches 169.9 hashes/s in self-test. Offloading cuts Core-0 load from over 70% to under 60%, trims the long-tail GPS-to-tag delay by roughly one-fifth, and keeps any slowdown under background I/O within 20%. The guard enforces a fixed 2 s replay window in hardware. Energy per hash drops from more than 1 J in software to about 9 mJ in self-test.These results show that modest FPGA help—checksum, timestamp, DMA, and hash—hardens IoT telemetry today while preserving a direct path to authenticated encryption with Ascon.",
    "title_zh": "玛阿特：用于确定性物联网遥测完整性的FPGA辅助双核架构",
    "abstract_zh": "关键任务型物联网节点必须在严格的功耗和延迟预算下，即使在I/O与内存压力下也能保护每一条遥测记录。我们提出Ma'at，一种基于Zynq-7000（PYNQ-Z1）的双核设计，将GPS数据接收（Core-0）与哈希计算（Core-1）分离，并添加了两个小型FPGA模块：一个内联NMEA 0183校验和+时间戳防护单元，以及一个通过简单模式AXI-DMA访问的Ascon-Hash加速器。在实际GPS数据流下，Ma'at在纯软件模式下可维持约1.0–1.1次/秒的哈希处理速度，而通过可编程逻辑卸载后提升至1.48次/秒；仅加速器自身在自测中可达169.9次/秒。卸载显著降低了Core-0的负载，从超过70%降至不足60%，并将GPS到标签的长尾延迟削减约五分之一，同时确保任何性能下降仍控制在背景I/O允许范围内的20%以内。防护单元在硬件层面强制实现固定的2秒重放窗口。每次哈希的能耗从纯软件模式下的超过1焦耳降至自测中的约9毫焦。这些结果表明，适度的FPGA辅助——包括校验和、时间戳、DMA及哈希加速——能够有效增强当前物联网遥测的安全性，同时保留直接通往Ascon认证加密的路径。"
  },
  {
    "date": "2026-1-1",
    "title": "Cross-Process Bayesian Multi-Objective Collaborative Optimization For Process Migration",
    "authors": "Zixi Guo, Ruiyu Lyu, Zhaori Bi, Changhao Yan, Ming Zhu, Xiulong Wu, Xuan Zeng",
    "publish": "2025 10th International Conference on Integrated Circuits and Microsystems (ICICM)",
    "url": "https://doi.org/10.1109/icicm66614.2025.11316045",
    "source": "IEEE",
    "abstract": "This paper introduces a cross-process Bayesian multi-objective collaborative optimization framework to address challenges in semiconductor technology migration. We leverage transfer learning within multi-objective Bayesian optimization through Gaussian copula, transforming simulation data into residual observations. By transferring prior knowledge from implemented technologies to advanced processes, we significantly reduce the number of required circuit simulations in newer technologies, effectively optimizing the parameter design space for next-generation processes. Furthermore, we establish Gaussian process regression models that correlate design variables with performance metrics across both advanced and implemented technologies. These models undergo continuous updates during iteration, enabling collaborative optimization throughout the technology migration engineering process.",
    "title_zh": "跨进程贝叶斯多目标协同优化在进程迁移中的应用",
    "abstract_zh": "本文提出了一种跨工艺的贝叶斯多目标协同优化框架，以应对半导体技术迁移中的挑战。通过在多目标贝叶斯优化中引入高斯互相关（Gaussian copula）实现迁移学习，将仿真数据转化为残差观测值。利用已实现工艺中的先验知识向先进工艺进行迁移，显著减少了在新型工艺中所需的电路仿真次数，有效优化了下一代工艺的参数设计空间。此外，我们建立了高斯过程回归模型，关联先进工艺与已实现工艺中设计变量与性能指标之间的关系。这些模型在迭代过程中持续更新，实现了技术迁移工程全过程中的协同优化。"
  }
]